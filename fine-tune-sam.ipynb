{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune SAM using a Table and collect custom metrics and embeddings\n",
    "\n",
    "This notebook is a modified version of the official colab tutorial from Encord which can be found [here](https://colab.research.google.com/drive/1F6uRommb3GswcRlPZWpkAQRMVNdVH7Ww).\n",
    "\n",
    "It demonstrates how you can use a 3LC Table to fine-tune Segment Anything Model (SAM). It also demonstrates how 3LC\n",
    "can collect custom metrics and embeddings within a training loop.\n",
    "\n",
    "In order to run this notebook, you must first have run the `create_sam_dataset.ipynb` notebook to create the\n",
    "Table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "DATASET_NAME = \"staver-dataset\"  # Need to match create_sam_dataset.ipnyb DATASET_NAME\n",
    "RUN_NAME = \"staver-run\"\n",
    "\n",
    "# Training parameters\n",
    "MODEL_TYPE = \"vit_b\"\n",
    "CHECKPOINT = \"sam_vit_b_01ec64.pth\"\n",
    "DEVICE = \"cuda:0\"\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Embedding parameters\n",
    "EMBEDDING_DIM = 2\n",
    "REDUCTION_METHOD = \"pacmap\"\n",
    "\n",
    "INSTALL_DEPENDENCIES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if INSTALL_DEPENDENCIES:\n",
    "    %pip --quiet install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "    %pip --quiet install torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "    %pip --quiet install git+https://github.com/facebookresearch/segment-anything.git\n",
    "    %pip --quiet install opencv-python\n",
    "    %pip --quiet install tlc[pacmap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tlc\n",
    "from typing import List, Union\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CHECKPOINT):\n",
    "    torch.hub.download_url_to_file(\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# 3LC parameters\n",
    "TABLE_URL = tlc.Table.default_write_location() / DATASET_NAME / \"table_0000.json\"\n",
    "RUN_URL = tlc.Run.default_write_location() / RUN_NAME / \"run.json\"\n",
    "\n",
    "\n",
    "# Derived Constants\n",
    "def create_model():\n",
    "    sam_model = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT)\n",
    "    sam_model.to(DEVICE)\n",
    "    sam_model.train()\n",
    "    return sam_model\n",
    "\n",
    "\n",
    "sam_model = create_model()\n",
    "\n",
    "\n",
    "\n",
    "run = tlc.Run(url=RUN_URL)\n",
    "\n",
    "\n",
    "run.write_to_url()\n",
    "run.add_input_table(TABLE_URL)\n",
    "\n",
    "\n",
    "resize_transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_sam_format(sample):\n",
    "    image = cv2.imread(sample[\"image\"])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "    input_image = transform.apply_image(image)\n",
    "    input_image_torch = torch.as_tensor(input_image, device=DEVICE)\n",
    "    transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "    input_image = sam_model.preprocess(transformed_image)\n",
    "\n",
    "    original_image_size = image.shape[:2]\n",
    "\n",
    "    input_size = tuple(transformed_image.shape[-2:])\n",
    "\n",
    "    ground_truth_masks = cv2.imread(sample[\"mask\"], cv2.IMREAD_GRAYSCALE)\n",
    "    ground_truth_masks = ground_truth_masks == 0\n",
    "\n",
    "    prompt_box = np.array(\n",
    "        [\n",
    "            sample[\"prompt box\"][\"bb_list\"][0][\"x0\"],\n",
    "            sample[\"prompt box\"][\"bb_list\"][0][\"y0\"],\n",
    "            sample[\"prompt box\"][\"bb_list\"][0][\"x1\"],\n",
    "            sample[\"prompt box\"][\"bb_list\"][0][\"y1\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    transformed_sample = {\n",
    "        \"image\": input_image,\n",
    "        \"input_size\": input_size,\n",
    "        \"original_image_size\": original_image_size,\n",
    "        \"ground_truth_masks\": ground_truth_masks,\n",
    "        \"prompt_box\": prompt_box,\n",
    "    }\n",
    "\n",
    "    return transformed_sample\n",
    "\n",
    "\n",
    "def pre_reduce_embedding(embedding: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Prepare a batch of embeddings for writing to a metrics table by reducing its\n",
    "    size, moving it to CPU, and converting it to a numpy array.\n",
    "    \"\"\"\n",
    "    return embedding.mean(dim=[2, 3]).cpu().numpy()\n",
    "\n",
    "\n",
    "def create_metrics_writer() -> tlc.MetricsWriter:\n",
    "    size0 = tlc.DimensionNumericValue(\n",
    "        value_min=256,\n",
    "        value_max=256,\n",
    "        enforce_min=True,\n",
    "        enforce_max=True,\n",
    "    )\n",
    "\n",
    "    image_embedding_schema = tlc.Schema(\n",
    "        \"Image Embedding\",\n",
    "        writable=False,\n",
    "        computable=False,\n",
    "        value=tlc.Float32Value(number_role=tlc.NUMBER_ROLE_NN_EMBEDDING),\n",
    "        size0=size0,\n",
    "    )\n",
    "\n",
    "    prompt_embedding_schema = tlc.Schema(\n",
    "        \"Prompt Embedding\",\n",
    "        writable=False,\n",
    "        computable=False,\n",
    "        value=tlc.Float32Value(number_role=tlc.NUMBER_ROLE_NN_EMBEDDING),\n",
    "        size0=size0,\n",
    "    )\n",
    "\n",
    "    loss_schema = tlc.Schema(writable=False, value=tlc.Float32Value())\n",
    "    prediction_schema = tlc.Schema(writable=False, value=tlc.ImageUrlStringValue())\n",
    "\n",
    "    return tlc.MetricsWriter(\n",
    "        run_url=RUN_URL,\n",
    "        dataset_url=TABLE_URL.to_str(),\n",
    "        override_column_schemas={\n",
    "            \"image_embedding\": image_embedding_schema,\n",
    "            \"prompt_embedding\": prompt_embedding_schema,\n",
    "            \"loss\": loss_schema,\n",
    "            \"predicted_mask\": prediction_schema,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def capture_metrics(\n",
    "    image_embedding: torch.Tensor,\n",
    "    prompt_embedding: torch.Tensor,\n",
    "    loss: float,\n",
    "    prediction: torch.Tensor,\n",
    "    example_ids: List[int],\n",
    "    epoch: int,\n",
    "    metrics_writer: Union[tlc.MetricsWriter, None],\n",
    ") -> None:\n",
    "    if metrics_writer is None:\n",
    "        return\n",
    "\n",
    "    reduced_image_embedding = pre_reduce_embedding(image_embedding)\n",
    "    reduced_prompt_embedding = prompt_embedding.mean(dim=[1]).cpu().numpy()\n",
    "\n",
    "    prediction_url = metrics_writer.root_metrics_url.parent / \"predictions\" / str(epoch) / f\"{example_ids[0]}.png\"\n",
    "\n",
    "    prediction_url.make_parents(exist_ok=True)\n",
    "\n",
    "    img = ToPILImage()(1 - prediction.cpu().detach().squeeze())\n",
    "    img.save(prediction_url.to_str())\n",
    "\n",
    "    metrics_writer.add_batch(\n",
    "        {\n",
    "            \"image_embedding\": reduced_image_embedding,\n",
    "            \"prompt_embedding\": reduced_prompt_embedding,\n",
    "            \"loss\": [loss],\n",
    "            \"predicted_mask\": [prediction_url.to_str()],\n",
    "            \"example_id\": example_ids,\n",
    "            \"epoch\": [epoch],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def flush_metrics_writer(metrics_writer: Union[tlc.MetricsWriter, None]) -> None:\n",
    "    if metrics_writer is None:\n",
    "        return\n",
    "\n",
    "    metrics_writer.flush()\n",
    "    metrics_infos = metrics_writer.get_written_metrics_infos()\n",
    "    run.update_metrics(metrics_infos)\n",
    "\n",
    "\n",
    "def reduce_all_embeddings() -> None:\n",
    "    run.reduce_embeddings_by_example_table_url(TABLE_URL, method=REDUCTION_METHOD, n_components=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tlc.Table.from_url(TABLE_URL).map(transform_to_sam_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "cached_samples = [sample for sample in table]\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Create a 3LC metrics writer on the last epoch\n",
    "    metrics_writer = None\n",
    "    if (epoch + 1) == NUM_EPOCHS:\n",
    "        metrics_writer = create_metrics_writer()\n",
    "\n",
    "    for i, sample in enumerate(tqdm(cached_samples, desc=f\"Epoch {epoch}\")):\n",
    "        with torch.no_grad():\n",
    "            image_embedding = sam_model.image_encoder(sample[\"image\"])\n",
    "            prompt_box = sample[\"prompt_box\"]\n",
    "            box = resize_transform.apply_boxes(prompt_box, sample[\"original_image_size\"])\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float, device=DEVICE)\n",
    "            box_torch = box_torch[None, :]\n",
    "            sparse_prompt_embedding, dense_prompt_embedding = sam_model.prompt_encoder(\n",
    "                points=None,\n",
    "                boxes=box_torch,\n",
    "                masks=None,\n",
    "            )\n",
    "\n",
    "        low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "            image_embeddings=image_embedding,\n",
    "            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_prompt_embedding,\n",
    "            dense_prompt_embeddings=dense_prompt_embedding,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        upscaled_masks = sam_model.postprocess_masks(\n",
    "            low_res_masks, sample[\"input_size\"], sample[\"original_image_size\"]\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        binary_mask = normalize(threshold(upscaled_masks, 0.0, 0))\n",
    "        gt_mask_resized = torch.from_numpy(\n",
    "            np.resize(\n",
    "                sample[\"ground_truth_masks\"],\n",
    "                (1, 1, sample[\"ground_truth_masks\"].shape[0], sample[\"ground_truth_masks\"].shape[1]),\n",
    "            )\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "\n",
    "        loss = loss_fn(binary_mask, gt_binary_mask)\n",
    "\n",
    "        # Capture metrics with 3LC\n",
    "        capture_metrics(\n",
    "            image_embedding=image_embedding,\n",
    "            prompt_embedding=sparse_prompt_embedding,\n",
    "            example_ids=[i],\n",
    "            epoch=epoch,\n",
    "            loss=loss.item(),\n",
    "            prediction=binary_mask,\n",
    "            metrics_writer=metrics_writer,\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Flush one epoch of metrics\n",
    "    flush_metrics_writer(metrics_writer)\n",
    "\n",
    "# Reduce all captured embeddings\n",
    "reduce_all_embeddings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
