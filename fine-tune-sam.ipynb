{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4a6522",
   "metadata": {},
   "source": [
    "# Fine-tune SAM using a Table and collect custom metrics and embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53505613",
   "metadata": {},
   "source": [
    "<div style=\"display: inline-flex; align-items: center; gap: 10px;\">\n",
    "        <a href=\"https://colab.research.google.com/github/3lc-ai/notebook-examples/blob/main/fine-tune-sam.ipynb\"\n",
    "        target=\"_blank\"\n",
    "            style=\"background-color: transparent; text-decoration: none; display: inline-flex; align-items: center;\n",
    "            padding: 5px 10px; font-family: Arial, sans-serif;\"> <img\n",
    "            src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" style=\"height: 30px;\n",
    "            vertical-align: middle;box-shadow: none;\"/>\n",
    "        </a> <a href=\"https://github.com/3lc-ai/notebook-examples/blob/main/fine-tune-sam.ipynb\"\n",
    "            style=\"text-decoration: none; display: inline-flex; align-items: center; background-color: #ffffff; border:\n",
    "            1px solid #d1d5da; border-radius: 8px; padding: 2px 10px; color: #333; font-family: Arial, sans-serif;\">\n",
    "            <svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-mark-github\" viewBox=\"0 0 16 16\"\n",
    "            width=\"20\" height=\"20\" fill=\"#333\"\n",
    "            style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible; margin-right:\n",
    "            8px;\">\n",
    "                <path d=\"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2\n",
    "                0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0\n",
    "                0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16\n",
    "                1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51\n",
    "                1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68\n",
    "                1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z\"></path>\n",
    "            </svg> <span style=\"vertical-align: middle; color: #333;\">Open in GitHub</span>\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a952f",
   "metadata": {},
   "source": [
    "This notebook is a modified version of the official colab tutorial from Encord which can be found [here](https://colab.research.google.com/drive/1F6uRommb3GswcRlPZWpkAQRMVNdVH7Ww).\n",
    "\n",
    "It demonstrates how you can use a 3LC Table to fine-tune Segment Anything Model (SAM). It also demonstrates how 3LC\n",
    "can collect custom metrics and embeddings within a training loop.\n",
    "\n",
    "In order to run this notebook, you must first have run the `create_sam_dataset.ipynb` notebook to create the\n",
    "Table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d235cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "DATASET_NAME = \"staver-dataset\"  # Need to match create_sam_dataset.ipnyb DATASET_NAME\n",
    "RUN_NAME = \"staver-run\"\n",
    "\n",
    "# Training parameters\n",
    "MODEL_TYPE = \"vit_b\"\n",
    "CHECKPOINT = \"sam_vit_b_01ec64.pth\"\n",
    "DEVICE = \"cuda:0\"\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Embedding parameters\n",
    "EMBEDDING_DIM = 2\n",
    "REDUCTION_METHOD = \"pacmap\"\n",
    "\n",
    "INSTALL_DEPENDENCIES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03800dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if INSTALL_DEPENDENCIES:\n",
    "    %pip --quiet install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "    %pip --quiet install torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "    %pip --quiet install git+https://github.com/facebookresearch/segment-anything.git\n",
    "    %pip --quiet install opencv-python\n",
    "    %pip --quiet install tlc[pacmap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HIDDEN CELL ###\n",
    "\n",
    "# Reloads all modules every time before executing the Python code.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Ensure notebook_tests on PATH\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "import notebook_tests\n",
    "\n",
    "# Optionally override the default test data path\n",
    "if path := os.getenv(\"TLC_PUBLIC_EXAMPLES_TEST_DATA_PATH\"):\n",
    "    print(f\"Using test data path: {path}\")\n",
    "    TEST_DATA_PATH = path\n",
    "\n",
    "# Prints the current 3lc configuration\n",
    "!3lc config --list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef8f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tlc\n",
    "from typing import List, Union\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55e9b78",
   "metadata": {},
   "source": [
    "## Downloading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86039bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CHECKPOINT):\n",
    "    torch.hub.download_url_to_file(\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf9613",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# 3LC parameters\n",
    "TABLE_URL = tlc.Url.create_table_url(project_name=\"SAM_EXAMPLE\", dataset_name=DATASET_NAME)\n",
    "\n",
    "RUN_URL = tlc.Url.create_run_url(run_name=RUN_NAME)\n",
    "\n",
    "\n",
    "# Derived Constants\n",
    "def create_model():\n",
    "    sam_model = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT)\n",
    "    sam_model.to(DEVICE)\n",
    "    sam_model.train()\n",
    "    return sam_model\n",
    "\n",
    "\n",
    "sam_model = create_model()\n",
    "\n",
    "\n",
    "run = tlc.Run(url=RUN_URL)\n",
    "\n",
    "\n",
    "run.write_to_url()\n",
    "run.add_input_table(TABLE_URL)\n",
    "\n",
    "\n",
    "resize_transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f954edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_sam_format(sample):\n",
    "    image = cv2.imread(sample[\"image\"])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "    input_image = transform.apply_image(image)\n",
    "    input_image_torch = torch.as_tensor(input_image, device=DEVICE)\n",
    "    transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "    input_image = sam_model.preprocess(transformed_image)\n",
    "\n",
    "    original_image_size = image.shape[:2]\n",
    "\n",
    "    input_size = tuple(transformed_image.shape[-2:])\n",
    "\n",
    "    ground_truth_masks = cv2.imread(sample[\"mask\"], cv2.IMREAD_GRAYSCALE)\n",
    "    ground_truth_masks = ground_truth_masks == 0\n",
    "\n",
    "    prompt_box = np.array(\n",
    "        [\n",
    "            sample[\"prompt box\"][\"bb_list\"][0][\"x0\"],\n",
    "            sample[\"prompt box\"][\"bb_list\"][0][\"y0\"],\n",
    "            sample[\"prompt box\"][\"bb_list\"][0][\"x1\"],\n",
    "            sample[\"prompt box\"][\"bb_list\"][0][\"y1\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    transformed_sample = {\n",
    "        \"image\": input_image,\n",
    "        \"input_size\": input_size,\n",
    "        \"original_image_size\": original_image_size,\n",
    "        \"ground_truth_masks\": ground_truth_masks,\n",
    "        \"prompt_box\": prompt_box,\n",
    "    }\n",
    "\n",
    "    return transformed_sample\n",
    "\n",
    "\n",
    "def pre_reduce_embedding(embedding: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Prepare a batch of embeddings for writing to a metrics table by reducing its\n",
    "    size, moving it to CPU, and converting it to a numpy array.\n",
    "    \"\"\"\n",
    "    return embedding.mean(dim=[2, 3]).cpu().numpy()\n",
    "\n",
    "\n",
    "def create_metrics_writer() -> tlc.MetricsTableWriter:\n",
    "    size0 = tlc.DimensionNumericValue(\n",
    "        value_min=256,\n",
    "        value_max=256,\n",
    "        enforce_min=True,\n",
    "        enforce_max=True,\n",
    "    )\n",
    "\n",
    "    image_embedding_schema = tlc.Schema(\n",
    "        \"Image Embedding\",\n",
    "        writable=False,\n",
    "        computable=False,\n",
    "        value=tlc.Float32Value(number_role=tlc.NUMBER_ROLE_NN_EMBEDDING),\n",
    "        size0=size0,\n",
    "    )\n",
    "\n",
    "    prompt_embedding_schema = tlc.Schema(\n",
    "        \"Prompt Embedding\",\n",
    "        writable=False,\n",
    "        computable=False,\n",
    "        value=tlc.Float32Value(number_role=tlc.NUMBER_ROLE_NN_EMBEDDING),\n",
    "        size0=size0,\n",
    "    )\n",
    "\n",
    "    loss_schema = tlc.Schema(writable=False, value=tlc.Float32Value())\n",
    "    prediction_schema = tlc.Schema(writable=False, value=tlc.ImageUrlStringValue())\n",
    "\n",
    "    return tlc.MetricsTableWriter(\n",
    "        run_url=RUN_URL,\n",
    "        dataset_url=TABLE_URL.to_str(),\n",
    "        override_column_schemas={\n",
    "            \"image_embedding\": image_embedding_schema,\n",
    "            \"prompt_embedding\": prompt_embedding_schema,\n",
    "            \"loss\": loss_schema,\n",
    "            \"predicted_mask\": prediction_schema,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def capture_metrics(\n",
    "    image_embedding: torch.Tensor,\n",
    "    prompt_embedding: torch.Tensor,\n",
    "    loss: float,\n",
    "    prediction: torch.Tensor,\n",
    "    example_ids: List[int],\n",
    "    epoch: int,\n",
    "    metrics_writer: Union[tlc.MetricsTableWriter, None],\n",
    ") -> None:\n",
    "    if metrics_writer is None:\n",
    "        return\n",
    "\n",
    "    reduced_image_embedding = pre_reduce_embedding(image_embedding)\n",
    "    reduced_prompt_embedding = prompt_embedding.mean(dim=[1]).cpu().numpy()\n",
    "\n",
    "    prediction_url = metrics_writer.root_metrics_url.parent / \"predictions\" / str(epoch) / f\"{example_ids[0]}.png\"\n",
    "\n",
    "    prediction_url.make_parents(exist_ok=True)\n",
    "\n",
    "    img = ToPILImage()(1 - prediction.cpu().detach().squeeze())\n",
    "    img.save(prediction_url.to_str())\n",
    "\n",
    "    metrics_writer.add_batch(\n",
    "        {\n",
    "            \"image_embedding\": reduced_image_embedding,\n",
    "            \"prompt_embedding\": reduced_prompt_embedding,\n",
    "            \"loss\": [loss],\n",
    "            \"predicted_mask\": [prediction_url.to_str()],\n",
    "            \"example_id\": example_ids,\n",
    "            \"epoch\": [epoch],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def flush_metrics_writer(metrics_writer: Union[tlc.MetricsTableWriter, None]) -> None:\n",
    "    if metrics_writer is None:\n",
    "        return\n",
    "\n",
    "    metrics_writer.finalize()\n",
    "    metrics_infos = metrics_writer.get_written_metrics_infos()\n",
    "    run.update_metrics(metrics_infos)\n",
    "\n",
    "\n",
    "def reduce_all_embeddings() -> None:\n",
    "    run.reduce_embeddings_by_foreign_table_url(TABLE_URL, method=REDUCTION_METHOD, n_components=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tlc.Table.from_url(TABLE_URL).map(transform_to_sam_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "cached_samples = [sample for sample in table]\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Create a 3LC metrics writer on the last epoch\n",
    "    metrics_writer = None\n",
    "    if (epoch + 1) == NUM_EPOCHS:\n",
    "        metrics_writer = create_metrics_writer()\n",
    "\n",
    "    for i, sample in enumerate(tqdm(cached_samples, desc=f\"Epoch {epoch}\")):\n",
    "        with torch.no_grad():\n",
    "            image_embedding = sam_model.image_encoder(sample[\"image\"])\n",
    "            prompt_box = sample[\"prompt_box\"]\n",
    "            box = resize_transform.apply_boxes(prompt_box, sample[\"original_image_size\"])\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float, device=DEVICE)\n",
    "            box_torch = box_torch[None, :]\n",
    "            sparse_prompt_embedding, dense_prompt_embedding = sam_model.prompt_encoder(\n",
    "                points=None,\n",
    "                boxes=box_torch,\n",
    "                masks=None,\n",
    "            )\n",
    "\n",
    "        low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "            image_embeddings=image_embedding,\n",
    "            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_prompt_embedding,\n",
    "            dense_prompt_embeddings=dense_prompt_embedding,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        upscaled_masks = sam_model.postprocess_masks(\n",
    "            low_res_masks, sample[\"input_size\"], sample[\"original_image_size\"]\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        binary_mask = normalize(threshold(upscaled_masks, 0.0, 0))\n",
    "        gt_mask_resized = torch.from_numpy(\n",
    "            np.resize(\n",
    "                sample[\"ground_truth_masks\"],\n",
    "                (1, 1, sample[\"ground_truth_masks\"].shape[0], sample[\"ground_truth_masks\"].shape[1]),\n",
    "            )\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "\n",
    "        loss = loss_fn(binary_mask, gt_binary_mask)\n",
    "\n",
    "        # Capture metrics with 3LC\n",
    "        capture_metrics(\n",
    "            image_embedding=image_embedding,\n",
    "            prompt_embedding=sparse_prompt_embedding,\n",
    "            example_ids=[i],\n",
    "            epoch=epoch,\n",
    "            loss=loss.item(),\n",
    "            prediction=binary_mask,\n",
    "            metrics_writer=metrics_writer,\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Flush one epoch of metrics\n",
    "    flush_metrics_writer(metrics_writer)\n",
    "\n",
    "# Reduce all captured embeddings\n",
    "reduce_all_embeddings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
