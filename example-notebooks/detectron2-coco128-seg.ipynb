{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfde496",
   "metadata": {},
   "source": [
    "# COCO128 + Detectron2 + 3LC Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f67f7",
   "metadata": {},
   "source": [
    "<div style=\"display: inline-flex; align-items: center; gap: 10px;\">\n",
    "        <a href=\"https://colab.research.google.com/github/3lc-ai/3lc-examples/blob/main/example-notebooks/detectron2-coco128-seg.ipynb\"\n",
    "        target=\"_blank\"\n",
    "            style=\"background-color: transparent; text-decoration: none; display: inline-flex; align-items: center;\n",
    "            padding: 5px 10px; font-family: Arial, sans-serif;\"> <img\n",
    "            src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" style=\"height: 30px;\n",
    "            vertical-align: middle;box-shadow: none;\"/>\n",
    "        </a> <a href=\"https://github.com/3lc-ai/3lc-examples/blob/main/example-notebooks/detectron2-coco128-seg.ipynb\"\n",
    "            style=\"text-decoration: none; display: inline-flex; align-items: center; background-color: #ffffff; border:\n",
    "            1px solid #d1d5da; border-radius: 8px; padding: 2px 10px; color: #333; font-family: Arial, sans-serif;\">\n",
    "            <svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-mark-github\" viewBox=\"0 0 16 16\"\n",
    "            width=\"20\" height=\"20\" fill=\"#333\"\n",
    "            style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible; margin-right:\n",
    "            8px;\">\n",
    "                <path d=\"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2\n",
    "                0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0\n",
    "                0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16\n",
    "                1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51\n",
    "                1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68\n",
    "                1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z\"></path>\n",
    "            </svg> <span style=\"vertical-align: middle; color: #333;\">Open in GitHub</span>\n",
    "        </a>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "229d9879",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "This notebook focuses on metrics collection for object detection using the Detectron2 library and the COCO128 subset. No\n",
    "training is performed; instead, we use a pretrained model to evaluate performance. Metrics related to bounding boxes\n",
    "(true positives, false positives, false negatives, iou, confidence) are collected using the BoundingBoxMetricsCollector\n",
    "class. The same class is used to collect instance segmentation metrics.\n",
    "\n",
    "The notebook illustrates:\n",
    "\n",
    "+ Metrics collection on a pretrained Detectron2 model using the COCO128 subset.\n",
    "+ Using `BoundingBoxMetricsCollector` for collecting object detection and segmentation metrics.\n",
    "+ Collection per-sample embeddings using `EmbeddingsMetricsCollector`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be1ed5",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da8ed16",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"COCO128\"\n",
    "RUN_NAME = \"COCO128-Segmentation-Metrics-Collection\"\n",
    "DESCRIPTION = \"Collect segmentation metrics for COCO128\"\n",
    "TRAIN_DATASET_NAME = \"COCO128-seg\"\n",
    "TRANSIENT_DATA_PATH = \"../transient_data\"\n",
    "TEST_DATA_PATH = \"./data\"\n",
    "MODEL_CONFIG = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"\n",
    "MAX_DETECTIONS_PER_IMAGE = 30\n",
    "SCORE_THRESH_TEST = 0.5\n",
    "MASK_FORMAT = \"bitmask\"\n",
    "TLC_PUBLIC_EXAMPLES_DEVELOPER_MODE = True\n",
    "INSTALL_DEPENDENCIES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943fe4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if INSTALL_DEPENDENCIES:\n",
    "    # NOTE: There is no single version of detectron2 that is appropriate for all users and all systems.\n",
    "    #       This notebook uses a particular prebuilt version of detectron2 that is only available for\n",
    "    #       Linux and for specific versions of torch, torchvision, and CUDA. It may not be appropriate\n",
    "    #       for your system. See https://detectron2.readthedocs.io/en/latest/tutorials/install.html for\n",
    "    #       instructions on how to install or build a version of detectron2 for your system.\n",
    "    %pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 -f https://download.pytorch.org/whl/cu111/torch_stable.html\n",
    "    %pip install detectron2 -f \"https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html\"\n",
    "    %pip install 3lc[pacmap]\n",
    "    %pip install opencv-python\n",
    "    %pip install matplotlib\n",
    "    %pip install numpy==1.24.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c1af4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dbd035",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d288Z2mF5dC",
    "outputId": "c47c5426-64d6-4632-f868-e2f14dfe39be"
   },
   "outputs": [],
   "source": [
    "import detectron2\n",
    "import torch\n",
    "\n",
    "import tlc\n",
    "\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"tlc: \", tlc.__version__)\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52542f",
   "metadata": {
    "id": "ZyAvNCJMmvFF"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "logger = setup_logger()\n",
    "logger.setLevel(\"ERROR\")\n",
    "\n",
    "# import some common libraries\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b51e12dc",
   "metadata": {
    "id": "tjbUIhSxUdm_"
   },
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "A small subset of the [COCO dataset](https://github.com/3lc-ai/3lc-examples/tree/main/data/coco128) (in the COCO standard format) is available in the `./data/coco128` directory.\n",
    "\n",
    "It is provided while cloning our [repository](https://github.com/3lc-ai/3lc-examples/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2e988b9",
   "metadata": {},
   "source": [
    "## Register the dataset with 3LC\n",
    "\n",
    "Now that we have the dataset in the COCO format, we can register it with 3LC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlc.integration.detectron2 import register_coco_instances\n",
    "\n",
    "register_coco_instances(\n",
    "    TRAIN_DATASET_NAME,\n",
    "    {},\n",
    "    train_json_path.to_str(),\n",
    "    train_image_folder.to_str(),\n",
    "    project_name=PROJECT_NAME,\n",
    "    task=\"segment\",\n",
    "    mask_format=MASK_FORMAT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29efd996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The detectron2 dataset dicts and dataset metadata can be read from the DatasetCatalog and\n",
    "# MetadataCatalog, respectively.\n",
    "dataset_metadata = MetadataCatalog.get(TRAIN_DATASET_NAME)\n",
    "dataset_dicts = DatasetCatalog.get(TRAIN_DATASET_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94ccd32f",
   "metadata": {
    "id": "6ljbWTX0Wi8E"
   },
   "source": [
    "To verify the dataset is in correct format, let's visualize the annotations of randomly selected samples in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4d7ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UkNbUzUOLYf0",
    "outputId": "4f5ed932-624a-4ede-9d5b-22371569fe1d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from detectron2.utils.file_io import PathManager\n",
    "\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    filename = tlc.Url(d[\"file_name\"]).to_absolute().to_str()\n",
    "    if \"s3://\" in filename:\n",
    "        with PathManager.open(filename, \"rb\") as f:\n",
    "            img = np.asarray(bytearray(f.read()), dtype=\"uint8\")\n",
    "            img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "    else:\n",
    "        img = cv2.imread(filename)\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=dataset_metadata, scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    out_rgb = cv2.cvtColor(out.get_image(), cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(out_rgb[:, :, ::-1])\n",
    "    plt.title(filename.split(\"/\")[-1])\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48410f5f",
   "metadata": {
    "id": "wlqXIXXhW8dA"
   },
   "source": [
    "## Start a 3LC Run and collect bounding box evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = tlc.init(\n",
    "    PROJECT_NAME,\n",
    "    run_name=RUN_NAME,\n",
    "    description=DESCRIPTION,\n",
    "    if_exists=\"overwrite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(MODEL_CONFIG))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(MODEL_CONFIG)\n",
    "cfg.DATASETS.TRAIN = (TRAIN_DATASET_NAME,)\n",
    "cfg.OUTPUT_DIR = TRANSIENT_DATA_PATH\n",
    "cfg.DATALOADER.NUM_WORKERS = 0\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 80\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = MAX_DETECTIONS_PER_IMAGE\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = SCORE_THRESH_TEST\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
    "cfg.INPUT.MASK_FORMAT = MASK_FORMAT\n",
    "\n",
    "config = {\n",
    "    \"model_config\": MODEL_CONFIG,\n",
    "    \"test.detections_per_image\": MAX_DETECTIONS_PER_IMAGE,\n",
    "    \"model.roi_heads.score_thresh_test\": SCORE_THRESH_TEST,\n",
    "    \"input.mask_format\": MASK_FORMAT,\n",
    "}\n",
    "\n",
    "run.set_parameters(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037fe5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7unkuuiqLdqd",
    "outputId": "ba1716cd-3f3b-401d-bae5-8fbbd2199d9c"
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "from tlc.integration.detectron2 import MetricsCollectionHook\n",
    "\n",
    "trainer = DefaultTrainer(cfg)\n",
    "\n",
    "# Define Embeddings metrics\n",
    "layer_index = 138  # Index of the layer to collect embeddings from\n",
    "embeddings_metrics_collector = tlc.EmbeddingsMetricsCollector(layers=[layer_index])\n",
    "\n",
    "predictor = tlc.Predictor(trainer.model, layers=[layer_index])\n",
    "\n",
    "bounding_box_metrics_collector = tlc.BoundingBoxMetricsCollector(\n",
    "    classes=dataset_metadata.thing_classes,\n",
    "    label_mapping=dataset_metadata.thing_dataset_id_to_contiguous_id,\n",
    "    save_segmentations=True,\n",
    ")\n",
    "\n",
    "metrics_collection_hook = MetricsCollectionHook(\n",
    "    dataset_name=TRAIN_DATASET_NAME,\n",
    "    metrics_collectors=[bounding_box_metrics_collector, embeddings_metrics_collector],\n",
    "    collect_metrics_before_train=True,\n",
    "    predictor=predictor,  # Needs to be used for embeddings metrics\n",
    ")\n",
    "\n",
    "trainer.register_hooks([metrics_collection_hook])\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.before_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlc import Table\n",
    "\n",
    "val_table = Table.from_url(dataset_metadata.get(\"latest_tlc_table_url\")).url  # Get the last revision of the val table\n",
    "\n",
    "url_mapping = run.reduce_embeddings_by_foreign_table_url(\n",
    "    val_table,\n",
    "    method=\"pacmap\",\n",
    "    n_components=3,\n",
    "    n_neighbors=5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
