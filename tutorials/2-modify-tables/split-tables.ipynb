{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split tables\n",
    "\n",
    "Datasets are commonly divided into splits for training, validation and testing.\n",
    "\n",
    "This notebook shows how a single Table can be divided into two or more such splits, with different strategies for how to split the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tlc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tlc_tools.split import split_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup\n",
    "\n",
    "We will reuse the table from the notebook [create-image-classification-table.ipynb](../1-create-tables/create-image-classification-table.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"cats-and-dogs\"\n",
    "project_name = \"3LC Tutorials\"\n",
    "table_name = \"initial\"\n",
    "\n",
    "table = tlc.Table.from_names(    \n",
    "    table_name=\"initial\",\n",
    "    dataset_name=dataset_name,\n",
    "    project_name=project_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random splitting\n",
    "\n",
    "A simple strategy is to shuffle the data, and then randomly split the data. We use the function split_table from the tlc_tools package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_splits = split_table(\n",
    "    table,\n",
    "    splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2},\n",
    "    split_strategy=\"random\",\n",
    "    shuffle=True,\n",
    "    random_seed=1,\n",
    ")\n",
    "\n",
    "random_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the distribution of the classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, table_split in random_splits.items():\n",
    "    num_dogs = sum(1 for row in table_split if row[1] == 1)\n",
    "    num_cats = len(table_split) - num_dogs\n",
    "    print(f\"{split_name} - dogs: {num_dogs}, cats: {num_cats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified sampling\n",
    "\n",
    "One problem with random sampling is there is no guarantee the distribution of classes is consistent across classes. Notice how there are no cats in the test set! This is where stratified sampling comes in. In this case, the data is sampled such that the fraction of each class (or some other property of a row) is consistent across the splits.\n",
    "\n",
    "Note that to use stratified sampling, we need to specify which column or property to split by. Here we select 1, which means the second element in a given row (which is the class index for this dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_stratified = split_table(\n",
    "    table=table,\n",
    "    splits={\"train\": 0.7, \"val\": 0.3},\n",
    "    split_strategy=\"stratified\",\n",
    "    split_by=1, # Each row is a tuple, we want to split by the second element, the class index\n",
    ")\n",
    "\n",
    "splits_stratified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that each split has both dogs and cats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, table_split in splits_stratified.items():\n",
    "    num_dogs = sum(1 for row in table_split if row[1] == 1)\n",
    "    num_cats = len(table_split) - num_dogs\n",
    "    print(f\"{split_name} - dogs: {num_dogs}, cats: {num_cats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling by Traversal Index\n",
    "\n",
    "While stratified sampling is a good way of ensuring a consistent distribution of each class, many datasets have further imbalances inherent in the samples. One such example is a dataset where a small subset of images are taken at night, and we would like to ensure that each split gets some night-time images. We would like to ensure that such properties are also considered when splitting our dataset, and this is where sampling by traversal index comes in.\n",
    "\n",
    "In order to sample by traversal index, we need to point at a column with embeddings. This could be from a pretrained model such as in [add-embeddings](add-embeddings), or with your own model. From this, the splits are created such that they are stratified with respect to the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need a table with an embeddings column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_with_embeddings = tlc.Table.from_names(\n",
    "    project_name=\"3LC Tutorials\",\n",
    "    dataset_name=\"COCO128\",\n",
    "    table_name=\"reduced_0000\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_with_embeddings = tlc.Table.from_url(\"/Users/frederik/Library/Application Support/3LC/projects/TACO/datasets/taco/tables/reduced_0000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_with_embeddings[0][\"embedding_pacmap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_traversal_index = split_table(\n",
    "    table_with_embeddings,\n",
    "    splits={\"train\": 0.8, \"val\": 0.1, \"test\": 0.1},\n",
    "    split_strategy=\"traversal_index\",\n",
    "    split_by=\"embedding_pacmap\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, table_split in splits_traversal_index.items():\n",
    "    print(f\"{split_name} - {len(table_split)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These splits can be visualized in the 3LC Dashboard, but let's also show them here in the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "for split_name, tbl in reversed(splits_traversal_index.items()):\n",
    "    embeddings = [row[\"embedding_pacmap\"] for row in tbl]\n",
    "    plt.scatter(x=[x[0] for x in embeddings], y=[x[1] for x in embeddings], label=split_name)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
