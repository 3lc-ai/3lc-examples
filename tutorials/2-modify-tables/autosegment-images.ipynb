{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Auto-segment images using SAM with grid prompting\n",
        "\n",
        "This notebook creates a 3LC Table with auto-generated segmentation masks from SAM using grid-based point prompting. Unlike the bounding box-based approach, this method automatically discovers objects in images without requiring ground truth annotations.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tlc\n",
        "import torch\n",
        "from PIL import Image\n",
        "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
        "\n",
        "from tlc_tools.common import infer_torch_device\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project and model configuration\n",
        "PROJECT_NAME = \"3LC Tutorials\"\n",
        "DATASET_NAME = \"AutoSegmented Images\"\n",
        "MODEL_TYPE = \"vit_b\"\n",
        "CHECKPOINT = \"../../transient_data/sam_vit_b_01ec64.pth\"\n",
        "\n",
        "# Image dataset configuration\n",
        "IMAGE_DIR = \"../../data/coco128/images\"\n",
        "MAX_IMAGES = 20  # Limit for initial testing - set to None for all images\n",
        "\n",
        "# Segmentation filtering parameters\n",
        "MIN_AREA_THRESHOLD = 1000  # Minimum area in pixels to keep a segment\n",
        "MIN_CONFIDENCE_THRESHOLD = 0.7  # Minimum confidence score to keep a segment\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Load Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20 images to process\n",
            "Sample images: ['000000000009.jpg', '000000000025.jpg', '000000000030.jpg', '000000000034.jpg', '000000000036.jpg']\n"
          ]
        }
      ],
      "source": [
        "# Get list of image files\n",
        "image_dir = Path(IMAGE_DIR)\n",
        "image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
        "image_files = [f for f in image_dir.glob('*') if f.suffix.lower() in image_extensions]\n",
        "\n",
        "if MAX_IMAGES is not None:\n",
        "    image_files = image_files[:MAX_IMAGES]\n",
        "\n",
        "print(f\"Found {len(image_files)} images to process\")\n",
        "print(f\"Sample images: {[f.name for f in image_files[:5]]}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Initialize SAM Automatic Mask Generator\n",
        "\n",
        "We'll use SAM's `SamAutomaticMaskGenerator` which uses automatic point grid prompting to segment all objects in each image without requiring any input prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized SAM Automatic Mask Generator on device: cuda\n",
            "Using model type: vit_b\n",
            "Checkpoint: ../../transient_data/sam_vit_b_01ec64.pth\n"
          ]
        }
      ],
      "source": [
        "# Load SAM model\n",
        "device = infer_torch_device()\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT)\n",
        "sam.to(device=device)\n",
        "\n",
        "# Initialize the automatic mask generator\n",
        "mask_generator = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=32,  # Grid size for point prompts\n",
        "    pred_iou_thresh=0.86,  # IoU threshold for mask quality filtering\n",
        "    stability_score_thresh=0.92,  # Stability score threshold\n",
        "    crop_n_layers=1,  # Number of crop layers\n",
        "    crop_n_points_downscale_factor=2,  # Downscale factor for crop points\n",
        "    min_mask_region_area=MIN_AREA_THRESHOLD,  # Minimum area in pixels\n",
        ")\n",
        "\n",
        "print(f\"Initialized SAM Automatic Mask Generator on device: {device}\")\n",
        "print(f\"Using model type: {MODEL_TYPE}\")\n",
        "print(f\"Checkpoint: {CHECKPOINT}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Process Images and Generate Masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing images:   0%|          | 0/20 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'cv2' has no attribute 'imread'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m segmentations_data = []\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m tqdm(image_files, desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing images\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     image = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m(\u001b[38;5;28mstr\u001b[39m(image_path))\n\u001b[32m      9\u001b[39m     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Generate masks for this image\u001b[39;00m\n",
            "\u001b[31mAttributeError\u001b[39m: module 'cv2' has no attribute 'imread'"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Process each image and collect segmentations\n",
        "segmentations_data = []\n",
        "\n",
        "for image_path in tqdm(image_files, desc=\"Processing images\"):\n",
        "    # Load image\n",
        "    image = cv2.imread(str(image_path))\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Generate masks for this image\n",
        "    masks = mask_generator.generate(image_rgb)\n",
        "    \n",
        "    # Filter masks by confidence/stability score if desired\n",
        "    filtered_masks = [\n",
        "        mask for mask in masks \n",
        "        if mask.get('stability_score', 0) >= MIN_CONFIDENCE_THRESHOLD\n",
        "    ]\n",
        "    \n",
        "    print(f\"{image_path.name}: Generated {len(masks)} masks, kept {len(filtered_masks)} after filtering\")\n",
        "    \n",
        "    if filtered_masks:\n",
        "        # Convert masks to the format expected by 3LC\n",
        "        h, w = image_rgb.shape[:2]\n",
        "        \n",
        "        # Stack all masks for this image into a single array\n",
        "        mask_array = np.stack([mask['segmentation'] for mask in filtered_masks], axis=2).astype(np.uint8)\n",
        "        \n",
        "        # Create instance properties (scores, areas, etc.)\n",
        "        instance_properties = {\n",
        "            'score': [mask['stability_score'] for mask in filtered_masks],\n",
        "            'area': [mask['area'] for mask in filtered_masks],\n",
        "            'predicted_iou': [mask['predicted_iou'] for mask in filtered_masks],\n",
        "        }\n",
        "        \n",
        "        segmentation_data = {\n",
        "            'image_height': h,\n",
        "            'image_width': w,\n",
        "            'masks': mask_array,\n",
        "            'instance_properties': instance_properties,\n",
        "        }\n",
        "        \n",
        "        row_data = {\n",
        "            'image': str(image_path),\n",
        "            'segments': segmentation_data,\n",
        "        }\n",
        "        \n",
        "        segmentations_data.append(row_data)\n",
        "\n",
        "print(f\"\\\\nProcessed {len(segmentations_data)} images with valid segmentations\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Create 3LC Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the 3LC table with auto-generated segmentations\n",
        "table_writer = tlc.TableWriter(\n",
        "    project_name=PROJECT_NAME,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    column_schemas={\n",
        "        'image': tlc.Schema(value=tlc.ImageUrlStringValue()),\n",
        "        'segments': tlc.InstanceSegmentationMasks(\n",
        "            'auto-generated segments',\n",
        "            instance_properties_structure={\n",
        "                'score': tlc.Schema(value=tlc.Float32Value(0, 1), writable=False),\n",
        "                'area': tlc.Schema(value=tlc.Int32Value(), writable=False),\n",
        "                'predicted_iou': tlc.Schema(value=tlc.Float32Value(0, 1), writable=False),\n",
        "            },\n",
        "        ),\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add all the segmentation data to the table\n",
        "for row_data in segmentations_data:\n",
        "    table_writer.add_row(row_data)\n",
        "\n",
        "# Finalize the table\n",
        "table = table_writer.finalize()\n",
        "\n",
        "print(f\"\\\\nCreated 3LC table: {table.name}\")\n",
        "print(f\"Table URL: {table.url}\")\n",
        "print(f\"Total rows: {len(table)}\")\n",
        "\n",
        "# Display some statistics\n",
        "total_segments = sum(len(row['segments']['instance_properties']['score']) for row in segmentations_data)\n",
        "avg_segments_per_image = total_segments / len(segmentations_data) if segmentations_data else 0\n",
        "\n",
        "print(f\"\\\\nSegmentation Statistics:\")\n",
        "print(f\"Total segments collected: {total_segments}\")\n",
        "print(f\"Average segments per image: {avg_segments_per_image:.1f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Visualize Sample Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.colors import ListedColormap\n",
        "import random\n",
        "\n",
        "if len(table) > 0:\n",
        "    # Get a sample from the table\n",
        "    sample_idx = 0\n",
        "    sample = table[sample_idx]\n",
        "    \n",
        "    # Load the original image\n",
        "    image_path = sample['image']\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Get the segmentation masks\n",
        "    masks = sample['segments']['masks']\n",
        "    scores = sample['segments']['instance_properties']['score']\n",
        "    areas = sample['segments']['instance_properties']['area']\n",
        "    \n",
        "    print(f\"Sample image: {Path(image_path).name}\")\n",
        "    print(f\"Number of segments: {masks.shape[2]}\")\n",
        "    print(f\"Score range: {min(scores):.3f} - {max(scores):.3f}\")\n",
        "    print(f\"Area range: {min(areas)} - {max(areas)} pixels\")\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    \n",
        "    # Original image\n",
        "    axes[0].imshow(image_rgb)\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # All masks overlay\n",
        "    axes[1].imshow(image_rgb)\n",
        "    combined_mask = np.zeros((masks.shape[0], masks.shape[1]))\n",
        "    colors = plt.cm.tab20(np.linspace(0, 1, min(20, masks.shape[2])))\n",
        "    \n",
        "    for i in range(min(masks.shape[2], 20)):  # Show up to 20 masks\n",
        "        mask = masks[:, :, i]\n",
        "        if mask.sum() > 0:\n",
        "            # Create colored overlay\n",
        "            colored_mask = np.zeros((*mask.shape, 4))\n",
        "            colored_mask[mask == 1] = colors[i % len(colors)]\n",
        "            axes[1].imshow(colored_mask, alpha=0.7)\n",
        "    \n",
        "    axes[1].set_title(f\"All Segments Overlay ({min(masks.shape[2], 20)} shown)\")\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    # Individual high-quality masks\n",
        "    axes[2].imshow(image_rgb)\n",
        "    # Show only the top 5 masks by score\n",
        "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:5]\n",
        "    \n",
        "    for i, mask_idx in enumerate(top_indices):\n",
        "        mask = masks[:, :, mask_idx]\n",
        "        if mask.sum() > 0:\n",
        "            colored_mask = np.zeros((*mask.shape, 4))\n",
        "            colored_mask[mask == 1] = colors[i % len(colors)]\n",
        "            axes[2].imshow(colored_mask, alpha=0.8)\n",
        "    \n",
        "    axes[2].set_title(\"Top 5 Segments by Score\")\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No images processed successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
