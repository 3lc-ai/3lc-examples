{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- For reference:\n",
    "\n",
    "from ultralytics.data.annotator import auto_annotate\n",
    "from ultralytics.models.sam import Predictor as SAMPredictor\n",
    "from ultralytics import SAM -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tlc\n",
    "import torch\n",
    "import tqdm\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "from tlc_tools.common import infer_torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"3LC Tutorials\"\n",
    "MODEL_TYPE = \"vit_b\"\n",
    "MODEL_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
    "CHECKPOINT = \"../../transient_data/sam_vit_b_01ec64.pth\"\n",
    "EMBEDDING_DIM = 3\n",
    "REDUCTION_METHOD = \"umap\"\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = infer_torch_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbs_to_segments(\n",
    "    input_table: tlc.Table,\n",
    "    sam_model_type: str = MODEL_TYPE,\n",
    "    checkpoint: str = CHECKPOINT,\n",
    "):\n",
    "    sam_model = sam_model_registry[sam_model_type](checkpoint=checkpoint)\n",
    "    sam_model.to(device)\n",
    "    sam_model.eval()\n",
    "\n",
    "    sam_predictor = SamPredictor(sam_model)\n",
    "\n",
    "    value_map = input_table.get_value_map(\"bbs.bb_list.label\")\n",
    "\n",
    "    table_writer = tlc.TableWriter(\n",
    "        f\"{input_table.name}-sam\",\n",
    "        input_table.dataset_name,\n",
    "        input_table.project_name,\n",
    "        description=\"Added SAM-segmentations from bounding box prompts\",\n",
    "        column_schemas={\n",
    "            \"image\": tlc.ImageUrlStringValue(),\n",
    "            \"segments\": tlc.InstanceSegmentationMasks(\n",
    "                \"segments\",\n",
    "                instance_properties_structure={\n",
    "                    \"label\": tlc.CategoricalLabel(\"label\", classes=value_map),\n",
    "                    \"score\": tlc.Schema(\n",
    "                        value=tlc.Float32Value(0, 1), description=\"Confidence score (predicted IoU) of the segmentation\"\n",
    "                    ),\n",
    "                },\n",
    "                is_prediction=True,\n",
    "            ),\n",
    "        },\n",
    "        input_tables=[input_table.url],\n",
    "    )\n",
    "\n",
    "    for row in tqdm.tqdm(input_table, desc=\"Processing rows\", total=len(input_table)):\n",
    "        image = cv2.imread(row[\"image\"])\n",
    "        sam_predictor.set_image(image)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for bb in row[\"bbs\"][\"bb_list\"]:\n",
    "            box_arr = np.array([bb[\"x0\"], bb[\"y0\"], bb[\"x0\"] + bb[\"x1\"], bb[\"y0\"] + bb[\"y1\"]])\n",
    "            boxes.append(box_arr)\n",
    "            labels.append(bb[\"label\"])\n",
    "\n",
    "        boxes = np.array(boxes)\n",
    "\n",
    "        # Call Predictor's predict_torch instead of predict, to allow multiple box prompts\n",
    "        if boxes.size > 0:\n",
    "            boxes = sam_predictor.transform.apply_boxes(boxes, sam_predictor.original_size)\n",
    "            boxes_torch = torch.as_tensor(boxes, dtype=torch.float, device=device)\n",
    "\n",
    "            masks, scores, _ = sam_predictor.predict_torch(\n",
    "                None,\n",
    "                None,\n",
    "                boxes=boxes_torch,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "\n",
    "            # Convert masks from (num_bbs, 1, h, w) to (h, w, num_bbs)\n",
    "            segments = np.asfortranarray(masks.squeeze(1).permute(1, 2, 0).cpu().numpy().astype(np.uint8))\n",
    "            scores = scores.squeeze(1).cpu().numpy().tolist()\n",
    "        else:\n",
    "            segments = np.zeros((0, 0, 0), dtype=np.uint8)\n",
    "            scores = []\n",
    "\n",
    "        output_row = {\n",
    "            \"image\": row[\"image\"],\n",
    "            \"segments\": {\n",
    "                \"image_height\": image.shape[0],\n",
    "                \"image_width\": image.shape[1],\n",
    "                \"masks\": segments,\n",
    "                \"instance_properties\": {\n",
    "                    \"label\": labels,\n",
    "                    \"score\": scores,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "        table_writer.add_row(output_row)\n",
    "\n",
    "    out_table = table_writer.finalize()\n",
    "    return out_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = tlc.Table.from_names(\"train\", \"Aerial-aerial-sheep\", \"RF100VL\")\n",
    "table = tlc.Table.from_names(\"initial\", \"COCO128\", \"3LC Tutorials\")\n",
    "out_table = bbs_to_segments(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
