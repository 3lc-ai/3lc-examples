{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Create a SAM-ready 3LC Table using TableWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook is a pre-requisite for running the `fine-tune-sam.ipynb` notebook, and is a modified version of the official colab tutorial from Encord which can be found [here](https://colab.research.google.com/drive/1F6uRommb3GswcRlPZWpkAQRMVNdVH7Ww).\n",
    "\n",
    "It demonstrates how you can manually create a 3LC Table by iteratively adding its rows. By using a 3LC Table, as opposed to some other data format, you will be able to view and edit the samples of the dataset in the 3LC Dashboard. The Table created can be used to fine-tune Segment Anything Model (SAM), and is created from the Stamp Verification (StaVer) dataset on kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tlc\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Project setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"3LC Tutorials\"\n",
    "DATASET_NAME = \"staver-dataset\"\n",
    "STAVER_DATASET_PATH = \"../../../transient_data/stamp-verification-staver-dataset\"  # Path to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Downloading StaVer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to set up your kaggle API key, see https://github.com/Kaggle/kaggle-api#api-credentials\n",
    "# You can also manually download the dataset from https://www.kaggle.com/rtatman/stamp-verification-staver-dataset\n",
    "!mkdir $STAVER_DATASET_PATH\n",
    "!kaggle datasets download rtatman/stamp-verification-staver-dataset \n",
    "!unzip -o stamp-verification-staver-dataset.zip -d $STAVER_DATASET_PATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"stamp-verification-staver-dataset.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Create Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAVER_DATASET = Path(STAVER_DATASET_PATH).absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bboxes():\n",
    "    # Exclude scans with zero or multiple bboxes (of the first 100)\n",
    "    stamps_to_exclude = {\n",
    "        \"stampDS-00008\",\n",
    "        \"stampDS-00010\",\n",
    "        \"stampDS-00015\",\n",
    "        \"stampDS-00021\",\n",
    "        \"stampDS-00027\",\n",
    "        \"stampDS-00031\",\n",
    "        \"stampDS-00039\",\n",
    "        \"stampDS-00041\",\n",
    "        \"stampDS-00049\",\n",
    "        \"stampDS-00053\",\n",
    "        \"stampDS-00059\",\n",
    "        \"stampDS-00069\",\n",
    "        \"stampDS-00073\",\n",
    "        \"stampDS-00080\",\n",
    "        \"stampDS-00090\",\n",
    "        \"stampDS-00098\",\n",
    "        \"stampDS-00100\",\n",
    "    }.union(\n",
    "        {\n",
    "            \"stampDS-00012\",\n",
    "            \"stampDS-00013\",\n",
    "            \"stampDS-00014\",\n",
    "        }\n",
    "    )  # Exclude 3 scans that aren't the type of scan we want to be fine tuning for\n",
    "\n",
    "    bbox_coords = {}\n",
    "    for f in sorted((STAVER_DATASET / \"ground-truth-maps\" / \"ground-truth-maps\").iterdir())[:100]:\n",
    "        k = f.stem[:-3]\n",
    "        if k not in stamps_to_exclude:\n",
    "            im = cv2.imread(f.as_posix())\n",
    "            gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "            contours, _ = cv2.findContours(gray, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "            if len(contours) > 1:\n",
    "                x, y, w, h = cv2.boundingRect(contours[0])\n",
    "                height, width, _ = im.shape\n",
    "                bbox_coords[k] = {\n",
    "                    \"image_height\": height,\n",
    "                    \"image_width\": width,\n",
    "                    \"bb_list\": [{\"x0\": float(x), \"x1\": float(x + w), \"y0\": float(y), \"y1\": float(y + h), \"label\": 0}],\n",
    "                }\n",
    "\n",
    "    return bbox_coords\n",
    "\n",
    "\n",
    "def create_schema():\n",
    "    schema = tlc.SampleType.from_structure(\n",
    "        {\n",
    "            \"image\": tlc.ImagePath,\n",
    "            \"mask\": tlc.Schema(\n",
    "                value=tlc.SegmentationMaskUrlStringValue(\n",
    "                    map={\n",
    "                        255.0: tlc.MapElement(\"background\"),\n",
    "                        0.0: tlc.MapElement(\"stamp\"),\n",
    "                    }\n",
    "                )\n",
    "            ),\n",
    "            \"My parameter A\": tlc.Float,\n",
    "            \"My parameter B\": tlc.Float,\n",
    "        }\n",
    "    ).schema\n",
    "\n",
    "    schema.add_sub_schema(\n",
    "        \"prompt box\", tlc.BoundingBoxListSchema({0.0: tlc.MapElement(\"Stamp\")}, include_segmentation=False)\n",
    "    )\n",
    "\n",
    "    return schema\n",
    "\n",
    "\n",
    "def create_table():\n",
    "    table_writer = tlc.TableWriter(\n",
    "        project_name=PROJECT_NAME,\n",
    "        dataset_name=DATASET_NAME,\n",
    "        column_schemas=create_schema().values,\n",
    "    )\n",
    "\n",
    "    bboxes = create_bboxes()\n",
    "\n",
    "    for key in bboxes:\n",
    "        table_writer.add_row(\n",
    "            {\n",
    "                \"image\": (STAVER_DATASET / \"scans\" / \"scans\" / f\"{key}.png\").as_posix(),\n",
    "                \"mask\": (STAVER_DATASET / \"ground-truth-pixel\" / \"ground-truth-pixel\" / f\"{key}-px.png\").as_posix(),\n",
    "                \"prompt box\": bboxes[key],\n",
    "                \"My parameter A\": random(),\n",
    "                \"My parameter B\": random(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    table = table_writer.finalize()\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(table[0][\"mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.array(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
