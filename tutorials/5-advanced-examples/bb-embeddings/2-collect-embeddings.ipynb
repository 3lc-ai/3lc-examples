{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect and Reduce Classifier Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will use an existing classifier model to generate per-bounding-box embeddings for a COCO-style object detection dataset. We will then reduce these embeddings to 3D using PaCMAP.\n",
    "\n",
    "To run this notebook, you must also have run:\n",
    "* [1-fine-tune-on-crops.ipynb](https://github.com/3lc-ai/3lc-examples/blob/main/tutorials/bb-embeddings/1-fine-tune-on-crops.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import pacmap\n",
    "import timm\n",
    "import tlc\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from tlc_tools.common import infer_torch_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"3LC Tutorials\"\n",
    "TRANSIENT_DATA_PATH = \"../../../transient_data\"\n",
    "EMBEDDING_SAVE_PATH = TRANSIENT_DATA_PATH + \"/bb_classifier_embeddings.npy\"\n",
    "LABELS_SAVE_PATH = TRANSIENT_DATA_PATH + \"/bb_classifier_labels.npy\"\n",
    "MODEL_CHECKPOINT = TRANSIENT_DATA_PATH + \"/bb_classifier.pth\"\n",
    "MODEL_NAME = \"efficientnet_b0\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_COMPONENTS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = infer_torch_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Input Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Table used in the previous notebook\n",
    "input_table = tlc.Table.from_names(\n",
    "    table_name=\"initial\",\n",
    "    dataset_name=\"COCO128\", \n",
    "    project_name=PROJECT_NAME, \n",
    ")\n",
    "\n",
    "# Get the schema of the bounding box list, required to crop\n",
    "bb_schema = input_table.rows_schema.values[\"bbs\"].values[\"bb_list\"]\n",
    "assert bb_schema\n",
    "NUM_CLASSES = len(bb_schema.values[\"label\"].value.map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model trained in the previous notebook\n",
    "model = timm.create_model(\n",
    "    MODEL_NAME, \n",
    "    num_classes=NUM_CLASSES, \n",
    "    checkpoint_path=MODEL_CHECKPOINT,\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# The hidden layer whose activations we will use for embeddings\n",
    "hidden_layer = model.global_pool.flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transformation to apply to the image before feeding it to the model\n",
    "image_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# An iterator over every transformed BB crop in a single sample (image)\n",
    "def single_sample_bb_crop_iterator(sample):\n",
    "    image_filename = sample[\"image\"]\n",
    "    image_bytes = tlc.Url(image_filename).to_absolute(input_table.url).read()\n",
    "    image = Image.open(BytesIO(image_bytes))\n",
    "    w, h = image.size\n",
    "\n",
    "    for bb in sample[\"bbs\"][\"bb_list\"]:\n",
    "        bb_crop = tlc.BBCropInterface.crop(image, bb, bb_schema, h, w)\n",
    "        yield image_transform(bb_crop)\n",
    "\n",
    "# An iterator over every transformed BB crop in the dataset\n",
    "def bb_crop_iterator():\n",
    "    for sample in input_table:\n",
    "        for bb_crop in single_sample_bb_crop_iterator(sample):\n",
    "            yield bb_crop\n",
    "\n",
    "# A batched iterator over every transformed BB crop in the dataset\n",
    "def batched_bb_crop_iterator():\n",
    "    batch = []\n",
    "    for bb_crop in bb_crop_iterator():\n",
    "        batch.append(bb_crop)\n",
    "        if len(batch) == BATCH_SIZE:\n",
    "            yield torch.stack(batch).to(device)\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield torch.stack(batch).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a model hook which saves activations to output_list\n",
    "output_list = []\n",
    "def hook_fn(module, input, output):\n",
    "    output_list.append(output.cpu())\n",
    "\n",
    "hook_handle = hidden_layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our iterator to run the model on every BB crop in the dataset\n",
    "labels_list = []\n",
    "for batch in tqdm(batched_bb_crop_iterator(), desc=\"Running model inference\"):\n",
    "    with torch.no_grad():\n",
    "        output = model(batch)\n",
    "        predicted_labels = torch.argmax(output, dim=1)\n",
    "        labels_list.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the embeddings and labels into a single numpy array\n",
    "embeddings = np.vstack(output_list)\n",
    "labels = np.array(labels_list)\n",
    "\n",
    "# Reduce the 1280-dimensional activations to NUM_COMPONENTS using pacmap\n",
    "reducer = pacmap.PaCMAP(n_components=NUM_COMPONENTS)\n",
    "embeddings_nd = reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings for use in the next notebook(s)\n",
    "np.save(EMBEDDING_SAVE_PATH, embeddings_nd)\n",
    "np.save(LABELS_SAVE_PATH, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
