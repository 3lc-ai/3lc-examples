{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect and Reduce Classifier Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will use an existing classifier model to generate per-bounding-box embeddings for a COCO-style object detection dataset. We will then reduce these embeddings to 3D using PaCMAP.\n",
    "\n",
    "To run this notebook, you must also have run:\n",
    "* [1-fine-tune-on-crops.ipynb](https://github.com/3lc-ai/3lc-examples/blob/main/tutorials/bb-embeddings/1-fine-tune-on-crops.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"3LC Tutorials\"\n",
    "TRANSIENT_DATA_PATH = \"../../../transient_data\"\n",
    "BATCH_SIZE = 32\n",
    "DATASET_NAME = \"COCO128\"\n",
    "DEVICE = None\n",
    "INSTALL_DEPENDENCIES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if INSTALL_DEPENDENCIES:\n",
    "    %pip --quiet install pacmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import pacmap\n",
    "import timm\n",
    "import tlc\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVICE is None:\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "else:\n",
    "    device = DEVICE\n",
    "\n",
    "device = torch.device(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Input Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Table created in the previous notebook\n",
    "input_table = tlc.Table.from_names(\n",
    "    project_name=PROJECT_NAME, \n",
    "    dataset_name=DATASET_NAME, \n",
    "    table_name=\"initial\"\n",
    ")\n",
    "\n",
    "# Get the schema of the bounding box list, required to crop\n",
    "bb_schema = input_table.rows_schema.values[\"bbs\"].values[\"bb_list\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model trained in the previous notebook\n",
    "model = timm.create_model(\n",
    "    \"efficientnet_b0\", \n",
    "    num_classes=81, \n",
    "    checkpoint_path=TRANSIENT_DATA_PATH + \"/bb_classifier.pth\"\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# The hidden layer whose activations we will use for embeddings\n",
    "hidden_layer = model.global_pool.flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transformation to apply to the image before feeding it to the model\n",
    "image_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# An iterator over every transformed BB crop in a single sample (image)\n",
    "def single_sample_bb_crop_iterator(sample):\n",
    "    image_filename = sample[\"image\"]\n",
    "    image_bytes = tlc.Url(image_filename).read()\n",
    "    image = Image.open(BytesIO(image_bytes))\n",
    "    w, h = image.size\n",
    "\n",
    "    for bb in sample[\"bbs\"][\"bb_list\"]:\n",
    "        bb_crop = tlc.BBCropInterface.crop(image, bb, bb_schema, h, w)\n",
    "        yield image_transform(bb_crop)\n",
    "\n",
    "# An iterator over every transformed BB crop in the dataset\n",
    "def bb_crop_iterator():\n",
    "    for sample in input_table:\n",
    "        for bb_crop in single_sample_bb_crop_iterator(sample):\n",
    "            yield bb_crop\n",
    "\n",
    "# A batched iterator over every transformed BB crop in the dataset\n",
    "def batched_bb_crop_iterator():\n",
    "    batch = []\n",
    "    for bb_crop in bb_crop_iterator():\n",
    "        batch.append(bb_crop)\n",
    "        if len(batch) == BATCH_SIZE:\n",
    "            yield torch.stack(batch).to(device)\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield torch.stack(batch).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a model hook which saves activations to output_list\n",
    "output_list = []\n",
    "def hook_fn(module, input, output):\n",
    "    output_list.append(output.cpu())\n",
    "\n",
    "hook_handle = hidden_layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our iterator to run the model on every BB crop in the dataset\n",
    "for batch in tqdm(batched_bb_crop_iterator(), desc=\"Running model inference\"):\n",
    "    with torch.no_grad():\n",
    "        model(batch)\n",
    "\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the embeddings into a single numpy array\n",
    "embeddings = np.vstack(output_list)\n",
    "\n",
    "# Reduce the 1280-dimensional activations to 3D using pacmap\n",
    "reducer = pacmap.PaCMAP(n_components=3)\n",
    "embeddings_3d = reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings for use in the next notebook(s)\n",
    "np.save(TRANSIENT_DATA_PATH + \"/bb_classifier_embeddings.npy\", embeddings_3d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
