{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a Classifier Using Bounding Box Data from a 3LC Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will fine-tune a classifier using bounding box data from a 3LC `Table`.\n",
    "\n",
    "We will load the COCO128 table from an earlier notebook and use it to create a\n",
    "`torch.utils.Dataset` of bounding box crops. These cropped images will be used to\n",
    "fine-tune a classifier. In a later tutorial, we will use this trained model to\n",
    "generate embeddings and predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "import tlc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm.notebook as tqdm\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "from tlc_tools.common import infer_torch_device\n",
    "from tlc_tools.datasets import BBCropDataset\n",
    "from tlc_tools.split import split_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "IMAGES_PER_EPOCH = 1000\n",
    "MODEL_CHECKPOINT = \"../../../transient_data/bb_classifier.pth\"\n",
    "MODEL_NAME = \"efficientnet_b0\"\n",
    "BATCH_SIZE = 32\n",
    "INCLUDE_BACKGROUND = False\n",
    "X_MAX_OFFSET = 0.1\n",
    "Y_MAX_OFFSET = 0.1\n",
    "X_SCALE_RANGE = (0.9, 1.1)\n",
    "Y_SCALE_RANGE = (0.9, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = infer_torch_device()\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Input Table\n",
    "\n",
    "We will reuse the table created in the notebook [create-table-from-coco.ipynb](../../1-create-tables/create-table-from-coco.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table = tlc.Table.from_names(\n",
    "    \"initial\",\n",
    "    \"COCO128\",\n",
    "    \"3LC Tutorials\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the schema of the bounding box column of the input table\n",
    "bb_schema = input_table.schema.values[\"rows\"].values[\"bbs\"].values[\"bb_list\"]\n",
    "label_map = input_table.get_simple_value_map(\"bbs.bb_list.label\")\n",
    "print(f\"Input table uses {len(label_map)} unique labels: {json.dumps(label_map, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Path(MODEL_CHECKPOINT).parent.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(label_map) + 1 if INCLUDE_BACKGROUND else len(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits for training and validation\n",
    "splits = split_table(input_table, {\"train\": 0.8, \"val\": 0.2})\n",
    "\n",
    "train_table = splits[\"train\"]\n",
    "val_table = splits[\"val\"]\n",
    "\n",
    "print(f\"Using table {train_table} for training\")\n",
    "print(f\"Using table {val_table} for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations to be applied to the images\n",
    "common_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.3),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        *common_transforms.transforms,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the datasets and dataloader\n",
    "train_dataset = BBCropDataset(\n",
    "    train_table,\n",
    "    transform=train_transforms,\n",
    "    add_background=INCLUDE_BACKGROUND,\n",
    "    is_train=True,\n",
    "    x_max_offset=X_MAX_OFFSET,\n",
    "    y_max_offset=Y_MAX_OFFSET,\n",
    "    x_scale_range=X_SCALE_RANGE,\n",
    "    y_scale_range=Y_SCALE_RANGE,\n",
    ")\n",
    "\n",
    "val_dataset = BBCropDataset(\n",
    "    val_table,\n",
    "    transform=common_transforms,\n",
    "    add_background=False,\n",
    "    is_train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_image_grid(images, labels, rows: int, cols: int, title: str):\n",
    "    assert len(images) == len(labels), \"Number of images and labels must be the same.\"\n",
    "    assert len(images) <= rows * cols, \"Not enough space in the grid for all images.\"\n",
    "\n",
    "    unnormalize = transforms.Normalize(\n",
    "        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],  # Reverse normalization\n",
    "        std=[1 / 0.229, 1 / 0.224, 1 / 0.225],\n",
    "    )\n",
    "    to_pil = transforms.ToPILImage()\n",
    "\n",
    "    _, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, (image, label) in enumerate(zip(images, labels)):\n",
    "        # Unnormalize the tensor image\n",
    "        image = unnormalize(image)\n",
    "        pil_image = to_pil(image)\n",
    "\n",
    "        # Plot the image\n",
    "        axes[idx].imshow(pil_image)\n",
    "        axes[idx].set_title(label)\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for ax in axes[len(images) :]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "for i in range(4 * 3):\n",
    "    image, label = train_dataset[i]\n",
    "    train_images.append(image)\n",
    "    train_labels.append(label_map.get(label.item(), \"Background\"))\n",
    "\n",
    "write_image_grid(train_images, train_labels, 4, 3, \"Training Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = []\n",
    "val_labels = []\n",
    "for i in range(4 * 3):\n",
    "    image, label = val_dataset[i]\n",
    "    val_images.append(image)\n",
    "    val_labels.append(label_map[label.item()])\n",
    "\n",
    "write_image_grid(val_images, val_labels, 4, 3, \"Validation Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3LC Run\n",
    "run = tlc.init(project_name=input_table.project_name, run_name=\"Train Bounding Box Classifier\")\n",
    "\n",
    "# Create sampler\n",
    "num_bbs_per_image = [len(row[\"bbs\"][\"bb_list\"]) for row in train_table.table_rows]\n",
    "sampler = WeightedRandomSampler(weights=num_bbs_per_image, num_samples=IMAGES_PER_EPOCH)\n",
    "\n",
    "# Create the dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Load an EfficientNet model using timm\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9516)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "    for inputs, labels in tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    train_loss /= train_total\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm.tqdm(val_dataloader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss /= val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    tlc.log(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "run.set_status_completed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a pth file:\n",
    "torch.save(model.state_dict(), MODEL_CHECKPOINT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
