{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a Classifier Using Bounding Box Data from a 3LC Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will fine-tune a classifier using bounding box data from a 3LC `Table`.\n",
    "\n",
    "The Table will initially be created from a COCO-style dataset (Balloons), and we will\n",
    "use 3LC to generate a cropped image for every bounding box in the dataset. These\n",
    "cropped images will be used to fine-tune a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"3LC Tutorials\"\n",
    "EPOCHS = 10\n",
    "IMAGES_PER_EPOCH = 100\n",
    "TEST_DATA_PATH = \"../../../data\"\n",
    "TRANSIENT_DATA_PATH = \"../../../transient_data\"\n",
    "BATCH_SIZE = 32\n",
    "DATASET_NAME = \"COCO128\"\n",
    "INSTALL_DEPENDENCIES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if INSTALL_DEPENDENCIES:\n",
    "    %pip --quiet install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "    %pip --quiet install torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "    %pip --quiet install timm\n",
    "    %pip --quiet install 3lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from io import BytesIO\n",
    "\n",
    "import timm\n",
    "import tlc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm.notebook as tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from tlc_tools.common import infer_torch_device\n",
    "\n",
    "device = infer_torch_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Input Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use the TableFromCoco we created in 1-create-tables/\n",
    "input_table = tlc.Table.from_names(\"initial\", \"COCO128\", \"3LC Tutorials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input table uses 80 unique labels: {\n",
      "  \"0\": \"person\",\n",
      "  \"1\": \"bicycle\",\n",
      "  \"2\": \"car\",\n",
      "  \"3\": \"motorcycle\",\n",
      "  \"4\": \"airplane\",\n",
      "  \"5\": \"bus\",\n",
      "  \"6\": \"train\",\n",
      "  \"7\": \"truck\",\n",
      "  \"8\": \"boat\",\n",
      "  \"9\": \"traffic light\",\n",
      "  \"10\": \"fire hydrant\",\n",
      "  \"11\": \"stop sign\",\n",
      "  \"12\": \"parking meter\",\n",
      "  \"13\": \"bench\",\n",
      "  \"14\": \"bird\",\n",
      "  \"15\": \"cat\",\n",
      "  \"16\": \"dog\",\n",
      "  \"17\": \"horse\",\n",
      "  \"18\": \"sheep\",\n",
      "  \"19\": \"cow\",\n",
      "  \"20\": \"elephant\",\n",
      "  \"21\": \"bear\",\n",
      "  \"22\": \"zebra\",\n",
      "  \"23\": \"giraffe\",\n",
      "  \"24\": \"backpack\",\n",
      "  \"25\": \"umbrella\",\n",
      "  \"26\": \"handbag\",\n",
      "  \"27\": \"tie\",\n",
      "  \"28\": \"suitcase\",\n",
      "  \"29\": \"frisbee\",\n",
      "  \"30\": \"skis\",\n",
      "  \"31\": \"snowboard\",\n",
      "  \"32\": \"sports ball\",\n",
      "  \"33\": \"kite\",\n",
      "  \"34\": \"baseball bat\",\n",
      "  \"35\": \"baseball glove\",\n",
      "  \"36\": \"skateboard\",\n",
      "  \"37\": \"surfboard\",\n",
      "  \"38\": \"tennis racket\",\n",
      "  \"39\": \"bottle\",\n",
      "  \"40\": \"wine glass\",\n",
      "  \"41\": \"cup\",\n",
      "  \"42\": \"fork\",\n",
      "  \"43\": \"knife\",\n",
      "  \"44\": \"spoon\",\n",
      "  \"45\": \"bowl\",\n",
      "  \"46\": \"banana\",\n",
      "  \"47\": \"apple\",\n",
      "  \"48\": \"sandwich\",\n",
      "  \"49\": \"orange\",\n",
      "  \"50\": \"broccoli\",\n",
      "  \"51\": \"carrot\",\n",
      "  \"52\": \"hot dog\",\n",
      "  \"53\": \"pizza\",\n",
      "  \"54\": \"donut\",\n",
      "  \"55\": \"cake\",\n",
      "  \"56\": \"chair\",\n",
      "  \"57\": \"couch\",\n",
      "  \"58\": \"potted plant\",\n",
      "  \"59\": \"bed\",\n",
      "  \"60\": \"dining table\",\n",
      "  \"61\": \"toilet\",\n",
      "  \"62\": \"tv\",\n",
      "  \"63\": \"laptop\",\n",
      "  \"64\": \"mouse\",\n",
      "  \"65\": \"remote\",\n",
      "  \"66\": \"keyboard\",\n",
      "  \"67\": \"cell phone\",\n",
      "  \"68\": \"microwave\",\n",
      "  \"69\": \"oven\",\n",
      "  \"70\": \"toaster\",\n",
      "  \"71\": \"sink\",\n",
      "  \"72\": \"refrigerator\",\n",
      "  \"73\": \"book\",\n",
      "  \"74\": \"clock\",\n",
      "  \"75\": \"vase\",\n",
      "  \"76\": \"scissors\",\n",
      "  \"77\": \"teddy bear\",\n",
      "  \"78\": \"hair drier\",\n",
      "  \"79\": \"toothbrush\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get the schema of the bounding box column of the input table\n",
    "bb_schema = input_table.schema.values[\"rows\"].values[\"bbs\"].values[\"bb_list\"]\n",
    "label_map = input_table.get_simple_value_map(\"bbs.bb_list.label\")\n",
    "print(f\"Input table uses {len(label_map)} unique labels: {json.dumps(label_map, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the number of bounding boxes per image to control sampling\n",
    "num_bbs_per_image = [len(row[\"bbs\"][\"bb_list\"]) for row in input_table.table_rows]\n",
    "sampler = WeightedRandomSampler(weights=num_bbs_per_image, num_samples=IMAGES_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BBCropDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        table: tlc.Table,\n",
    "        transform=None,\n",
    "        label_map=None,\n",
    "        add_background=False,\n",
    "        background_freq=0.5,\n",
    "        is_train=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom dataset for cropping bounding boxes and generating background patches.\n",
    "\n",
    "        Args:\n",
    "            table: tlc.Table, the input table containing image and bounding box data.\n",
    "            transform: callable, transformations to apply to cropped images.\n",
    "            label_map: dict, mapping from original labels to contiguous integer labels.\n",
    "            add_background: bool, whether to include background patches.\n",
    "            background_freq: float, probability of sampling a background patch.\n",
    "            is_train: bool, whether the dataset is used for training (affects background generation).\n",
    "            crop_func: callable, function to crop bounding boxes (defaults to tlc.BBCropInterface.crop).\n",
    "        \"\"\"\n",
    "        self.table = table\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map or table.get_value_map(\"bbs.bb_list.label\")\n",
    "        self.bb_schema = table.schema.values[\"rows\"].values[\"bbs\"].values[\"bb_list\"]\n",
    "        self.add_background = add_background\n",
    "        self.background_freq = background_freq\n",
    "        self.is_train = is_train\n",
    "        self.background_label = len(self.label_map) if add_background else None\n",
    "        self.random_gen = random.Random(42)  # Fixed seed for reproducibility\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table)  # Dataset length tied to the number of table rows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch a sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx: int, index provided by the sampler.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (cropped image, label) where label is a tensor.\n",
    "        \"\"\"\n",
    "        # Determine if a background patch should be generated\n",
    "        is_background = (\n",
    "            self.add_background and self.random_gen.random() < self.background_freq and self.is_train\n",
    "        )\n",
    "\n",
    "        # Select a random row for background or use the given index\n",
    "        if is_background:\n",
    "            row_idx = self.random_gen.randint(0, len(self.table) - 1)\n",
    "        else:\n",
    "            row_idx = idx\n",
    "\n",
    "        row = self.table.table_rows[row_idx]\n",
    "        image = self.load_image_data(row)\n",
    "        \n",
    "        bbs = row[\"bbs\"][\"bb_list\"]\n",
    "        while len(bbs) == 0 and not is_background:\n",
    "            row_idx = self.random_gen.randint(0, len(self.table) - 1)\n",
    "            row = self.table.table_rows[row_idx]\n",
    "            image = self.load_image_data(row)\n",
    "            bbs = row[\"bbs\"][\"bb_list\"]\n",
    "\n",
    "        if is_background:\n",
    "            crop, label = self.generate_background(image, bbs)\n",
    "        else:\n",
    "            crop, label = self.generate_bb_crop(image, bbs)\n",
    "\n",
    "        if self.transform:\n",
    "            crop = self.transform(crop)\n",
    "\n",
    "        return crop, label\n",
    "\n",
    "    def load_image_data(self, row):\n",
    "        image_bytes = tlc.Url(row[\"image\"]).read()\n",
    "        image = Image.open(BytesIO(image_bytes))\n",
    "        return image\n",
    "\n",
    "    def generate_bb_crop(self, image, bbs):\n",
    "        \"\"\"\n",
    "        Crop a bounding box from the image.\n",
    "\n",
    "        Args:\n",
    "            image: PIL.Image, the input image.\n",
    "            bbs: list, bounding boxes associated with the image.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (cropped image, label) where label is a tensor.\n",
    "        \"\"\"\n",
    "        if not bbs:\n",
    "            raise ValueError(\"No bounding boxes found. Check your sampler.\")\n",
    "\n",
    "        random_bb = random.choice(bbs)\n",
    "        crop = tlc.BBCropInterface.crop(image, random_bb, self.bb_schema)\n",
    "        \n",
    "        label = random_bb[\"label\"]\n",
    "        return crop, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def generate_background(self, image, bbs):\n",
    "        \"\"\"\n",
    "        Generate a background patch from the image.\n",
    "\n",
    "        Args:\n",
    "            image: PIL.Image, the input image.\n",
    "            bbs: list, bounding boxes associated with the image.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (background patch, background label) where label is a tensor.\n",
    "        \"\"\"\n",
    "        image_width, image_height = image.size\n",
    "        bb_factory = tlc.BoundingBox.from_schema(self.bb_schema)\n",
    "        gt_boxes_xywh = [\n",
    "            bb_factory([bb[\"x0\"], bb[\"y0\"], bb[\"x1\"], bb[\"y1\"]])\n",
    "            .to_top_left_xywh()\n",
    "            .denormalize(image_width, image_height)\n",
    "            for bb in bbs\n",
    "        ]\n",
    "\n",
    "        while True:\n",
    "            # Generate a random box\n",
    "            x = max(\n",
    "                min(int(self.random_gen.normalvariate(mu=image_width // 2, sigma=image_width // 6)), image_width - 1), 0\n",
    "            )\n",
    "            y = max(\n",
    "                min(\n",
    "                    int(self.random_gen.normalvariate(mu=image_height // 2, sigma=image_height // 6)),\n",
    "                    image_height - 1,\n",
    "                ),\n",
    "                0,\n",
    "            )\n",
    "            w = max(\n",
    "                min(\n",
    "                    int(self.random_gen.normalvariate(mu=image_width // 8, sigma=image_width // 16)), image_width - x\n",
    "                ),\n",
    "                1,\n",
    "            )\n",
    "            h = max(\n",
    "                min(\n",
    "                    int(self.random_gen.normalvariate(mu=image_height // 8, sigma=image_height // 16)), image_height - y\n",
    "                ),\n",
    "                1,\n",
    "            )\n",
    "            proposal_box = [x, y, w, h]\n",
    "\n",
    "            # Ensure the proposed box does not intersect any ground truth boxes\n",
    "            if not any(self._intersects(proposal_box, gt_box) for gt_box in gt_boxes_xywh):\n",
    "                break\n",
    "\n",
    "        # Crop the background patch from the image\n",
    "        background_patch = image.crop((x, y, x + w, y + h))\n",
    "        return background_patch, torch.tensor(self.background_label, dtype=torch.long)\n",
    "\n",
    "    @staticmethod\n",
    "    def _intersects(box1, box2):\n",
    "        \"\"\"\n",
    "        Check if two bounding boxes intersect.\n",
    "\n",
    "        Args:\n",
    "            box1: list[int], first bounding box [x, y, w, h].\n",
    "            box2: list[int], second bounding box [x, y, w, h].\n",
    "\n",
    "        Returns:\n",
    "            bool: True if boxes intersect, otherwise False.\n",
    "        \"\"\"\n",
    "        x1, y1, w1, h1 = box1\n",
    "        x2, y2, w2, h2 = box2\n",
    "        return not (x1 + w1 <= x2 or x2 + w2 <= x1 or y1 + h1 <= y2 or y2 + h2 <= y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations to be applied to the images\n",
    "\n",
    "common_transforms = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.3),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    *common_transforms.transforms,\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataset = BBCropDataset(\n",
    "    input_table,\n",
    "    transform=train_transforms,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "val_dataset = BBCropDataset(\n",
    "    input_table,\n",
    "    transform=common_transforms,\n",
    "    is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "def save_image_grid(images: List[torch.Tensor], labels: List[str], rows: int, cols: int, save_path: str):\n",
    "    \"\"\"\n",
    "    Saves a grid of images with corresponding labels as subplot titles.\n",
    "\n",
    "    Args:\n",
    "        images (List[torch.Tensor]): List of image tensors (C x H x W) to display.\n",
    "        labels (List[str]): List of labels corresponding to each image.\n",
    "        rows (int): Number of rows in the grid.\n",
    "        cols (int): Number of columns in the grid.\n",
    "        save_path (str): Path to save the output image grid.\n",
    "    \"\"\"\n",
    "    assert len(images) == len(labels), \"Number of images and labels must be the same.\"\n",
    "    assert len(images) <= rows * cols, \"Not enough space in the grid for all images.\"\n",
    "\n",
    "    # Unnormalize and convert tensors to PIL images\n",
    "    unnormalize = transforms.Normalize(\n",
    "        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],  # Reverse normalization\n",
    "        std=[1 / 0.229, 1 / 0.224, 1 / 0.225],\n",
    "    )\n",
    "    to_pil = transforms.ToPILImage()\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, (image, label) in enumerate(zip(images, labels)):\n",
    "        # Unnormalize the tensor image\n",
    "        image = unnormalize(image)\n",
    "        pil_image = to_pil(image)\n",
    "\n",
    "        # Plot the image\n",
    "        axes[idx].imshow(pil_image)\n",
    "        axes[idx].set_title(label)\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for ax in axes[len(images):]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'image_bytes' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m     train_images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[0;32m      6\u001b[0m     train_labels\u001b[38;5;241m.\u001b[39mappend(label_map[label\u001b[38;5;241m.\u001b[39mitem()])\n",
      "Cell \u001b[1;32mIn[21], line 68\u001b[0m, in \u001b[0;36mBBCropDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     65\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable\u001b[38;5;241m.\u001b[39mtable_rows[row_idx]\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_image_data(row)\n\u001b[1;32m---> 68\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(BytesIO(\u001b[43mimage_bytes\u001b[49m))\n\u001b[0;32m     69\u001b[0m bbs \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbb_list\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bbs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_background:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'image_bytes' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "for i in range(4*3):\n",
    "    image, label = train_dataset[i]\n",
    "    train_images.append(image)\n",
    "    train_labels.append(label_map[label.item()])\n",
    "\n",
    "save_image_grid(train_images, train_labels, 4, 3, \"Training Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = []\n",
    "val_labels = []\n",
    "for i in range(4*3):\n",
    "    image, label = val_dataset[i]\n",
    "    val_images.append(image)\n",
    "    val_labels.append(label_map[label.item()])\n",
    "\n",
    "save_image_grid(val_images, val_labels, 4, 3, \"Validation Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Load an EfficientNet model using timm\n",
    "model = timm.create_model(\"efficientnet_b0\", pretrained=True, num_classes=len(label_map)+1).to(device)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9516)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "    for inputs, labels in tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    train_loss /= train_total\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm.tqdm(val_dataloader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss /= val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a pth file:\n",
    "torch.save(model.state_dict(), TRANSIENT_DATA_PATH + \"/bb_classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
