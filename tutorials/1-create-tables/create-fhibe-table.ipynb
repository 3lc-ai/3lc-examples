{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Load FHIBE Dataset\n",
    "\n",
    "![img](../images/fhibe.png)\n",
    "\n",
    "This notebook loads the Sony AI's \"Fair Human-Centric Image Benchmark\" dataset as a 3LC Table, including keypoints, segmentation, bounding boxes, as well as rich subject metadata.\n",
    "\n",
    "<!-- Tags: [\"keypoints\", \"instance-segmentation\", \"object-detection\"] -->\n",
    "\n",
    "To download the dataset, you need to register at [fairnessbenchmark.ai.sony](https://fairnessbenchmark.ai.sony/). To read the original research paper, check see [here](https://www.nature.com/articles/s41586-025-09716-2).\n",
    "\n",
    "Several versions of the dataset exist, for this tutorial we will use version contained in `fhibe.20250716.u.gT5_rFTA_downsampled_public.tar.gz`, but the ingestion script should work for any version of the dataset. \n",
    "\n",
    "We have tried to include as much as possible of the metadata contained in the dataset, omitting only a few attributes in the name of simplicity, e.g. the `<attr>_QA_annotator_id` fields have been left out.\n",
    "\n",
    "The data can be categorized as follows:\n",
    "- Main image\n",
    "- Geometric annotations (instance segmentations, keypoints, facial bounding box)\n",
    "- Image-level metadata (shutter speed, camera manufacturer, weather, etc.)\n",
    "- Subject-level metadata (ancestry, hair color, age, etc.)\n",
    "\n",
    "This script reads all this data from per-subject JSON files and converts it to a format suitable for a 3LC Table. Several of the columns are stored as \"categorical strings\" (e.g. hair color \"Blond\", \"Gray\", \"White\", ...), these values are converted to integers, with their corresponding string values stored in the schema. This makes it easier to filter and work with these values in the 3LC Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q 3lc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tlc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Project setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"3LC Tutorials - FHIBE\"\n",
    "DATASET_NAME = \"FHIBE\"\n",
    "TABLE_NAME = \"initial\"\n",
    "MAX_SAMPLES = None\n",
    "DOWNLOAD_PATH = \"D:/Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FHIBE_ROOT = Path(DOWNLOAD_PATH) / \"fhibe\"\n",
    "DATA_ROOT = FHIBE_ROOT / \"data/raw/fhibe_downsampled\"\n",
    "\n",
    "if not FHIBE_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"FHIBE_ROOT does not exist: {FHIBE_ROOT}\")\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"DATA_ROOT does not exist: {DATA_ROOT}\")\n",
    "\n",
    "annotation_paths = list(DATA_ROOT.glob(\"**/main_annos_*.json\"))\n",
    "print(f\"Found {len(annotation_paths)} annotation files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Prepare value mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Image-level mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_position_value_map = {\"Typical\": 0, \"Atypical High\": 1, \"Atypical Low\": 2}\n",
    "camera_distance_value_map = {\"CD I\": 0, \"CD II\": 1, \"CD III\": 2, \"CD IV\": 3, \"CD V\": 4}\n",
    "lighting_value_map = {\n",
    "    \"Lighting from above the head/face\": 0,\n",
    "    \"Lighting from below the head/face\": 1,\n",
    "    \"Lighting from in front of the head/face\": 2,\n",
    "    \"Lighting from behind the head/face\": 3,\n",
    "    \"Lighting from the left of the head/face\": 4,\n",
    "    \"Lighting from the right of the head/face\": 5,\n",
    "}\n",
    "weather_value_map = {\"Fog\": 0, \"Haze\": 1, \"Snow/hail\": 2, \"Rain\": 3, \"Humid\": 4, \"Cloud\": 5, \"Clear\": 6}\n",
    "user_hour_captured_value_map = {\"0000-0559\": 0, \"0600-1159\": 1, \"1200-1759\": 2, \"1800-2359\": 3}\n",
    "scene_value_map = {\n",
    "    \"Outdoor Water, ice, snow\": 0,\n",
    "    \"Outdoor Mountains, hills, desert, sky\": 1,\n",
    "    \"Outdoor Forest, field, jungle\": 2,\n",
    "    \"Outdoor Man-made elements\": 3,\n",
    "    \"Outdoor Transportation\": 4,\n",
    "    \"Outdoor Cultural or historical building/place\": 5,\n",
    "    \"Outdoor Sports fields, parks, leisure spaces\": 6,\n",
    "    \"Outdoor Industrial and construction\": 7,\n",
    "    \"Outdoor Houses, cabins, gardens, and farms\": 8,\n",
    "    \"Outdoor Commercial buildings, shops, markets, cities, and towns\": 9,\n",
    "    \"Indoor Shopping and dining\": 10,\n",
    "    \"Indoor Workplace\": 11,\n",
    "    \"Indoor Home or hotel\": 12,\n",
    "    \"Indoor Transportation\": 13,\n",
    "    \"Indoor Sports and leisure\": 14,\n",
    "    \"Indoor Cultural\": 15,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Subject-level mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_value_map = {\n",
    "    \"Face skin\": 0.0,\n",
    "    \"Upper body skin\": 1.0,\n",
    "    \"Right eye\": 2.0,\n",
    "    \"Nose\": 3.0,\n",
    "    \"Upper lip\": 4.0,\n",
    "    \"Lower lip\": 5.0,\n",
    "    \"Inner mouth\": 6.0,\n",
    "    \"Left shoe\": 7.0,\n",
    "    \"Right shoe\": 8.0,\n",
    "    \"Left arm skin\": 9.0,\n",
    "    \"Upper body clothes\": 10.0,\n",
    "    \"Lower body clothes\": 11.0,\n",
    "    \"Sock or legwarmer\": 12.0,\n",
    "    \"Jewelry or timepiece\": 13.0,\n",
    "    \"Right arm skin\": 14.0,\n",
    "    \"Left leg skin\": 15.0,\n",
    "    \"Right leg skin\": 16.0,\n",
    "    \"Head hair\": 17.0,\n",
    "    \"Left eyebrow\": 18.0,\n",
    "    \"Right eyebrow\": 19.0,\n",
    "    \"Left eye\": 20.0,\n",
    "    \"Bag\": 21.0,\n",
    "    \"Eyewear\": 22.0,\n",
    "    \"Full body clothes\": 23.0,\n",
    "    \"Headwear\": 24.0,\n",
    "    \"Mask\": 25.0,\n",
    "    \"Neckwear\": 26.0,\n",
    "    \"Glove\": 27.0,\n",
    "}\n",
    "\n",
    "pronoun_value_map = {\n",
    "    \"She/her/hers\": 0,\n",
    "    \"He/him/his\": 1,\n",
    "    \"They/them/their\": 2,\n",
    "    \"Ze/zir/zirs\": 3,\n",
    "    \"None of the above\": 4,\n",
    "    \"Prefer not to say\": 5,\n",
    "}\n",
    "\n",
    "head_pose_value_map = {\n",
    "    \"Typical\": 0,\n",
    "    \"Atypical\": 1,\n",
    "}\n",
    "\n",
    "facial_marks_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Tattoos\": 1,\n",
    "    \"Birthmarks\": 2,\n",
    "    \"Scars\": 3,\n",
    "    \"Burns\": 4,\n",
    "    \"Growths\": 5,\n",
    "    \"Make-up\": 6,\n",
    "    \"Face paint\": 7,\n",
    "    \"Acne\": 8,\n",
    "    \"Not listed\": 9,\n",
    "    \"Free-text\": 10,\n",
    "}\n",
    "\n",
    "ancestry_value_map = {\n",
    "    \"Africa\": 0,\n",
    "    \"Eastern Africa\": 1,\n",
    "    \"Northern Africa\": 2,\n",
    "    \"Middle Africa\": 3,\n",
    "    \"Southern Africa\": 4,\n",
    "    \"Western Africa\": 5,\n",
    "    \"Americas\": 6,\n",
    "    \"Caribbean\": 7,\n",
    "    \"Central America\": 8,\n",
    "    \"South America\": 9,\n",
    "    \"Northern America\": 10,\n",
    "    \"Asia\": 11,\n",
    "    \"Central Asia\": 12,\n",
    "    \"Eastern Asia\": 13,\n",
    "    \"South-eastern Asia\": 14,\n",
    "    \"Southern Asia\": 15,\n",
    "    \"Western Asia\": 16,\n",
    "    \"Europe\": 17,\n",
    "    \"Eastern Europe\": 18,\n",
    "    \"Northern Europe\": 19,\n",
    "    \"Southern Europe\": 20,\n",
    "    \"Western Europe\": 21,\n",
    "    \"Oceania\": 22,\n",
    "    \"Australia and New Zealand\": 23,\n",
    "    \"Polynesia\": 24,\n",
    "}\n",
    "\n",
    "skin_color_value_map = {\n",
    "    \"[102, 78, 65]\": 0,\n",
    "    \"[136, 105, 81]\": 1,\n",
    "    \"[164, 131, 103]\": 2,\n",
    "    \"[175, 148, 120]\": 3,\n",
    "    \"[189, 163, 137]\": 4,\n",
    "    \"[198, 180, 157]\": 5,\n",
    "}\n",
    "\n",
    "haircolor_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Very light blond\": 1,\n",
    "    \"Light blond\": 2,\n",
    "    \"Blond\": 3,\n",
    "    \"Dark blond\": 4,\n",
    "    \"Light brown to medium brown\": 5,\n",
    "    \"Dark brown/black\": 6,\n",
    "    \"Red\": 7,\n",
    "    \"Gray\": 8,\n",
    "    \"Red blond\": 9,\n",
    "    \"White\": 10,\n",
    "    \"Not listed\": 11,\n",
    "    \"Free-text\": 12,\n",
    "}\n",
    "\n",
    "hairstyle_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Buzz cut\": 1,\n",
    "    \"Up (Short)\": 10,\n",
    "    \"Half-up (Short)\": 11,\n",
    "    \"Down (Short)\": 12,\n",
    "    \"Not listed(Short)\": 13,\n",
    "    \"Up (Medium)\": 14,\n",
    "    \"Half-up (Medium)\": 15,\n",
    "    \"Down (Medium)\": 2,\n",
    "    \"Not listed(Medium)\": 3,\n",
    "    \"Up (Long)\": 4,\n",
    "    \"Half-up (Long)\": 5,\n",
    "    \"Down (Long)\": 6,\n",
    "    \"Not listed(Long)\": 7,\n",
    "    \"Not listed\": 8,\n",
    "    \"Free-text\": 9,\n",
    "}\n",
    "\n",
    "facial_hairstyle_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Beard\": 1,\n",
    "    \"Mustache\": 2,\n",
    "    \"Goatee\": 3,\n",
    "}\n",
    "\n",
    "hair_type_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Straight\": 1,\n",
    "    \"Wavy\": 2,\n",
    "    \"Curly\": 3,\n",
    "    \"Kinky-coily\": 4,\n",
    "    \"Not listed\": 5,\n",
    "    \"Free-text\": 6,\n",
    "}\n",
    "\n",
    "action_body_pose_value_map = {\n",
    "    \"Standing\": 0,\n",
    "    \"Sitting\": 1,\n",
    "    \"Walking\": 2,\n",
    "    \"Bending/bowing\": 3,\n",
    "    \"Lying down/sleeping\": 4,\n",
    "    \"Performing martial/fighting arts\": 5,\n",
    "    \"Dancing\": 6,\n",
    "    \"Running/jogging\": 7,\n",
    "    \"Crouching/kneeling\": 8,\n",
    "    \"Getting up\": 9,\n",
    "    \"Jumping/leaping\": 10,\n",
    "    \"Falling down\": 11,\n",
    "    \"Crawling\": 12,\n",
    "    \"Swimming\": 13,\n",
    "    \"Not listed\": 14,\n",
    "    \"Free-text\": 15,\n",
    "}\n",
    "\n",
    "action_subject_object_interaction_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Riding\": 1,\n",
    "    \"Driving\": 2,\n",
    "    \"Watching\": 3,\n",
    "    \"Smoking\": 4,\n",
    "    \"Eating\": 5,\n",
    "    \"Drinking\": 6,\n",
    "    \"Opening or closing\": 7,\n",
    "    \"Lifting/picking up or putting down\": 8,\n",
    "    \"Writing/drawing or painting\": 9,\n",
    "    \"Catching or throwing\": 10,\n",
    "    \"Pushing, pulling or extracting\": 11,\n",
    "    \"Putting on or taking off clothing\": 12,\n",
    "    \"Entering or exiting\": 13,\n",
    "    \"Climbing\": 14,\n",
    "    \"Pointing at\": 15,\n",
    "    \"Shooting at\": 16,\n",
    "    \"Digging/shoveling\": 17,\n",
    "    \"Playing with pets/animals\": 18,\n",
    "    \"Playing musical instrument\": 19,\n",
    "    \"Playing\": 20,\n",
    "    \"Using an electronic device\": 21,\n",
    "    \"Cutting or chopping\": 22,\n",
    "    \"Cooking\": 23,\n",
    "    \"Fishing\": 24,\n",
    "    \"Rowing\": 25,\n",
    "    \"Sailing\": 26,\n",
    "    \"Brushing teeth\": 27,\n",
    "    \"Hitting\": 28,\n",
    "    \"Kicking\": 29,\n",
    "    \"Turning\": 30,\n",
    "    \"Not listed\": 31,\n",
    "    \"Free-text\": 32,\n",
    "}\n",
    "\n",
    "eye_color_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Blue\": 1,\n",
    "    \"Gray\": 2,\n",
    "    \"Green\": 3,\n",
    "    \"Hazel\": 4,\n",
    "    \"Brown\": 5,\n",
    "    \"Red and violet\": 6,\n",
    "    \"Not listed\": 7,\n",
    "    \"Free-text\": 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Define data processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(s):\n",
    "    if \". \" in s:\n",
    "        # Many FHIBE strings are numbered, e.g. \"1. Right eye inner\". For readability, remove the numbering\n",
    "        s = s.split(\". \")[1]\n",
    "    # MapElements in 3LC do not support \":\" in the name\n",
    "    return s.replace(\":\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KEYPOINTS = 33\n",
    "SKELETON = [\n",
    "    11,\n",
    "    12,\n",
    "    11,\n",
    "    13,\n",
    "    13,\n",
    "    15,\n",
    "    12,\n",
    "    14,\n",
    "    14,\n",
    "    16,\n",
    "    12,\n",
    "    24,\n",
    "    11,\n",
    "    23,\n",
    "    23,\n",
    "    24,\n",
    "    24,\n",
    "    26,\n",
    "    26,\n",
    "    28,\n",
    "    23,\n",
    "    25,\n",
    "    25,\n",
    "    27,\n",
    "    27,\n",
    "    29,\n",
    "    29,\n",
    "    31,\n",
    "    28,\n",
    "    30,\n",
    "    30,\n",
    "    32,\n",
    "]\n",
    "\n",
    "KEYPOINTS = [\n",
    "    \"Nose\",\n",
    "    \"Right eye inner\",\n",
    "    \"Right eye\",\n",
    "    \"Right eye outer\",\n",
    "    \"Left eye inner\",\n",
    "    \"Left eye\",\n",
    "    \"Left eye outer\",\n",
    "    \"Right ear\",\n",
    "    \"Left ear\",\n",
    "    \"Mouth right\",\n",
    "    \"Mouth left\",\n",
    "    \"Right shoulder\",\n",
    "    \"Left shoulder\",\n",
    "    \"Right elbow\",\n",
    "    \"Left elbow\",\n",
    "    \"Right wrist\",\n",
    "    \"Left wrist\",\n",
    "    \"Right pinky knuckle\",\n",
    "    \"Left pinky knuckle\",\n",
    "    \"Right index knuckle\",\n",
    "    \"Left index knuckle\",\n",
    "    \"Right thumb knuckle\",\n",
    "    \"Left thumb knuckle\",\n",
    "    \"Right hip\",\n",
    "    \"Left hip\",\n",
    "    \"Right knee\",\n",
    "    \"Left knee\",\n",
    "    \"Right ankle\",\n",
    "    \"Left ankle\",\n",
    "    \"Right heel\",\n",
    "    \"Left heel\",\n",
    "    \"Right foot index\",\n",
    "    \"Left foot index\",\n",
    "]\n",
    "\n",
    "\n",
    "def process_keypoints(keypoints, image_width, image_height):\n",
    "    keypoints = {clean_str(kpt_name): v for kpt_name, v in keypoints.items()}\n",
    "    kpts_arr = np.zeros((NUM_KEYPOINTS, 3), dtype=np.float32)\n",
    "    for i, kpt_name in enumerate(KEYPOINTS):\n",
    "        if kpt_name not in keypoints:\n",
    "            continue\n",
    "        x, y, viz = keypoints[kpt_name]\n",
    "        viz = 2 if viz else 0\n",
    "        kpts_arr[i, :] = [x, y, viz]\n",
    "\n",
    "    instances = tlc.Keypoints2DInstances.create_empty(\n",
    "        image_width=image_width,\n",
    "        image_height=image_height,\n",
    "        include_keypoint_visibilities=True,\n",
    "        include_instance_bbs=False,\n",
    "    )\n",
    "\n",
    "    instances.add_instance(\n",
    "        keypoints=kpts_arr,\n",
    "        label=0,\n",
    "    )\n",
    "\n",
    "    return instances.to_row()\n",
    "\n",
    "\n",
    "def process_segments(segments, image_width, image_height):\n",
    "    polygons = []\n",
    "    labels = []\n",
    "\n",
    "    for segment in segments:\n",
    "        class_name = clean_str(segment[\"class_name\"])\n",
    "        polygon = segment[\"polygon\"]\n",
    "        poly_2_tuples = [[p[\"x\"], p[\"y\"]] for p in polygon]\n",
    "        flattened_poly = [item for sublist in poly_2_tuples for item in sublist]\n",
    "        polygons.append(flattened_poly)\n",
    "        labels.append(segments_value_map[class_name])\n",
    "\n",
    "    segs = tlc.SegmentationPolygonsDict(\n",
    "        image_width=image_width,\n",
    "        image_height=image_height,\n",
    "        polygons=polygons,\n",
    "        instance_properties={\"label\": labels},\n",
    "    )\n",
    "    return segs\n",
    "\n",
    "\n",
    "def process_bboxes(face_bbox, image_width, image_height):\n",
    "    bboxes = {\n",
    "        tlc.IMAGE_WIDTH: image_width,\n",
    "        tlc.IMAGE_HEIGHT: image_height,\n",
    "        tlc.BOUNDING_BOX_LIST: [\n",
    "            {\n",
    "                tlc.X0: face_bbox[0],\n",
    "                tlc.Y0: face_bbox[1],\n",
    "                tlc.X1: face_bbox[2],\n",
    "                tlc.Y1: face_bbox[3],\n",
    "                tlc.LABEL: 0,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_annotation(image_annotation):\n",
    "    aperture_value = image_annotation[\"aperture_value\"]  # float\n",
    "    camera_distance = camera_distance_value_map[clean_str(image_annotation[\"camera_distance\"])]  # str with value map\n",
    "    camera_position = camera_position_value_map[clean_str(image_annotation[\"camera_position\"])]  # str with value map\n",
    "    focal_length = image_annotation[\"focal_length\"]  # float\n",
    "    iso_speed_ratings = image_annotation[\"iso_speed_ratings\"]  # int\n",
    "    lighting = [lighting_value_map[clean_str(li)] for li in image_annotation[\"lighting\"]]  # list of str\n",
    "    location_country = image_annotation[\"location_country\"]  # str\n",
    "    location_region = image_annotation[\"location_region\"]  # str\n",
    "    manufacturer = image_annotation[\"manufacturer\"]  # str\n",
    "    model = image_annotation[\"model\"]  # str\n",
    "    scene = scene_value_map[clean_str(image_annotation[\"scene\"])]  # str\n",
    "    shutter_speed_value = image_annotation[\"shutter_speed_value\"]  # float\n",
    "    user_date_captured = image_annotation[\"user_date_captured\"]  # str\n",
    "    user_hour_captured = user_hour_captured_value_map[clean_str(image_annotation[\"user_hour_captured\"])]  # str\n",
    "    weather = [weather_value_map[clean_str(w)] for w in image_annotation[\"weather\"]]  # list of str\n",
    "\n",
    "    image_annotation_dict = {\n",
    "        \"aperture_value\": aperture_value,\n",
    "        \"camera_distance\": camera_distance,\n",
    "        \"camera_position\": camera_position,\n",
    "        \"focal_length\": focal_length,\n",
    "        \"iso_speed_ratings\": iso_speed_ratings,\n",
    "        \"lighting\": lighting,\n",
    "        \"location_country\": location_country,\n",
    "        \"location_region\": location_region,\n",
    "        \"manufacturer\": manufacturer,\n",
    "        \"model\": model,\n",
    "        \"scene\": scene,\n",
    "        \"shutter_speed_value\": shutter_speed_value,\n",
    "        \"user_date_captured\": user_date_captured,\n",
    "        \"user_hour_captured\": user_hour_captured,\n",
    "        \"weather\": weather,\n",
    "    }\n",
    "\n",
    "    return image_annotation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject_annotation(subject_annotation, image_height, image_width):\n",
    "    keypoints = process_keypoints(subject_annotation[\"keypoints\"], image_height, image_width)\n",
    "    segments = process_segments(subject_annotation[\"segments\"], image_height, image_width)\n",
    "    bboxes = process_bboxes(subject_annotation[\"face_bbox\"], image_height, image_width)\n",
    "    subject_id = subject_annotation[\"subject_id\"]  # str\n",
    "    age = subject_annotation[\"age\"]  # int\n",
    "    nationality = subject_annotation[\"nationality\"]  # list of str\n",
    "    ancestry = [ancestry_value_map[clean_str(a)] for a in subject_annotation[\"ancestry\"]]  # list of str\n",
    "    pronoun = [pronoun_value_map[clean_str(p)] for p in subject_annotation[\"pronoun\"]]  # list of str\n",
    "    natural_skin_color = skin_color_value_map[clean_str(subject_annotation[\"natural_skin_color\"])]  # categorical str\n",
    "    apparent_skin_color = skin_color_value_map[clean_str(subject_annotation[\"apparent_skin_color\"])]  # categorical str\n",
    "    hairstyle = hairstyle_value_map[clean_str(subject_annotation[\"hairstyle\"])]  # categorical str\n",
    "    natural_hair_type = hair_type_value_map[clean_str(subject_annotation[\"natural_hair_type\"])]  # categorical str\n",
    "    apparent_hair_type = hair_type_value_map[clean_str(subject_annotation[\"apparent_hair_type\"])]  # categorical str\n",
    "    natural_hair_color = [\n",
    "        haircolor_value_map[clean_str(h)] for h in subject_annotation[\"natural_hair_color\"]\n",
    "    ]  # list of categorical str\n",
    "    apparent_hair_color = [\n",
    "        haircolor_value_map[clean_str(h)] for h in subject_annotation[\"apparent_hair_color\"]\n",
    "    ]  # list of categorical str\n",
    "    facial_hairstyle = [\n",
    "        facial_hairstyle_value_map[clean_str(f)] for f in subject_annotation[\"facial_hairstyle\"]\n",
    "    ]  # list of categorical str\n",
    "    natural_facial_hair_color = [\n",
    "        haircolor_value_map[clean_str(h)] for h in subject_annotation[\"natural_facial_haircolor\"]\n",
    "    ]  # list of categorical str\n",
    "    apparent_facial_hair_color = [\n",
    "        haircolor_value_map[clean_str(h)] for h in subject_annotation[\"apparent_facial_haircolor\"]\n",
    "    ]  # list of categorical str\n",
    "    natural_left_eye_color = [\n",
    "        eye_color_value_map[clean_str(h)] for h in subject_annotation[\"natural_left_eye_color\"]\n",
    "    ]  # list of categorical str\n",
    "    apparent_left_eye_color = [\n",
    "        eye_color_value_map[clean_str(h)] for h in subject_annotation[\"apparent_left_eye_color\"]\n",
    "    ]  # list of categorical str\n",
    "    natural_right_eye_color = [\n",
    "        eye_color_value_map[clean_str(h)] for h in subject_annotation[\"natural_right_eye_color\"]\n",
    "    ]  # list of categorical str\n",
    "    apparent_right_eye_color = [\n",
    "        eye_color_value_map[clean_str(h)] for h in subject_annotation[\"apparent_right_eye_color\"]\n",
    "    ]  # list of categorical str\n",
    "    facial_marks = [\n",
    "        facial_marks_value_map[clean_str(f)] for f in subject_annotation[\"facial_marks\"]\n",
    "    ]  # list of categorical str\n",
    "    action_body_pose = action_body_pose_value_map[clean_str(subject_annotation[\"action_body_pose\"])]  # categorical str\n",
    "    action_subject_object_interaction = [\n",
    "        action_subject_object_interaction_value_map[clean_str(a)]\n",
    "        for a in subject_annotation[\"action_subject_object_interaction\"]\n",
    "    ]  # list of categorical str\n",
    "    head_pose = head_pose_value_map[clean_str(subject_annotation[\"head_pose\"])]  # categorical str\n",
    "\n",
    "    subject_annotation_dict = {\n",
    "        \"bbs\": bboxes,\n",
    "        \"segments\": segments,\n",
    "        \"keypoints\": keypoints,\n",
    "        \"subject_id\": subject_id,\n",
    "        \"age\": age,\n",
    "        \"nationality\": nationality,\n",
    "        \"ancestry\": ancestry,\n",
    "        \"pronoun\": pronoun,\n",
    "        \"natural_skin_color\": natural_skin_color,\n",
    "        \"apparent_skin_color\": apparent_skin_color,\n",
    "        \"hairstyle\": hairstyle,\n",
    "        \"natural_hair_type\": natural_hair_type,\n",
    "        \"apparent_hair_type\": apparent_hair_type,\n",
    "        \"natural_hair_color\": natural_hair_color,\n",
    "        \"apparent_hair_color\": apparent_hair_color,\n",
    "        \"facial_hairstyle\": facial_hairstyle,\n",
    "        \"natural_facial_hair_color\": natural_facial_hair_color,\n",
    "        \"apparent_facial_hair_color\": apparent_facial_hair_color,\n",
    "        \"natural_left_eye_color\": natural_left_eye_color,\n",
    "        \"apparent_left_eye_color\": apparent_left_eye_color,\n",
    "        \"natural_right_eye_color\": natural_right_eye_color,\n",
    "        \"apparent_right_eye_color\": apparent_right_eye_color,\n",
    "        \"facial_marks\": facial_marks,\n",
    "        \"action_body_pose\": action_body_pose,\n",
    "        \"action_subject_object_interaction\": action_subject_object_interaction,\n",
    "        \"head_pose\": head_pose,\n",
    "    }\n",
    "\n",
    "    return subject_annotation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "This is the main loop where we iterate over annotation files, extract and process annotations, and store the processed data in a list of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "total = len(annotation_paths) if MAX_SAMPLES is None else min(len(annotation_paths), MAX_SAMPLES)\n",
    "\n",
    "for annotation_path in tqdm(annotation_paths, total=total, desc=\"Processing annotations\"):\n",
    "    with open(annotation_path) as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    image_path = annotation_path.with_name(f\"main_{annotations['image']['file_name']}\")\n",
    "    image_annotation = annotations[\"image_annotation\"]\n",
    "    subject_annotations = annotations[\"subject_annotation\"]\n",
    "    image_annotation_dict = process_image_annotation(image_annotation)\n",
    "\n",
    "    for subject_annotation in subject_annotations:\n",
    "        subject_annotation_dict = process_subject_annotation(\n",
    "            subject_annotation, image_annotation[\"image_height\"], image_annotation[\"image_width\"]\n",
    "        )\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"image\": tlc.Url(image_path).to_relative().to_str(),\n",
    "                **image_annotation_dict,\n",
    "                **subject_annotation_dict,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if MAX_SAMPLES is not None and len(rows) >= MAX_SAMPLES:\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    break  # Max samples reached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Write 3LC Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "override_schemas = {\n",
    "    # Image annotations\n",
    "    \"aperture_value\": tlc.Float32Schema(default_visible=False, writable=False),\n",
    "    \"camera_distance\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: k for k, v in camera_distance_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"camera_position\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: k for k, v in camera_position_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"focal_length\": tlc.Float32Schema(default_visible=False, writable=False),\n",
    "    \"iso_speed_ratings\": tlc.Int32Schema(default_visible=False, writable=False),\n",
    "    \"lighting\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in lighting_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"location_country\": tlc.StringSchema(default_visible=False, writable=False),\n",
    "    \"location_region\": tlc.StringSchema(default_visible=False, writable=False),\n",
    "    \"manufacturer\": tlc.StringSchema(default_visible=False, writable=False),\n",
    "    \"model\": tlc.StringSchema(default_visible=False, writable=False),\n",
    "    \"scene\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: k for k, v in scene_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"shutter_speed_value\": tlc.Float32Schema(default_visible=False, writable=False),\n",
    "    \"user_date_captured\": tlc.StringSchema(default_visible=False, writable=False),\n",
    "    \"user_hour_captured\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: k for k, v in user_hour_captured_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"weather\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in weather_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    # Subject annotations\n",
    "    \"subject_id\": tlc.StringSchema(default_visible=False, writable=False),\n",
    "    \"age\": tlc.Int32Schema(default_visible=False, writable=False),\n",
    "    \"nationality\": tlc.StringListSchema(default_visible=False, writable=False),\n",
    "    \"ancestry\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in ancestry_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"pronoun\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in pronoun_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"natural_skin_color\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: k for k, v in skin_color_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"apparent_skin_color\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: k for k, v in skin_color_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"hairstyle\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: k for k, v in hairstyle_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"natural_hair_type\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: k for k, v in hair_type_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"apparent_hair_type\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: k for k, v in hair_type_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"natural_hair_color\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in haircolor_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"apparent_hair_color\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in haircolor_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"facial_hairstyle\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in facial_hairstyle_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"natural_facial_hair_color\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in haircolor_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"apparent_facial_hair_color\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in haircolor_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"natural_left_eye_color\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in eye_color_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"apparent_left_eye_color\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in eye_color_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"natural_right_eye_color\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in eye_color_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"apparent_right_eye_color\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in eye_color_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"facial_marks\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in facial_marks_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"action_body_pose\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: k for k, v in action_body_pose_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "    \"action_subject_object_interaction\": tlc.CategoricalLabelListSchema(\n",
    "        classes={v: k for k, v in action_subject_object_interaction_value_map.items()},\n",
    "        default_visible=False,\n",
    "        writable=False,\n",
    "    ),\n",
    "    \"head_pose\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: k for k, v in head_pose_value_map.items()}, default_visible=False, writable=False\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_writer = tlc.TableWriter(\n",
    "    table_name=TABLE_NAME,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    project_name=PROJECT_NAME,\n",
    "    column_schemas={\n",
    "        \"image\": tlc.ImageUrlSchema(),\n",
    "        \"keypoints\": tlc.Keypoints2DSchema(\n",
    "            classes=[\"person\"],\n",
    "            num_keypoints=NUM_KEYPOINTS,\n",
    "            lines=SKELETON,\n",
    "            point_attributes=KEYPOINTS,\n",
    "            include_per_point_visibility=True,\n",
    "        ),\n",
    "        \"bbs\": tlc.BoundingBoxListSchema(\n",
    "            label_value_map={0: tlc.MapElement(\"face\")},\n",
    "            include_segmentation=False,\n",
    "            x1_number_role=tlc.NUMBER_ROLE_BB_SIZE_X,\n",
    "            y1_number_role=tlc.NUMBER_ROLE_BB_SIZE_Y,\n",
    "        ),\n",
    "        \"segments\": tlc.SegmentationSchema(\n",
    "            label_value_map={v: tlc.MapElement(k) for k, v in segments_value_map.items()},\n",
    "        ),\n",
    "        **override_schemas,\n",
    "    },\n",
    ")\n",
    "\n",
    "for row in tqdm(rows, total=len(rows), desc=\"Writing rows\"):\n",
    "    table_writer.add_row(row)\n",
    "\n",
    "table = table_writer.finalize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
