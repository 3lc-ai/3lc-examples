{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Load FHIBE Dataset\n",
    "\n",
    "This notebook loads the Sony AI's \"Fair Human-Centric Image Benchmark\" dataset as a 3LC Table, including keypoints, segmentation, bounding boxes, as well as rich subject metadata.\n",
    "\n",
    "![img](../images/fhibe.png)\n",
    "\n",
    "<!-- Tags: [\"keypoints\", \"instance-segmentation\", \"object-detection\"] -->\n",
    "\n",
    "To download the dataset, you need to register at [fairnessbenchmark.ai.sony](https://fairnessbenchmark.ai.sony/). To read the original research paper, see [here](https://www.nature.com/articles/s41586-025-09716-2).\n",
    "\n",
    "Several versions of the dataset exist, for this tutorial we will use version from `fhibe.20250716.u.gT5_rFTA_downsampled_public.tar.gz`, but the ingestion script should work for any version of the dataset, as the internal layout of the dataset is the same.\n",
    "\n",
    "We include as much as possible of the metadata contained in the dataset, omitting only a few attributes in the name of simplicity, specifically the `<attr>_QA_annotator_id` fields have been left out.\n",
    "\n",
    "The data can be categorized as follows:\n",
    "- Main image\n",
    "- Geometric annotations (instance segmentations, keypoints, facial bounding box)\n",
    "- Image-level metadata (shutter speed, camera manufacturer, weather conditions, etc.)\n",
    "- Subject-level metadata (ancestry, hair color, age, etc.)\n",
    "\n",
    "This script reads all data from the CSV file and converts it to a format suitable for a 3LC Table. Several of the columns are stored as \"categorical strings\" (e.g. hair color \"Blond\", \"Gray\", \"White\", ...), these values are converted to integers, with their corresponding string values stored in the schema. This makes it easier to filter and work with these values in the 3LC Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q 3lc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tlc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Project setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"3LC Tutorials - FHIBE\"\n",
    "DATASET_NAME = \"FHIBE\"\n",
    "TABLE_NAME = \"initial\"\n",
    "MAX_SAMPLES = 2000  # Set to None to process all rows\n",
    "DOWNLOAD_PATH = \"/Users/gudbrand/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loading: 6.91s (10941 rows)\n",
      "Parsing serialized columns: 14.46s\n",
      "Loaded 10941 rows from CSV\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "FHIBE_ROOT = Path(DOWNLOAD_PATH) / \"fhibe\"\n",
    "CSV_FILE = FHIBE_ROOT / \"data/processed/fhibe_downsampled/fhibe_downsampled.csv\"\n",
    "\n",
    "if not CSV_FILE.exists():\n",
    "    raise FileNotFoundError(f\"CSV_FILE does not exist: {CSV_FILE}\")\n",
    "\n",
    "# Load CSV\n",
    "t0 = time.time()\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "print(f\"CSV loading: {time.time() - t0:.2f}s ({len(df)} rows)\")\n",
    "\n",
    "\n",
    "def fast_parse(s):\n",
    "    \"\"\"Parse serialized Python literal using json.loads (faster than ast.literal_eval).\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return s\n",
    "    # Replace single quotes with double quotes for JSON compatibility\n",
    "    # Handle escaped quotes and None values\n",
    "    s = s.replace(\"'\", '\"').replace(\"None\", \"null\").replace(\"True\", \"true\").replace(\"False\", \"false\")\n",
    "    return json.loads(s)\n",
    "\n",
    "\n",
    "# Parse columns containing serialized Python literals\n",
    "SERIALIZED_COLUMNS = [\n",
    "    'lighting', 'weather', 'nationality', 'ancestry', 'pronoun',\n",
    "    'natural_hair_color', 'apparent_hair_color', 'facial_hairstyle',\n",
    "    'natural_facial_haircolor', 'apparent_facial_haircolor',\n",
    "    'natural_left_eye_color', 'apparent_left_eye_color',\n",
    "    'natural_right_eye_color', 'apparent_right_eye_color',\n",
    "    'facial_marks', 'action_subject_object_interaction',\n",
    "    'keypoints', 'segments', 'face_bbox', 'person_bbox',\n",
    "]\n",
    "\n",
    "t0 = time.time()\n",
    "for col in SERIALIZED_COLUMNS:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(fast_parse)\n",
    "print(f\"Parsing serialized columns: {time.time() - t0:.2f}s\")\n",
    "\n",
    "print(f\"Loaded {len(df)} rows from CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "767deb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to ingest (excluding QA annotator columns and other metadata)\n",
    "COLUMNS_TO_INGEST = [\n",
    "    # Image-level metadata\n",
    "    'aperture_value', 'camera_distance', 'camera_position', 'focal_length',\n",
    "    'iso_speed_ratings', 'lighting', 'location_country', 'location_region',\n",
    "    'manufacturer', 'model', 'scene', 'shutter_speed_value', 'user_date_captured',\n",
    "    'user_hour_captured', 'weather',\n",
    "    # Subject-level metadata\n",
    "    'subject_id', 'age', 'nationality', 'ancestry', 'pronoun',\n",
    "    'natural_skin_color', 'apparent_skin_color', 'hairstyle',\n",
    "    'natural_hair_type', 'apparent_hair_type', 'natural_hair_color',\n",
    "    'apparent_hair_color', 'facial_hairstyle', 'natural_facial_haircolor',\n",
    "    'apparent_facial_haircolor', 'natural_left_eye_color', 'apparent_left_eye_color',\n",
    "    'natural_right_eye_color', 'apparent_right_eye_color', 'facial_marks',\n",
    "    'action_body_pose', 'action_subject_object_interaction', 'head_pose',\n",
    "]\n",
    "\n",
    "# Special columns requiring custom processing (output as separate columns)\n",
    "SPECIAL_COLUMNS = ['keypoints', 'segments', 'face_bbox']\n",
    "\n",
    "# Auxiliary columns (used internally but not output directly)\n",
    "AUXILIARY_COLUMNS = ['person_bbox', 'image_height', 'image_width', 'filepath']\n",
    "\n",
    "# Columns to treat as plain strings (not categorical due to high cardinality)\n",
    "STRING_COLUMNS = ['user_date_captured', 'subject_id', 'location_region', 'model']\n",
    "\n",
    "# Columns with skin color values that need display_color in schema\n",
    "SKIN_COLOR_COLUMNS = ['natural_skin_color', 'apparent_skin_color']\n",
    "\n",
    "# Threshold for auto-detecting categorical columns (max unique values)\n",
    "CATEGORICAL_THRESHOLD = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "These functions handle value cleaning, type detection, and schema inference for the categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Value cleaning and mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_internal_name(s: str) -> str:\n",
    "    \"\"\"Create a valid internal name for a 3LC MapElement.\n",
    "    \n",
    "    Removes numbered prefixes and all disallowed characters.\n",
    "    Disallowed characters: <>\\\\|.:\"'?*&\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return str(s)\n",
    "    # Remove numbered prefix like \"0. \" or \"12. \"\n",
    "    s = re.sub(r'^\\d+\\.\\s*', '', s)\n",
    "    # Remove disallowed characters\n",
    "    for char in '<>\\\\|.:\"\\'?*&':\n",
    "        s = s.replace(char, '')\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def get_unique_values(series: pd.Series, is_list: bool = False) -> list:\n",
    "    \"\"\"Extract unique values from a column (already parsed from string literals).\"\"\"\n",
    "    if is_list:\n",
    "        all_vals = set()\n",
    "        for val in series.dropna():\n",
    "            if isinstance(val, list):\n",
    "                all_vals.update(val)\n",
    "        return list(all_vals)\n",
    "    return list(series.dropna().unique())\n",
    "\n",
    "\n",
    "def sort_by_prefix(values: list) -> list:\n",
    "    \"\"\"Sort values by their numeric prefix if present (e.g., '0. Standing' before '1. Sitting').\"\"\"\n",
    "    def key(v):\n",
    "        match = re.match(r'^(\\d+)\\.', str(v))\n",
    "        return (int(match.group(1)), str(v)) if match else (999, str(v))\n",
    "    return sorted(values, key=key)\n",
    "\n",
    "\n",
    "def build_value_map(series: pd.Series, is_list: bool = False) -> dict[str, tuple[int, str]]:\n",
    "    \"\"\"Build a mapping from internal_name to (index, display_name).\n",
    "    \n",
    "    The display_name is the original value, internal_name has disallowed chars removed.\n",
    "    Returns: {internal_name: (index, display_name), ...}\n",
    "    \"\"\"\n",
    "    unique_vals = sort_by_prefix(get_unique_values(series, is_list))\n",
    "    return {\n",
    "        make_internal_name(v): (i, v)  # v is the original value for display\n",
    "        for i, v in enumerate(unique_vals)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Type detection and schema inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_column_type(col_name: str, series: pd.Series) -> str:\n",
    "    \"\"\"Detect the type of a column for schema inference.\n",
    "    \n",
    "    Returns one of: 'numeric', 'string', 'categorical', 'categorical_list', 'special'\n",
    "    \"\"\"\n",
    "    if col_name in SPECIAL_COLUMNS:\n",
    "        return 'special'\n",
    "    if col_name in STRING_COLUMNS:\n",
    "        return 'string'\n",
    "    if series.dtype in ['int64', 'float64']:\n",
    "        return 'numeric'\n",
    "    \n",
    "    # Check if column contains lists (already parsed)\n",
    "    sample = series.dropna().iloc[0] if len(series.dropna()) > 0 else None\n",
    "    if isinstance(sample, dict):\n",
    "        return 'special'\n",
    "    if isinstance(sample, list):\n",
    "        if sample and isinstance(sample[0], str):\n",
    "            return 'categorical_list'\n",
    "        return 'special'\n",
    "    \n",
    "    # For string columns, use unique count to determine categorical vs string\n",
    "    return 'categorical' if series.nunique() <= CATEGORICAL_THRESHOLD else 'string'\n",
    "\n",
    "\n",
    "def tuple2hex(t: str) -> str:\n",
    "    \"\"\"Convert a serialized RGB list to hex color: '[255, 255, 255]' -> '#FFFFFF'\"\"\"\n",
    "    nums = [int(c) for c in t.strip('[]').split(',')]\n",
    "    return \"#{:02X}{:02X}{:02X}\".format(*nums)\n",
    "\n",
    "\n",
    "def build_map_elements(value_map: dict, col_name: str = None) -> dict:\n",
    "    \"\"\"Build MapElement dict from value_map for use in schema.\n",
    "    \n",
    "    Args:\n",
    "        value_map: {internal_name: (index, display_name), ...}\n",
    "        col_name: Column name, used for special handling (e.g., skin color)\n",
    "    \n",
    "    Returns: {index: MapElement, ...}\n",
    "    \"\"\"\n",
    "    elements = {}\n",
    "    for internal_name, (idx, display_name) in value_map.items():\n",
    "        kwargs = {\"display_name\": display_name}\n",
    "        \n",
    "        # Special handling for skin color columns\n",
    "        if col_name in SKIN_COLOR_COLUMNS:\n",
    "            kwargs[\"display_color\"] = tuple2hex(internal_name)\n",
    "        \n",
    "        elements[idx] = tlc.MapElement(internal_name, **kwargs)\n",
    "    \n",
    "    return elements\n",
    "\n",
    "\n",
    "def infer_schema(col_name: str, series: pd.Series, default_args: dict):\n",
    "    \"\"\"Infer the appropriate 3LC schema for a column based on its data.\"\"\"\n",
    "    col_type = detect_column_type(col_name, series)\n",
    "    is_list = col_type == 'categorical_list'\n",
    "    \n",
    "    if col_type == 'numeric':\n",
    "        return tlc.Int32Schema(**default_args) if series.dtype == 'int64' else tlc.Float32Schema(**default_args)\n",
    "    \n",
    "    if col_type == 'string':\n",
    "        return tlc.StringSchema(**default_args)\n",
    "    \n",
    "    if col_type in ('categorical', 'categorical_list'):\n",
    "        value_map = build_value_map(series, is_list=is_list)\n",
    "        map_elements = build_map_elements(value_map, col_name)\n",
    "        \n",
    "        if is_list:\n",
    "            return tlc.CategoricalLabelListSchema(classes=map_elements, **default_args)\n",
    "        return tlc.CategoricalLabelSchema(classes=map_elements, **default_args)\n",
    "    \n",
    "    return None  # Special columns handled separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Consolidation of country spelling variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/SonyResearch/fhibe_evaluation_api/blob/main/fhibe_eval_api/datasets/fhibe.py\n",
    "\n",
    "loc_country_name_mapping = {\n",
    "    \"Abgola\": \"Angola\",\n",
    "    \"Abuja\": \"Nigeria\",\n",
    "    \"Argentiina\": \"Argentina\",\n",
    "    \"Australie\": \"Australia\",\n",
    "    \"Autsralia\": \"Australia\",\n",
    "    \"Auustralia\": \"Australia\",\n",
    "    \"Bahamas, The\": \"Bahamas\",\n",
    "    \"Caanada\": \"Canada\",\n",
    "    \"Canadad\": \"Canada\",\n",
    "    \"French\": \"France\",\n",
    "    \"Hanoi Vietnam\": \"Viet Nam\",\n",
    "    \"Ho Chi Min\": \"Viet Nam\",\n",
    "    \"Hong Kong\": \"China, Hong Kong Special Administrative Region\",\n",
    "    \"I Go\": None,\n",
    "    \"Italiana\": \"Italy\",\n",
    "    \"Keenya\": \"Kenya\",\n",
    "    \"Kenyan\": \"Kenya\",\n",
    "    \"Kiambu\": \"Kenya\",\n",
    "    \"Lagos\": \"Nigeria\",\n",
    "    \"Lceland\": \"Iceland\",\n",
    "    \"Mexican\": \"Mexico\",\n",
    "    \"Micronesia\": \"Micronesia (Federated States of)\",\n",
    "    \"Mironesi\": \"Micronesia (Federated States of)\",\n",
    "    \"Mironesia\": \"Micronesia (Federated States of)\",\n",
    "    \"Morroco\": \"Morocco\",\n",
    "    \"Muranga\": \"Kenya\",\n",
    "    \"Nairobi Nairobi\": \"Kenya\",\n",
    "    \"Netherlands\": \"Netherlands (Kingdom of the)\",\n",
    "    \"Nigerian\": \"Nigeria\",\n",
    "    \"Nigeriia\": \"Nigeria\",\n",
    "    \"Niheria\": \"Nigeria\",\n",
    "    \"Nugeria\": \"Nigeria\",\n",
    "    \"Nyari\": \"Kenya\",\n",
    "    \"Owow Disable Abilities Off Level Up\": None,\n",
    "    \"Pakisan\": \"Pakistan\",\n",
    "    \"Pakisatn\": \"Pakistan\",\n",
    "    \"Pakistain\": \"Pakistan\",\n",
    "    \"Paksitan\": \"Pakistan\",\n",
    "    \"Phillipines\": \"Philippines\",\n",
    "    \"Punjab\": \"Pakistan\",\n",
    "    \"South Afica\": \"South Africa\",\n",
    "    \"South Afria\": \"South Africa\",\n",
    "    \"South African\": \"South Africa\",\n",
    "    \"Southern Africa\": \"South Africa\",\n",
    "    \"South Korea\": \"Republic of Korea\",\n",
    "    \"Tanzania\": \"United Republic of Tanzania\",\n",
    "    \"Trinidad And Tobago\": \"Trinidad and Tobago\",\n",
    "    \"Turkey\": \"Türkiye\",\n",
    "    \"Ua\": \"Ukraine\",\n",
    "    \"Uae\": \"United Arab Emirates\",\n",
    "    \"Ugnd\": \"Uganda\",\n",
    "    \"Uk\": \"United Kingdom of Great Britain and Northern Ireland\",\n",
    "    \"United Kingdom\": \"United Kingdom of Great Britain and Northern Ireland\",\n",
    "    \"Ukaine\": \"Ukraine\",\n",
    "    \"United States\": \"United States of America\",\n",
    "    \"Usa\": \"United States of America\",\n",
    "    \"Venezuela\": \"Venezuela (Bolivarian Republic of)\",\n",
    "    \"Veitnam\": \"Viet Nam\",\n",
    "    \"Vienam\": \"Viet Nam\",\n",
    "    \"Vietam\": \"Viet Nam\",\n",
    "    \"Vietnam\": \"Viet Nam\",\n",
    "    \"Vietname\": \"Viet Nam\",\n",
    "    \"Viietnam\": \"Viet Nam\",\n",
    "    \"Vitenam\": \"Viet Nam\",\n",
    "    \"Vitnam\": \"Viet Nam\",\n",
    "    \"Viwtnam\": \"Viet Nam\",\n",
    "}\n",
    "\n",
    "\n",
    "def fix_location_country(country: str) -> str:\n",
    "    \"\"\"Format the location_country attribute string.\n",
    "\n",
    "    Some countries are misspelled or inconsistently formatted.\n",
    "\n",
    "    Args:\n",
    "        country: The original string annotation\n",
    "\n",
    "    Return:\n",
    "        The re-formatted string\n",
    "    \"\"\"\n",
    "    if pd.isna(country):\n",
    "        return country\n",
    "    if country in loc_country_name_mapping:\n",
    "        return loc_country_name_mapping[country]\n",
    "    country_fmt = country.strip().title()\n",
    "    if country_fmt in loc_country_name_mapping:\n",
    "        return loc_country_name_mapping[country_fmt]\n",
    "    else:\n",
    "        return country_fmt\n",
    "\n",
    "\n",
    "# Apply normalization to DataFrame before building value maps\n",
    "df['location_country'] = df['location_country'].apply(fix_location_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Define data processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KEYPOINTS = 33\n",
    "\n",
    "# fmt: off\n",
    "KEYPOINTS = [\n",
    "    \"Nose\",                # 0\n",
    "    \"Right eye inner\",     # 1\n",
    "    \"Right eye\",           # 2\n",
    "    \"Right eye outer\",     # 3\n",
    "    \"Left eye inner\",      # 4\n",
    "    \"Left eye\",            # 5 \n",
    "    \"Left eye outer\",      # 6\n",
    "    \"Right ear\",           # 7\n",
    "    \"Left ear\",            # 8\n",
    "    \"Mouth right\",         # 9\n",
    "    \"Mouth left\",          # 10\n",
    "    \"Right shoulder\",      # 11\n",
    "    \"Left shoulder\",       # 12\n",
    "    \"Right elbow\",         # 13\n",
    "    \"Left elbow\",          # 14\n",
    "    \"Right wrist\",         # 15\n",
    "    \"Left wrist\",          # 16\n",
    "    \"Right pinky knuckle\", # 17\n",
    "    \"Left pinky knuckle\",  # 18\n",
    "    \"Right index knuckle\", # 19\n",
    "    \"Left index knuckle\",  # 20\n",
    "    \"Right thumb knuckle\", # 21\n",
    "    \"Left thumb knuckle\",  # 22\n",
    "    \"Right hip\",           # 23\n",
    "    \"Left hip\",            # 24\n",
    "    \"Right knee\",          # 25\n",
    "    \"Left knee\",           # 26\n",
    "    \"Right ankle\",         # 27\n",
    "    \"Left ankle\",          # 28\n",
    "    \"Right heel\",          # 29\n",
    "    \"Left heel\",           # 30\n",
    "    \"Right foot index\",    # 31\n",
    "    \"Left foot index\",     # 32\n",
    "]\n",
    "\n",
    "SKELETON = [\n",
    "    11, 12, 11, 13, 13, 15, 12, 14, 14, 16, 12, 24, 11, 23, 23, 24,\n",
    "    24, 26, 26, 28, 23, 25, 25, 27, 27, 29, 29, 31, 28, 30, 30, 32,\n",
    "    31, 27, 32, 28, 16, 18, 15, 17, 19, 17, 18, 20, 16, 20, 15, 19, 15, 21, 16, 22,\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "def build_segments_value_map(df: pd.DataFrame) -> dict[str, int]:\n",
    "    \"\"\"Build value map for segment classes from the DataFrame.\"\"\"\n",
    "    all_classes = set()\n",
    "    for segments in df['segments'].dropna():\n",
    "        for seg in segments:\n",
    "            all_classes.add(seg['class_name'])\n",
    "    sorted_classes = sort_by_prefix(list(all_classes))\n",
    "    return {make_internal_name(c): i for i, c in enumerate(sorted_classes)}\n",
    "\n",
    "\n",
    "# Build segments value map from data\n",
    "segments_value_map = build_segments_value_map(df)\n",
    "\n",
    "\n",
    "def process_keypoints(keypoints: dict, person_bbox: list, image_width: int, image_height: int):\n",
    "    \"\"\"Convert keypoints to 3LC format.\n",
    "    \n",
    "    The person_bbox is used as the bounding box for the keypoints instance.\n",
    "    \"\"\"\n",
    "    keypoints = {make_internal_name(kpt_name): v for kpt_name, v in keypoints.items()}\n",
    "    \n",
    "    # Convert person_bbox from [x, y, w, h] to [x0, y0, x1, y1]\n",
    "    person_bbox_xyxy = [person_bbox[0], person_bbox[1], \n",
    "                        person_bbox[0] + person_bbox[2], person_bbox[1] + person_bbox[3]]\n",
    "    \n",
    "    kpts_arr = np.zeros((NUM_KEYPOINTS, 3), dtype=np.float32)\n",
    "    for i, kpt_name in enumerate(KEYPOINTS):\n",
    "        if kpt_name not in keypoints:\n",
    "            continue\n",
    "        x, y, viz = keypoints[kpt_name]\n",
    "        viz = 2 if viz else 0\n",
    "        kpts_arr[i, :] = [x, y, viz]\n",
    "\n",
    "    instances = tlc.Keypoints2DInstances.create_empty(\n",
    "        image_width=image_width,\n",
    "        image_height=image_height,\n",
    "        include_keypoint_visibilities=True,\n",
    "        include_instance_bbs=True,\n",
    "    )\n",
    "    instances.add_instance(keypoints=kpts_arr, label=0, bbox=person_bbox_xyxy)\n",
    "    return instances.to_row()\n",
    "\n",
    "\n",
    "def process_segments(segments: list, image_width: int, image_height: int):\n",
    "    \"\"\"Convert segments to 3LC format.\"\"\"\n",
    "    \n",
    "    def group_segments_by_class(segments):\n",
    "        grouped: dict[str, list[list]] = defaultdict(list)\n",
    "        for segment in segments:\n",
    "            class_name = make_internal_name(segment[\"class_name\"])\n",
    "            poly = [[p[\"x\"], p[\"y\"]] for p in segment[\"polygon\"]]\n",
    "            flattened = [coord for point in poly for coord in point]\n",
    "            grouped[class_name].append(flattened)\n",
    "        return grouped\n",
    "\n",
    "    masks, labels = [], []\n",
    "    for class_name, polygons in group_segments_by_class(segments).items():\n",
    "        mask = tlc.SegmentationHelper.mask_from_polygons(polygons, image_height, image_width)\n",
    "        masks.append(mask)\n",
    "        labels.append(segments_value_map[class_name])\n",
    "\n",
    "    return tlc.SegmentationMasksDict(\n",
    "        image_width=image_width,\n",
    "        image_height=image_height,\n",
    "        masks=np.stack(masks, axis=-1),\n",
    "        instance_properties={\"label\": labels},\n",
    "    )\n",
    "\n",
    "\n",
    "def process_face_bbox(face_bbox: list, image_width: int, image_height: int):\n",
    "    \"\"\"Convert face bounding box to 3LC format.\"\"\"\n",
    "    return {\n",
    "        tlc.IMAGE_WIDTH: image_width,\n",
    "        tlc.IMAGE_HEIGHT: image_height,\n",
    "        tlc.BOUNDING_BOX_LIST: [{\n",
    "            tlc.X0: face_bbox[0],\n",
    "            tlc.Y0: face_bbox[1],\n",
    "            tlc.X1: face_bbox[2],  # Note: CSV stores as [x, y, w, h]\n",
    "            tlc.Y1: face_bbox[3],\n",
    "            tlc.LABEL: 0,\n",
    "        }],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_value(value, col_name: str, value_maps: dict):\n",
    "    \"\"\"Convert a raw value to the format expected by 3LC.\n",
    "    \n",
    "    For categorical columns, maps string values to integer indices.\n",
    "    For list columns, maps each value in the list.\n",
    "    \"\"\"\n",
    "    # Handle NaN values for scalar types - convert to None for proper handling\n",
    "    if not isinstance(value, (list, dict)) and pd.isna(value):\n",
    "        return None\n",
    "    \n",
    "    col_type = detect_column_type(col_name, df[col_name])\n",
    "    \n",
    "    if col_type == 'numeric':\n",
    "        return value\n",
    "    \n",
    "    if col_type == 'string':\n",
    "        return value\n",
    "    \n",
    "    if col_type == 'categorical_list':\n",
    "        value_map = value_maps.get(col_name)\n",
    "        if value_map is None:\n",
    "            return value\n",
    "        return [value_map[make_internal_name(v)][0] for v in value]  # [0] gets the index\n",
    "    \n",
    "    if col_type == 'categorical':\n",
    "        value_map = value_maps.get(col_name)\n",
    "        if value_map is None:\n",
    "            return value\n",
    "        return value_map[make_internal_name(value)][0]  # [0] gets the index\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built value maps for 29 categorical columns\n"
     ]
    }
   ],
   "source": [
    "# Build value maps for all categorical columns\n",
    "value_maps = {}\n",
    "for col_name in COLUMNS_TO_INGEST:\n",
    "    col_type = detect_column_type(col_name, df[col_name])\n",
    "    if col_type in ('categorical', 'categorical_list'):\n",
    "        is_list = col_type == 'categorical_list'\n",
    "        value_maps[col_name] = build_value_map(df[col_name], is_list=is_list)\n",
    "\n",
    "print(f\"Built value maps for {len(value_maps)} categorical columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Row iterator\n",
    "\n",
    "This is the main loop where we iterate over DataFrame rows, process each row, and yield them for the TableWriter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_iterator():\n",
    "    \"\"\"Iterate over DataFrame rows, yielding processed rows for 3LC.\"\"\"\n",
    "    df_subset = df if MAX_SAMPLES is None else df.head(MAX_SAMPLES)\n",
    "    \n",
    "    for idx, csv_row in tqdm(df_subset.iterrows(), total=len(df_subset), desc=\"Processing rows\"):\n",
    "        image_width = int(csv_row['image_width'])\n",
    "        image_height = int(csv_row['image_height'])\n",
    "        \n",
    "        # Build absolute image path and convert to relative 3LC URL\n",
    "        image_path = FHIBE_ROOT / csv_row['filepath']\n",
    "        image_url = tlc.Url(image_path).to_relative().to_str()\n",
    "        \n",
    "        # Build the output row with special columns\n",
    "        row = {\n",
    "            'image': image_url,\n",
    "            'keypoints': process_keypoints(csv_row['keypoints'], csv_row['person_bbox'], image_width, image_height),\n",
    "            'segments': process_segments(csv_row['segments'], image_width, image_height),\n",
    "            'face_bbox': process_face_bbox(csv_row['face_bbox'], image_width, image_height),\n",
    "        }\n",
    "        \n",
    "        # Add all other columns with appropriate conversions\n",
    "        for col_name in COLUMNS_TO_INGEST:\n",
    "            if col_name in SPECIAL_COLUMNS:\n",
    "                continue  # Already handled above\n",
    "            row[col_name] = convert_value(csv_row[col_name], col_name, value_maps)\n",
    "        \n",
    "        yield row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Define column schemas\n",
    "\n",
    "We are now ready to define our schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built schemas for 42 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tw/8h06_93x1xb6cd3kf0r2frhc0000gn/T/ipykernel_48820/1225914675.py:14: DeprecationWarning: label_value_map is deprecated, use classes instead\n",
      "  \"face_bbox\": tlc.BoundingBoxListSchema(\n",
      "/var/folders/tw/8h06_93x1xb6cd3kf0r2frhc0000gn/T/ipykernel_48820/1225914675.py:20: DeprecationWarning: label_value_map is deprecated, use classes instead\n",
      "  \"segments\": tlc.SegmentationSchema(\n"
     ]
    }
   ],
   "source": [
    "# Default schema args: hidden by default and read-only in UI\n",
    "default_schema_args = {\"default_visible\": False, \"writable\": False}\n",
    "\n",
    "# Build schemas for special columns\n",
    "special_schemas = {\n",
    "    \"image\": tlc.ImageUrlSchema(),\n",
    "    \"keypoints\": tlc.Keypoints2DSchema(\n",
    "        classes=[\"person\"],\n",
    "        num_keypoints=NUM_KEYPOINTS,\n",
    "        lines=SKELETON,\n",
    "        point_attributes=KEYPOINTS,\n",
    "        include_per_point_visibility=True,\n",
    "    ),\n",
    "    \"face_bbox\": tlc.BoundingBoxListSchema(\n",
    "        label_value_map={0: tlc.MapElement(\"face\")},\n",
    "        include_segmentation=False,\n",
    "        x1_number_role=tlc.NUMBER_ROLE_BB_SIZE_X,\n",
    "        y1_number_role=tlc.NUMBER_ROLE_BB_SIZE_Y,\n",
    "    ),\n",
    "    \"segments\": tlc.SegmentationSchema(\n",
    "        label_value_map={v: tlc.MapElement(k) for k, v in segments_value_map.items()},\n",
    "        sample_type=tlc.InstanceSegmentationMasks.sample_type,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Infer schemas for all other columns\n",
    "inferred_schemas = {}\n",
    "for col_name in COLUMNS_TO_INGEST:\n",
    "    if col_name in SPECIAL_COLUMNS:\n",
    "        continue\n",
    "    schema = infer_schema(col_name, df[col_name], default_schema_args)\n",
    "    if schema is not None:\n",
    "        inferred_schemas[col_name] = schema\n",
    "\n",
    "# Combine all schemas\n",
    "schemas = {**special_schemas, **inferred_schemas}\n",
    "print(f\"Built schemas for {len(schemas)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bj30bvm2b4m",
   "metadata": {},
   "source": [
    "## Preview a sample row\n",
    "\n",
    "Before writing all rows, let's preview a single row to verify the data looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0clpgyxlpk",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|          | 0/10941 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample row keys (42): ['image', 'keypoints', 'segments', 'face_bbox', 'aperture_value', 'camera_distance', 'camera_position', 'focal_length', 'iso_speed_ratings', 'lighting']...\n",
      "\n",
      "Image: /Users/gudbrand/data/fhibe/data/raw/fhibe_downsampled/9ec2ab4d-8b28-45ac-8c9a-ba9e5b04ad16/d9df58a0-70a7-4447-bd60-d761499a110e/main_d9df58a0-70a7-4447-bd60-d761499a110e.png\n",
      "Subject ID: 9ec2ab4d-8b28-45ac-8c9a-ba9e5b04ad16\n",
      "Age: 24\n",
      "Scene: 12\n",
      "Segments keys: ['image_width', 'image_height', 'masks', 'instance_properties']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the first row\n",
    "sample_row = next(row_iterator())\n",
    "print(f\"Sample row keys ({len(sample_row)}): {list(sample_row.keys())[:10]}...\")\n",
    "print(f\"\\nImage: {sample_row['image']}\")\n",
    "print(f\"Subject ID: {sample_row['subject_id']}\")\n",
    "print(f\"Age: {sample_row['age']}\")\n",
    "print(f\"Scene: {sample_row['scene']}\")\n",
    "print(f\"Segments keys: {list(sample_row['segments'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Write the Table\n",
    "\n",
    "Finally, we create a `TableWriter`, and add our rows to the Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  14%|█▍        | 1560/10941 [02:56<17:42,  8.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m table_writer = tlc.TableWriter(\n\u001b[32m      2\u001b[39m     table_name=TABLE_NAME,\n\u001b[32m      3\u001b[39m     dataset_name=DATASET_NAME,\n\u001b[32m      4\u001b[39m     project_name=PROJECT_NAME,\n\u001b[32m      5\u001b[39m     column_schemas=schemas,\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtable_writer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m table = table_writer.finalize()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mrow_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m col_name \u001b[38;5;129;01min\u001b[39;00m SPECIAL_COLUMNS:\n\u001b[32m     24\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Already handled above\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     row[col_name] = \u001b[43mconvert_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_row\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_maps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m row\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mconvert_value\u001b[39m\u001b[34m(value, col_name, value_maps)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m pd.isna(value):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m col_type = \u001b[43mdetect_column_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m col_type == \u001b[33m'\u001b[39m\u001b[33mnumeric\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mdetect_column_type\u001b[39m\u001b[34m(col_name, series)\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mnumeric\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Check if column contains lists (already parsed)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m sample = \u001b[43mseries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.iloc[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(series.dropna()) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mspecial\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/3lc-examples/.venv/lib/python3.12/site-packages/pandas/core/series.py:5820\u001b[39m, in \u001b[36mSeries.dropna\u001b[39m\u001b[34m(self, axis, inplace, how, ignore_index)\u001b[39m\n\u001b[32m   5809\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m   5810\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdropna\u001b[39m(\n\u001b[32m   5811\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5816\u001b[39m     ignore_index: \u001b[38;5;28mbool\u001b[39m = ...,\n\u001b[32m   5817\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5818\u001b[39m     ...\n\u001b[32m-> \u001b[39m\u001b[32m5820\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdropna\u001b[39m(\n\u001b[32m   5821\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5822\u001b[39m     *,\n\u001b[32m   5823\u001b[39m     axis: Axis = \u001b[32m0\u001b[39m,\n\u001b[32m   5824\u001b[39m     inplace: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   5825\u001b[39m     how: AnyAll | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5826\u001b[39m     ignore_index: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   5827\u001b[39m ) -> Series | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5828\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5829\u001b[39m \u001b[33;03m    Return a new Series with missing values removed.\u001b[39;00m\n\u001b[32m   5830\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5892\u001b[39m \u001b[33;03m    dtype: object\u001b[39;00m\n\u001b[32m   5893\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   5894\u001b[39m     inplace = validate_bool_kwarg(inplace, \u001b[33m\"\u001b[39m\u001b[33minplace\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "table_writer = tlc.TableWriter(\n",
    "    table_name=TABLE_NAME,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    project_name=PROJECT_NAME,\n",
    "    column_schemas=schemas,\n",
    ")\n",
    "\n",
    "for row in row_iterator():\n",
    "    table_writer.add_row(row)\n",
    "\n",
    "table = table_writer.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73af2dny2rh",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Created table with {len(table)} rows\")\n",
    "print(f\"Table URL: {table.url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
