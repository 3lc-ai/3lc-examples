{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Load FHIBE Dataset\n",
    "\n",
    "This notebook loads the Sony AI's \"Fair Human-Centric Image Benchmark\" dataset as a 3LC Table, including keypoints, segmentation, bounding boxes, as well as rich subject metadata.\n",
    "\n",
    "![img](../images/fhibe.png)\n",
    "\n",
    "<!-- Tags: [\"keypoints\", \"instance-segmentation\", \"object-detection\"] -->\n",
    "\n",
    "To download the dataset, you need to register at [fairnessbenchmark.ai.sony](https://fairnessbenchmark.ai.sony/). To read the original research paper, see [here](https://www.nature.com/articles/s41586-025-09716-2).\n",
    "\n",
    "Several versions of the dataset exist, for this tutorial we will use version from `fhibe.20250716.u.gT5_rFTA_downsampled_public.tar.gz`, but the ingestion script should work for any version of the dataset, as the internal layout of the dataset is the same.\n",
    "\n",
    "We include as much as possible of the metadata contained in the dataset, omitting only a few attributes in the name of simplicity, specifically the `<attr>_QA_annotator_id` fields have been left out.\n",
    "\n",
    "The data can be categorized as follows:\n",
    "- Main image\n",
    "- Geometric annotations (instance segmentations, keypoints, facial bounding box)\n",
    "- Image-level metadata (shutter speed, camera manufacturer, weather conditions, etc.)\n",
    "- Subject-level metadata (ancestry, hair color, age, etc.)\n",
    "\n",
    "This script reads all this data from per-subject JSON files and converts it to a format suitable for a 3LC Table. Several of the columns are stored as \"categorical strings\" (e.g. hair color \"Blond\", \"Gray\", \"White\", ...), these values are converted to integers, with their corresponding string values stored in the schema. This makes it easier to filter and work with these values in the 3LC Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q 3lc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tlc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Project setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"3LC Tutorials - FHIBE\"\n",
    "DATASET_NAME = \"FHIBE\"\n",
    "TABLE_NAME = \"initial\"\n",
    "MAX_SAMPLES = None\n",
    "DOWNLOAD_PATH = \"D:/Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FHIBE_ROOT = Path(DOWNLOAD_PATH) / \"fhibe\"\n",
    "DATA_ROOT = FHIBE_ROOT / \"data/raw/fhibe_downsampled\"\n",
    "CSV_FILE = FHIBE_ROOT / \"data/processed/fhibe_downsampled/fhibe_downsampled.csv\"\n",
    "\n",
    "if not FHIBE_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"FHIBE_ROOT does not exist: {FHIBE_ROOT}\")\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"DATA_ROOT does not exist: {DATA_ROOT}\")\n",
    "\n",
    "if not CSV_FILE.exists():\n",
    "    raise FileNotFoundError(f\"CSV_FILE does not exist: {CSV_FILE}\")\n",
    "\n",
    "annotation_paths = list(DATA_ROOT.glob(\"**/main_annos_*.json\"))\n",
    "print(f\"Found {len(annotation_paths)} annotation files\")\n",
    "\n",
    "person_bbox_df = pd.read_csv(CSV_FILE, usecols=[\"image_id\", \"subject_id\", \"person_bbox\"]).set_index(\n",
    "    [\"image_id\", \"subject_id\"]\n",
    ")\n",
    "\n",
    "assert len(person_bbox_df) == len(annotation_paths), \"expected one person_bbox per image-subject pair\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Prepare value mappings\n",
    "\n",
    "We performed an initial scan of the dataset to determine all unique values for all categorical columns. The string values are mapped to arbitrary integer values for storage in a 3LC Table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Image-level mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_position_value_map = {\"Typical\": 0, \"Atypical High\": 1, \"Atypical Low\": 2}\n",
    "camera_distance_value_map = {\"CD I\": 0, \"CD II\": 1, \"CD III\": 2, \"CD IV\": 3, \"CD V\": 4}\n",
    "lighting_value_map = {\n",
    "    \"Lighting from above the head/face\": 0,\n",
    "    \"Lighting from below the head/face\": 1,\n",
    "    \"Lighting from in front of the head/face\": 2,\n",
    "    \"Lighting from behind the head/face\": 3,\n",
    "    \"Lighting from the left of the head/face\": 4,\n",
    "    \"Lighting from the right of the head/face\": 5,\n",
    "}\n",
    "weather_value_map = {\"Fog\": 0, \"Haze\": 1, \"Snow/hail\": 2, \"Rain\": 3, \"Humid\": 4, \"Cloud\": 5, \"Clear\": 6}\n",
    "user_hour_captured_value_map = {\"0000-0559\": 0, \"0600-1159\": 1, \"1200-1759\": 2, \"1800-2359\": 3}\n",
    "scene_value_map = {\n",
    "    \"Outdoor Water, ice, snow\": 0,\n",
    "    \"Outdoor Mountains, hills, desert, sky\": 1,\n",
    "    \"Outdoor Forest, field, jungle\": 2,\n",
    "    \"Outdoor Man-made elements\": 3,\n",
    "    \"Outdoor Transportation\": 4,\n",
    "    \"Outdoor Cultural or historical building/place\": 5,\n",
    "    \"Outdoor Sports fields, parks, leisure spaces\": 6,\n",
    "    \"Outdoor Industrial and construction\": 7,\n",
    "    \"Outdoor Houses, cabins, gardens, and farms\": 8,\n",
    "    \"Outdoor Commercial buildings, shops, markets, cities, and towns\": 9,\n",
    "    \"Indoor Shopping and dining\": 10,\n",
    "    \"Indoor Workplace\": 11,\n",
    "    \"Indoor Home or hotel\": 12,\n",
    "    \"Indoor Transportation\": 13,\n",
    "    \"Indoor Sports and leisure\": 14,\n",
    "    \"Indoor Cultural\": 15,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Subject-level mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_value_map = {\n",
    "    \"Face skin\": 0.0,\n",
    "    \"Upper body skin\": 1.0,\n",
    "    \"Right eye\": 2.0,\n",
    "    \"Nose\": 3.0,\n",
    "    \"Upper lip\": 4.0,\n",
    "    \"Lower lip\": 5.0,\n",
    "    \"Inner mouth\": 6.0,\n",
    "    \"Left shoe\": 7.0,\n",
    "    \"Right shoe\": 8.0,\n",
    "    \"Left arm skin\": 9.0,\n",
    "    \"Upper body clothes\": 10.0,\n",
    "    \"Lower body clothes\": 11.0,\n",
    "    \"Sock or legwarmer\": 12.0,\n",
    "    \"Jewelry or timepiece\": 13.0,\n",
    "    \"Right arm skin\": 14.0,\n",
    "    \"Left leg skin\": 15.0,\n",
    "    \"Right leg skin\": 16.0,\n",
    "    \"Head hair\": 17.0,\n",
    "    \"Left eyebrow\": 18.0,\n",
    "    \"Right eyebrow\": 19.0,\n",
    "    \"Left eye\": 20.0,\n",
    "    \"Bag\": 21.0,\n",
    "    \"Eyewear\": 22.0,\n",
    "    \"Full body clothes\": 23.0,\n",
    "    \"Headwear\": 24.0,\n",
    "    \"Mask\": 25.0,\n",
    "    \"Neckwear\": 26.0,\n",
    "    \"Glove\": 27.0,\n",
    "}\n",
    "\n",
    "pronoun_value_map = {\n",
    "    \"She/her/hers\": 0,\n",
    "    \"He/him/his\": 1,\n",
    "    \"They/them/their\": 2,\n",
    "    \"Ze/zir/zirs\": 3,\n",
    "    \"None of the above\": 4,\n",
    "    \"Prefer not to say\": 5,\n",
    "}\n",
    "\n",
    "head_pose_value_map = {\n",
    "    \"Typical\": 0,\n",
    "    \"Atypical\": 1,\n",
    "}\n",
    "\n",
    "facial_marks_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Tattoos\": 1,\n",
    "    \"Birthmarks\": 2,\n",
    "    \"Scars\": 3,\n",
    "    \"Burns\": 4,\n",
    "    \"Growths\": 5,\n",
    "    \"Make-up\": 6,\n",
    "    \"Face paint\": 7,\n",
    "    \"Acne\": 8,\n",
    "    \"Not listed\": 9,\n",
    "    \"Free-text\": 10,\n",
    "}\n",
    "\n",
    "ancestry_value_map = {\n",
    "    \"Africa\": 0,\n",
    "    \"Eastern Africa\": 1,\n",
    "    \"Northern Africa\": 2,\n",
    "    \"Middle Africa\": 3,\n",
    "    \"Southern Africa\": 4,\n",
    "    \"Western Africa\": 5,\n",
    "    \"Americas\": 6,\n",
    "    \"Caribbean\": 7,\n",
    "    \"Central America\": 8,\n",
    "    \"South America\": 9,\n",
    "    \"Northern America\": 10,\n",
    "    \"Asia\": 11,\n",
    "    \"Central Asia\": 12,\n",
    "    \"Eastern Asia\": 13,\n",
    "    \"South-eastern Asia\": 14,\n",
    "    \"Southern Asia\": 15,\n",
    "    \"Western Asia\": 16,\n",
    "    \"Europe\": 17,\n",
    "    \"Eastern Europe\": 18,\n",
    "    \"Northern Europe\": 19,\n",
    "    \"Southern Europe\": 20,\n",
    "    \"Western Europe\": 21,\n",
    "    \"Oceania\": 22,\n",
    "    \"Australia and New Zealand\": 23,\n",
    "    \"Polynesia\": 24,\n",
    "}\n",
    "\n",
    "skin_color_value_map = {\n",
    "    \"[102, 78, 65]\": 0,\n",
    "    \"[136, 105, 81]\": 1,\n",
    "    \"[164, 131, 103]\": 2,\n",
    "    \"[175, 148, 120]\": 3,\n",
    "    \"[189, 163, 137]\": 4,\n",
    "    \"[198, 180, 157]\": 5,\n",
    "}\n",
    "\n",
    "haircolor_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Very light blond\": 1,\n",
    "    \"Light blond\": 2,\n",
    "    \"Blond\": 3,\n",
    "    \"Dark blond\": 4,\n",
    "    \"Light brown to medium brown\": 5,\n",
    "    \"Dark brown/black\": 6,\n",
    "    \"Red\": 7,\n",
    "    \"Gray\": 8,\n",
    "    \"Red blond\": 9,\n",
    "    \"White\": 10,\n",
    "    \"Not listed\": 11,\n",
    "    \"Free-text\": 12,\n",
    "}\n",
    "\n",
    "hairstyle_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Buzz cut\": 1,\n",
    "    \"Up (Short)\": 10,\n",
    "    \"Half-up (Short)\": 11,\n",
    "    \"Down (Short)\": 12,\n",
    "    \"Not listed(Short)\": 13,\n",
    "    \"Up (Medium)\": 14,\n",
    "    \"Half-up (Medium)\": 15,\n",
    "    \"Down (Medium)\": 2,\n",
    "    \"Not listed(Medium)\": 3,\n",
    "    \"Up (Long)\": 4,\n",
    "    \"Half-up (Long)\": 5,\n",
    "    \"Down (Long)\": 6,\n",
    "    \"Not listed(Long)\": 7,\n",
    "    \"Not listed\": 8,\n",
    "    \"Free-text\": 9,\n",
    "}\n",
    "\n",
    "facial_hairstyle_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Beard\": 1,\n",
    "    \"Mustache\": 2,\n",
    "    \"Goatee\": 3,\n",
    "}\n",
    "\n",
    "hair_type_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Straight\": 1,\n",
    "    \"Wavy\": 2,\n",
    "    \"Curly\": 3,\n",
    "    \"Kinky-coily\": 4,\n",
    "    \"Not listed\": 5,\n",
    "    \"Free-text\": 6,\n",
    "}\n",
    "\n",
    "action_body_pose_value_map = {\n",
    "    \"Standing\": 0,\n",
    "    \"Sitting\": 1,\n",
    "    \"Walking\": 2,\n",
    "    \"Bending/bowing\": 3,\n",
    "    \"Lying down/sleeping\": 4,\n",
    "    \"Performing martial/fighting arts\": 5,\n",
    "    \"Dancing\": 6,\n",
    "    \"Running/jogging\": 7,\n",
    "    \"Crouching/kneeling\": 8,\n",
    "    \"Getting up\": 9,\n",
    "    \"Jumping/leaping\": 10,\n",
    "    \"Falling down\": 11,\n",
    "    \"Crawling\": 12,\n",
    "    \"Swimming\": 13,\n",
    "    \"Not listed\": 14,\n",
    "    \"Free-text\": 15,\n",
    "}\n",
    "\n",
    "action_subject_object_interaction_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Riding\": 1,\n",
    "    \"Driving\": 2,\n",
    "    \"Watching\": 3,\n",
    "    \"Smoking\": 4,\n",
    "    \"Eating\": 5,\n",
    "    \"Drinking\": 6,\n",
    "    \"Opening or closing\": 7,\n",
    "    \"Lifting/picking up or putting down\": 8,\n",
    "    \"Writing/drawing or painting\": 9,\n",
    "    \"Catching or throwing\": 10,\n",
    "    \"Pushing, pulling or extracting\": 11,\n",
    "    \"Putting on or taking off clothing\": 12,\n",
    "    \"Entering or exiting\": 13,\n",
    "    \"Climbing\": 14,\n",
    "    \"Pointing at\": 15,\n",
    "    \"Shooting at\": 16,\n",
    "    \"Digging/shoveling\": 17,\n",
    "    \"Playing with pets/animals\": 18,\n",
    "    \"Playing musical instrument\": 19,\n",
    "    \"Playing\": 20,\n",
    "    \"Using an electronic device\": 21,\n",
    "    \"Cutting or chopping\": 22,\n",
    "    \"Cooking\": 23,\n",
    "    \"Fishing\": 24,\n",
    "    \"Rowing\": 25,\n",
    "    \"Sailing\": 26,\n",
    "    \"Brushing teeth\": 27,\n",
    "    \"Hitting\": 28,\n",
    "    \"Kicking\": 29,\n",
    "    \"Turning\": 30,\n",
    "    \"Not listed\": 31,\n",
    "    \"Free-text\": 32,\n",
    "}\n",
    "\n",
    "eye_color_value_map = {\n",
    "    \"None\": 0,\n",
    "    \"Blue\": 1,\n",
    "    \"Gray\": 2,\n",
    "    \"Green\": 3,\n",
    "    \"Hazel\": 4,\n",
    "    \"Brown\": 5,\n",
    "    \"Red and violet\": 6,\n",
    "    \"Not listed\": 7,\n",
    "    \"Free-text\": 8,\n",
    "}\n",
    "\n",
    "nationality_value_map = {\n",
    "    \"Angolan\": 0,\n",
    "    \"Somali\": 1,\n",
    "    \"Iraqi\": 2,\n",
    "    \"Rwandan\": 3,\n",
    "    \"Swedish\": 4,\n",
    "    \"Portuguese\": 5,\n",
    "    \"Honduran\": 6,\n",
    "    \"Jamaican\": 7,\n",
    "    \"Italian\": 8,\n",
    "    \"German\": 9,\n",
    "    \"Vietnamese\": 10,\n",
    "    \"Romanian\": 11,\n",
    "    \"Hungarian\": 12,\n",
    "    \"Nigerian\": 13,\n",
    "    \"Malaysian\": 14,\n",
    "    \"Ethiopian\": 15,\n",
    "    \"Ecuadorean\": 16,\n",
    "    \"Brazilian\": 17,\n",
    "    \"Icelandic\": 18,\n",
    "    \"Dutch\": 19,\n",
    "    \"South African\": 20,\n",
    "    \"Dominican\": 21,\n",
    "    \"Argentine\": 22,\n",
    "    \"Chinese\": 23,\n",
    "    \"American\": 24,\n",
    "    \"Jordanian\": 25,\n",
    "    \"Greek\": 26,\n",
    "    \"Danish\": 27,\n",
    "    \"Ugandan\": 28,\n",
    "    \"Cameroonian\": 29,\n",
    "    \"Anguillan\": 30,\n",
    "    \"Japanese\": 31,\n",
    "    \"Indian\": 32,\n",
    "    \"Lithuanian\": 33,\n",
    "    \"Algerian\": 34,\n",
    "    \"Venezuelan\": 35,\n",
    "    \"Norwegian\": 36,\n",
    "    \"Pakistani\": 37,\n",
    "    \"Moroccan\": 38,\n",
    "    \"Ivorian\": 39,\n",
    "    \"French\": 40,\n",
    "    \"Colombian\": 41,\n",
    "    \"Zambian\": 42,\n",
    "    \"Ghanaian\": 43,\n",
    "    \"Tanzanian\": 44,\n",
    "    \"British\": 45,\n",
    "    \"English\": 46,\n",
    "    \"Congolese (DRC)\": 47,\n",
    "    \"Not listed\": 48,\n",
    "    \"South Korean\": 49,\n",
    "    \"Nepalese\": 50,\n",
    "    \"Mexican\": 51,\n",
    "    \"Egyptian\": 52,\n",
    "    \"Indonesian\": 53,\n",
    "    \"Zimbabwean\": 54,\n",
    "    \"Eritrean\": 55,\n",
    "    \"Tunisian\": 56,\n",
    "    \"Ukrainian\": 57,\n",
    "    \"Filipino\": 58,\n",
    "    \"Haitian\": 59,\n",
    "    \"Puerto Rican\": 60,\n",
    "    \"Kazakh\": 61,\n",
    "    \"Fijian\": 62,\n",
    "    \"Canadian\": 63,\n",
    "    \"Australian\": 64,\n",
    "    \"Bulgarian\": 65,\n",
    "    \"Kenyan\": 66,\n",
    "    \"Slovak\": 67,\n",
    "    \"Irish\": 68,\n",
    "    \"Nigerien\": 69,\n",
    "    \"Spanish\": 70,\n",
    "    \"Citizen of the Dominican Republic\": 71,\n",
    "    \"New Zealander\": 72,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Consolidation of country spelling variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/SonyResearch/fhibe_evaluation_api/blob/main/fhibe_eval_api/datasets/fhibe.py\n",
    "\n",
    "loc_country_name_mapping = {\n",
    "    \"Abgola\": \"Angola\",\n",
    "    \"Abuja\": \"Nigeria\",\n",
    "    \"Argentiina\": \"Argentina\",\n",
    "    \"Australie\": \"Australia\",\n",
    "    \"Autsralia\": \"Australia\",\n",
    "    \"Auustralia\": \"Australia\",\n",
    "    \"Bahamas, The\": \"Bahamas\",\n",
    "    \"Caanada\": \"Canada\",\n",
    "    \"Canadad\": \"Canada\",\n",
    "    \"French\": \"France\",\n",
    "    \"Hanoi Vietnam\": \"Viet Nam\",\n",
    "    \"Ho Chi Min\": \"Viet Nam\",\n",
    "    \"Hong Kong\": \"China, Hong Kong Special Administrative Region\",\n",
    "    \"I Go\": None,\n",
    "    \"Italiana\": \"Italy\",\n",
    "    \"Keenya\": \"Kenya\",\n",
    "    \"Kenyan\": \"Kenya\",\n",
    "    \"Kiambu\": \"Kenya\",\n",
    "    \"Lagos\": \"Nigeria\",\n",
    "    \"Lceland\": \"Iceland\",\n",
    "    \"Mexican\": \"Mexico\",\n",
    "    \"Micronesia\": \"Micronesia (Federated States of)\",\n",
    "    \"Mironesi\": \"Micronesia (Federated States of)\",\n",
    "    \"Mironesia\": \"Micronesia (Federated States of)\",\n",
    "    \"Morroco\": \"Morocco\",\n",
    "    \"Muranga\": \"Kenya\",\n",
    "    \"Nairobi Nairobi\": \"Kenya\",\n",
    "    \"Netherlands\": \"Netherlands (Kingdom of the)\",\n",
    "    \"Nigerian\": \"Nigeria\",\n",
    "    \"Nigeriia\": \"Nigeria\",\n",
    "    \"Niheria\": \"Nigeria\",\n",
    "    \"Nugeria\": \"Nigeria\",\n",
    "    \"Nyari\": \"Kenya\",\n",
    "    \"Owow Disable Abilities Off Level Up\": None,\n",
    "    \"Pakisan\": \"Pakistan\",\n",
    "    \"Pakisatn\": \"Pakistan\",\n",
    "    \"Pakistain\": \"Pakistan\",\n",
    "    \"Paksitan\": \"Pakistan\",\n",
    "    \"Phillipines\": \"Philippines\",\n",
    "    \"Punjab\": \"Pakistan\",\n",
    "    \"South Afica\": \"South Africa\",\n",
    "    \"South Afria\": \"South Africa\",\n",
    "    \"South African\": \"South Africa\",\n",
    "    \"Southern Africa\": \"South Africa\",\n",
    "    \"South Korea\": \"Republic of Korea\",\n",
    "    \"Tanzania\": \"United Republic of Tanzania\",\n",
    "    \"Trinidad And Tobago\": \"Trinidad and Tobago\",\n",
    "    \"Turkey\": \"TÃ¼rkiye\",\n",
    "    \"Ua\": \"Ukraine\",\n",
    "    \"Uae\": \"United Arab Emirates\",\n",
    "    \"Ugnd\": \"Uganda\",\n",
    "    \"Uk\": \"United Kingdom of Great Britain and Northern Ireland\",\n",
    "    \"United Kingdom\": \"United Kingdom of Great Britain and Northern Ireland\",\n",
    "    \"Ukaine\": \"Ukraine\",\n",
    "    \"United States\": \"United States of America\",\n",
    "    \"Usa\": \"United States of America\",\n",
    "    \"Venezuela\": \"Venezuela (Bolivarian Republic of)\",\n",
    "    \"Veitnam\": \"Viet Nam\",\n",
    "    \"Vienam\": \"Viet Nam\",\n",
    "    \"Vietam\": \"Viet Nam\",\n",
    "    \"Vietnam\": \"Viet Nam\",\n",
    "    \"Vietname\": \"Viet Nam\",\n",
    "    \"Viietnam\": \"Viet Nam\",\n",
    "    \"Vitenam\": \"Viet Nam\",\n",
    "    \"Vitnam\": \"Viet Nam\",\n",
    "    \"Viwtnam\": \"Viet Nam\",\n",
    "}\n",
    "\n",
    "\n",
    "def fix_location_country(country: str) -> str:\n",
    "    \"\"\"Format the location_country attribute string.\n",
    "\n",
    "    Some countries are misspelled or inconsistently formatted.\n",
    "\n",
    "    Args:\n",
    "        country: The original string annotation\n",
    "\n",
    "    Return:\n",
    "        The re-formatted string\n",
    "    \"\"\"\n",
    "    if country in loc_country_name_mapping:\n",
    "        return loc_country_name_mapping[country]\n",
    "    country_fmt = country.strip().title()\n",
    "    if country_fmt in loc_country_name_mapping:\n",
    "        return loc_country_name_mapping[country_fmt]\n",
    "    else:\n",
    "        return country_fmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Define data processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(s):\n",
    "    if \". \" in s:\n",
    "        # Many FHIBE strings are numbered, e.g. \"1. Right eye inner\". For readability, remove the numbering\n",
    "        s = s.split(\". \")[1]\n",
    "    # MapElements in 3LC do not support \":\" in the internal name\n",
    "    return s.replace(\":\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KEYPOINTS = 33\n",
    "\n",
    "# fmt: off\n",
    "KEYPOINTS = [\n",
    "    \"Nose\",                # 0\n",
    "    \"Right eye inner\",     # 1\n",
    "    \"Right eye\",           # 2\n",
    "    \"Right eye outer\",     # 3\n",
    "    \"Left eye inner\",      # 4\n",
    "    \"Left eye\",            # 5 \n",
    "    \"Left eye outer\",      # 6\n",
    "    \"Right ear\",           # 7\n",
    "    \"Left ear\",            # 8\n",
    "    \"Mouth right\",         # 9\n",
    "    \"Mouth left\",          # 10\n",
    "    \"Right shoulder\",      # 11\n",
    "    \"Left shoulder\",       # 12\n",
    "    \"Right elbow\",         # 13\n",
    "    \"Left elbow\",          # 14\n",
    "    \"Right wrist\",         # 15\n",
    "    \"Left wrist\",          # 16\n",
    "    \"Right pinky knuckle\", # 17\n",
    "    \"Left pinky knuckle\",  # 18\n",
    "    \"Right index knuckle\", # 19\n",
    "    \"Left index knuckle\",  # 20\n",
    "    \"Right thumb knuckle\", # 21\n",
    "    \"Left thumb knuckle\",  # 22\n",
    "    \"Right hip\",           # 23\n",
    "    \"Left hip\",            # 24\n",
    "    \"Right knee\",          # 25\n",
    "    \"Left knee\",           # 26\n",
    "    \"Right ankle\",         # 27\n",
    "    \"Left ankle\",          # 28\n",
    "    \"Right heel\",          # 29\n",
    "    \"Left heel\",           # 30\n",
    "    \"Right foot index\",    # 31\n",
    "    \"Left foot index\",     # 32\n",
    "]\n",
    "\n",
    "# Add connecting lines between connected keypoints\n",
    "SKELETON = [\n",
    "    11, 12,\n",
    "    11, 13,\n",
    "    13, 15,\n",
    "    12, 14,\n",
    "    14, 16,\n",
    "    12, 24,\n",
    "    11, 23,\n",
    "    23, 24,\n",
    "    24, 26,\n",
    "    26, 28,\n",
    "    23, 25,\n",
    "    25, 27,\n",
    "    27, 29,\n",
    "    29, 31,\n",
    "    28, 30,\n",
    "    30, 32,\n",
    "    31, 27,\n",
    "    32, 28,\n",
    "    16, 18,\n",
    "    15, 17,\n",
    "    19, 17,\n",
    "    18, 20,\n",
    "    16, 20,\n",
    "    15, 19,\n",
    "    15, 21,\n",
    "    16, 22,\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "def process_keypoints(keypoints, bbox, image_width, image_height):\n",
    "    \"\"\"Convert keypoints to 3LC format\"\"\"\n",
    "    keypoints = {clean_str(kpt_name): v for kpt_name, v in keypoints.items()}\n",
    "    kpts_arr = np.zeros((NUM_KEYPOINTS, 3), dtype=np.float32)\n",
    "    for i, kpt_name in enumerate(KEYPOINTS):\n",
    "        if kpt_name not in keypoints:\n",
    "            continue\n",
    "        x, y, viz = keypoints[kpt_name]\n",
    "        viz = 2 if viz else 0\n",
    "        kpts_arr[i, :] = [x, y, viz]\n",
    "\n",
    "    instances = tlc.Keypoints2DInstances.create_empty(\n",
    "        image_width=image_width,\n",
    "        image_height=image_height,\n",
    "        include_keypoint_visibilities=True,\n",
    "        include_instance_bbs=True,\n",
    "    )\n",
    "\n",
    "    instances.add_instance(\n",
    "        keypoints=kpts_arr,\n",
    "        label=0,  # all instances have the same label: \"person\"\n",
    "        bbox=bbox,\n",
    "    )\n",
    "\n",
    "    return instances.to_row()\n",
    "\n",
    "\n",
    "def process_segments(segments, image_width, image_height):\n",
    "    \"\"\"Convert segments to 3LC format\"\"\"\n",
    "\n",
    "    def group_segments_by_class(segments):\n",
    "        grouped: dict[str, list[list[dict]]] = defaultdict(list)\n",
    "        for segment in segments:\n",
    "            class_name = clean_str(segment[\"class_name\"])\n",
    "            poly_2_tuples = [[p[\"x\"], p[\"y\"]] for p in segment[\"polygon\"]]\n",
    "            flattened_poly = [item for sublist in poly_2_tuples for item in sublist]\n",
    "            grouped[class_name].append(flattened_poly)\n",
    "        return grouped\n",
    "\n",
    "    masks = []\n",
    "    labels = []\n",
    "\n",
    "    for class_name, polygons in group_segments_by_class(segments).items():\n",
    "        # Convert the polygon to a flattened list of coordinates\n",
    "        mask = tlc.SegmentationHelper.mask_from_polygons(polygons, image_height, image_width)\n",
    "        masks.append(mask)\n",
    "        labels.append(segments_value_map[class_name])\n",
    "\n",
    "    segs = tlc.SegmentationMasksDict(\n",
    "        image_width=image_width,\n",
    "        image_height=image_height,\n",
    "        masks=np.stack(masks, axis=-1),\n",
    "        instance_properties={\"label\": labels},\n",
    "    )\n",
    "    return segs\n",
    "\n",
    "\n",
    "def process_bboxes(face_bbox, image_width, image_height):\n",
    "    \"\"\"Convert bounding box coordinates to 3LC format\"\"\"\n",
    "    bboxes = {\n",
    "        tlc.IMAGE_WIDTH: image_width,\n",
    "        tlc.IMAGE_HEIGHT: image_height,\n",
    "        tlc.BOUNDING_BOX_LIST: [\n",
    "            {\n",
    "                tlc.X0: face_bbox[0],\n",
    "                tlc.Y0: face_bbox[1],\n",
    "                tlc.X1: face_bbox[2],\n",
    "                tlc.Y1: face_bbox[3],\n",
    "                tlc.LABEL: 0,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def map_categorical(value, mapping):\n",
    "    \"\"\"Helper function to map categorical values to their corresponding integer values\"\"\"\n",
    "    if isinstance(value, list):\n",
    "        return [mapping[clean_str(v)] for v in value]\n",
    "    return mapping[clean_str(value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_annotation(image_annotation):\n",
    "    \"\"\"Process the image annotation dictionary\n",
    "\n",
    "    Categorical or list of categorical values are cleaned and converted to their\n",
    "    corresponding integer values, plain numerical values or strings are left as is.\n",
    "    \"\"\"\n",
    "    image_annotation_dict = {\n",
    "        \"aperture_value\": image_annotation[\"aperture_value\"],\n",
    "        \"camera_distance\": map_categorical(image_annotation[\"camera_distance\"], camera_distance_value_map),\n",
    "        \"camera_position\": map_categorical(image_annotation[\"camera_position\"], camera_position_value_map),\n",
    "        \"focal_length\": image_annotation[\"focal_length\"],\n",
    "        \"iso_speed_ratings\": image_annotation[\"iso_speed_ratings\"],\n",
    "        \"lighting\": map_categorical(image_annotation[\"lighting\"], lighting_value_map),\n",
    "        \"location_country\": fix_location_country(image_annotation[\"location_country\"]),\n",
    "        \"location_region\": image_annotation[\"location_region\"],\n",
    "        \"manufacturer\": image_annotation[\"manufacturer\"],\n",
    "        \"model\": image_annotation[\"model\"],\n",
    "        \"scene\": map_categorical(image_annotation[\"scene\"], scene_value_map),\n",
    "        \"shutter_speed_value\": image_annotation[\"shutter_speed_value\"],\n",
    "        \"user_date_captured\": image_annotation[\"user_date_captured\"],\n",
    "        \"user_hour_captured\": map_categorical(image_annotation[\"user_hour_captured\"], user_hour_captured_value_map),\n",
    "        \"weather\": map_categorical(image_annotation[\"weather\"], weather_value_map),\n",
    "    }\n",
    "\n",
    "    return image_annotation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E501\n",
    "# fmt: off\n",
    "def process_subject_annotation(subject_annotation, image_height, image_width):\n",
    "    \"\"\"Process the subject annotation dictionary\n",
    "    \n",
    "    Categorical or list of categorical values are cleaned and converted to their\n",
    "    corresponding integer values, plain numerical values or strings are left as is.\n",
    "    \"\"\"\n",
    "    subject_annotation_dict = {\n",
    "        \"face_bbox\": process_bboxes(subject_annotation[\"face_bbox\"], image_height, image_width),\n",
    "        \"segments\": process_segments(subject_annotation[\"segments\"], image_height, image_width),\n",
    "        \"keypoints\": process_keypoints(subject_annotation[\"keypoints\"], subject_annotation[\"person_bbox\"], image_height, image_width),\n",
    "        \"subject_id\": subject_annotation[\"subject_id\"],\n",
    "        \"age\": subject_annotation[\"age\"],\n",
    "        \"nationality\": map_categorical(subject_annotation[\"nationality\"], nationality_value_map),\n",
    "        \"ancestry\": map_categorical(subject_annotation[\"ancestry\"], ancestry_value_map),\n",
    "        \"pronoun\": map_categorical(subject_annotation[\"pronoun\"], pronoun_value_map),\n",
    "        \"natural_skin_color\": map_categorical(subject_annotation[\"natural_skin_color\"], skin_color_value_map),\n",
    "        \"apparent_skin_color\": map_categorical(subject_annotation[\"apparent_skin_color\"], skin_color_value_map),\n",
    "        \"hairstyle\": map_categorical(subject_annotation[\"hairstyle\"], hairstyle_value_map),\n",
    "        \"natural_hair_type\": map_categorical(subject_annotation[\"natural_hair_type\"], hair_type_value_map),\n",
    "        \"apparent_hair_type\": map_categorical(subject_annotation[\"apparent_hair_type\"], hair_type_value_map),\n",
    "        \"natural_hair_color\": map_categorical(subject_annotation[\"natural_hair_color\"], haircolor_value_map),\n",
    "        \"apparent_hair_color\": map_categorical(subject_annotation[\"apparent_hair_color\"], haircolor_value_map),\n",
    "        \"facial_hairstyle\": map_categorical(subject_annotation[\"facial_hairstyle\"], facial_hairstyle_value_map),\n",
    "        \"natural_facial_hair_color\": map_categorical(subject_annotation[\"natural_facial_haircolor\"], haircolor_value_map),\n",
    "        \"apparent_facial_hair_color\": map_categorical(subject_annotation[\"apparent_facial_haircolor\"], haircolor_value_map),\n",
    "        \"natural_left_eye_color\": map_categorical(subject_annotation[\"natural_left_eye_color\"], eye_color_value_map),\n",
    "        \"apparent_left_eye_color\": map_categorical(subject_annotation[\"apparent_left_eye_color\"], eye_color_value_map),\n",
    "        \"natural_right_eye_color\": map_categorical(subject_annotation[\"natural_right_eye_color\"], eye_color_value_map),\n",
    "        \"apparent_right_eye_color\": map_categorical(subject_annotation[\"apparent_right_eye_color\"], eye_color_value_map),\n",
    "        \"facial_marks\": map_categorical(subject_annotation[\"facial_marks\"], facial_marks_value_map),\n",
    "        \"action_body_pose\": map_categorical(subject_annotation[\"action_body_pose\"], action_body_pose_value_map),\n",
    "        \"action_subject_object_interaction\": map_categorical(subject_annotation[\"action_subject_object_interaction\"], action_subject_object_interaction_value_map),\n",
    "        \"head_pose\": map_categorical(subject_annotation[\"head_pose\"], head_pose_value_map),\n",
    "    }\n",
    "\n",
    "    return subject_annotation_dict\n",
    "# ruff: noqa: enable\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "This is the main loop where we iterate over annotation files, extract and process annotations, and store the processed data in a list of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "total = len(annotation_paths) if MAX_SAMPLES is None else min(len(annotation_paths), MAX_SAMPLES)\n",
    "\n",
    "for annotation_path in tqdm(annotation_paths, total=total, desc=\"Processing annotations\"):\n",
    "    with open(annotation_path) as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    image_path = annotation_path.with_name(f\"main_{annotations['image']['file_name']}\")\n",
    "    image_annotation = annotations[\"image_annotation\"]\n",
    "    subject_annotations = annotations[\"subject_annotation\"]\n",
    "    image_annotation_dict = process_image_annotation(image_annotation)\n",
    "\n",
    "    image_id = annotations[\"image\"][\"id\"]\n",
    "\n",
    "    for subject_annotation in subject_annotations:\n",
    "        subject_bbox = person_bbox_df.loc[(image_id, subject_annotation[\"subject_id\"])]\n",
    "        person_bbox_xywh = eval(subject_bbox[\"person_bbox\"])\n",
    "        person_bbox_xyxy = [\n",
    "            person_bbox_xywh[0],\n",
    "            person_bbox_xywh[1],\n",
    "            person_bbox_xywh[0] + person_bbox_xywh[2],\n",
    "            person_bbox_xywh[1] + person_bbox_xywh[3],\n",
    "        ]\n",
    "        subject_annotation[\"person_bbox\"] = person_bbox_xyxy\n",
    "        subject_annotation_dict = process_subject_annotation(\n",
    "            subject_annotation, image_annotation[\"image_height\"], image_annotation[\"image_width\"]\n",
    "        )\n",
    "        rows.append(\n",
    "            {\n",
    "                \"image\": tlc.Url(image_path).to_relative().to_str(),\n",
    "                **image_annotation_dict,\n",
    "                **subject_annotation_dict,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if MAX_SAMPLES is not None and len(rows) >= MAX_SAMPLES:\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    break  # Max samples reached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Define column schemas\n",
    "\n",
    "We are now ready to define our schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, 3LC Schemas are visible and writable. Since this dataset has a\n",
    "# large number of columns, we set the default visibility to False. We also make\n",
    "# the columns read-only in the UI. Columns can easily be made visible in the UI\n",
    "# by selecting them from the \"wrench\" menu.\n",
    "default_schema_args = {\n",
    "    \"default_visible\": False,\n",
    "    \"writable\": False,\n",
    "}\n",
    "\n",
    "\n",
    "# Override color palette for the skin-color related columns\n",
    "def tuple2hex(t: str):\n",
    "    \"\"\"Convert a serialized list of integers to a hex string: '[255, 255, 255]' -> '#FFFFFF'\"\"\"\n",
    "    return \"#{:02X}{:02X}{:02X}\".format(*(int(c) for c in t.strip(\"[]\").split(\",\")))\n",
    "\n",
    "\n",
    "# fmt: off\n",
    "# ruff: noqa: E501\n",
    "metadata_schemas = {\n",
    "    # Image annotations\n",
    "    \"aperture_value\": tlc.Float32Schema(**default_schema_args),\n",
    "    \"camera_distance\": tlc.CategoricalLabelSchema(classes=camera_distance_value_map.keys(), **default_schema_args),\n",
    "    \"camera_position\": tlc.CategoricalLabelSchema(classes=camera_position_value_map.keys(), **default_schema_args),\n",
    "    \"focal_length\": tlc.Float32Schema(**default_schema_args),\n",
    "    \"iso_speed_ratings\": tlc.Int32Schema(**default_schema_args),\n",
    "    \"lighting\": tlc.CategoricalLabelListSchema(classes=lighting_value_map.keys(), **default_schema_args),\n",
    "    \"location_country\": tlc.StringSchema(**default_schema_args),\n",
    "    \"location_region\": tlc.StringSchema(**default_schema_args),\n",
    "    \"manufacturer\": tlc.StringSchema(**default_schema_args),\n",
    "    \"model\": tlc.StringSchema(**default_schema_args),\n",
    "    \"scene\": tlc.CategoricalLabelSchema(classes=scene_value_map.keys(), **default_schema_args),\n",
    "    \"shutter_speed_value\": tlc.Float32Schema(**default_schema_args),\n",
    "    \"user_date_captured\": tlc.StringSchema(**default_schema_args),\n",
    "    \"user_hour_captured\": tlc.CategoricalLabelSchema(classes=user_hour_captured_value_map.keys(), **default_schema_args),\n",
    "    \"weather\": tlc.CategoricalLabelListSchema(classes=weather_value_map.keys(), **default_schema_args),\n",
    "    # Subject annotations\n",
    "    \"subject_id\": tlc.StringSchema(**default_schema_args),\n",
    "    \"age\": tlc.Int32Schema(**default_schema_args),\n",
    "    \"nationality\": tlc.CategoricalLabelListSchema(classes=nationality_value_map.keys(), **default_schema_args),\n",
    "    \"ancestry\": tlc.CategoricalLabelListSchema(classes=ancestry_value_map.keys(), **default_schema_args),\n",
    "    \"pronoun\": tlc.CategoricalLabelListSchema(classes=pronoun_value_map.keys(), **default_schema_args),\n",
    "    \"natural_skin_color\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: tlc.MapElement(k, display_color=tuple2hex(k)) for k, v in skin_color_value_map.items()},\n",
    "        **default_schema_args,\n",
    "    ),\n",
    "    \"apparent_skin_color\": tlc.CategoricalLabelSchema(\n",
    "        classes={v: tlc.MapElement(k, display_color=tuple2hex(k)) for k, v in skin_color_value_map.items()},\n",
    "        **default_schema_args,\n",
    "    ),\n",
    "    \"hairstyle\": tlc.CategoricalLabelSchema(classes=hairstyle_value_map.keys(), **default_schema_args),\n",
    "    \"natural_hair_type\": tlc.CategoricalLabelSchema(classes=hair_type_value_map.keys(), **default_schema_args),\n",
    "    \"apparent_hair_type\": tlc.CategoricalLabelSchema(classes=hair_type_value_map.keys(), **default_schema_args),\n",
    "    \"natural_hair_color\": tlc.CategoricalLabelListSchema(classes=haircolor_value_map.keys(), **default_schema_args),\n",
    "    \"apparent_hair_color\": tlc.CategoricalLabelListSchema(classes=haircolor_value_map.keys(), **default_schema_args),\n",
    "    \"facial_hairstyle\": tlc.CategoricalLabelListSchema(classes=facial_hairstyle_value_map.keys(), **default_schema_args),\n",
    "    \"natural_facial_hair_color\": tlc.CategoricalLabelListSchema(classes=haircolor_value_map.keys(), **default_schema_args),\n",
    "    \"apparent_facial_hair_color\": tlc.CategoricalLabelListSchema(classes=haircolor_value_map.keys(), **default_schema_args),\n",
    "    \"natural_left_eye_color\": tlc.CategoricalLabelListSchema(classes=eye_color_value_map.keys(), **default_schema_args),\n",
    "    \"apparent_left_eye_color\": tlc.CategoricalLabelListSchema(classes=eye_color_value_map.keys(), **default_schema_args),\n",
    "    \"natural_right_eye_color\": tlc.CategoricalLabelListSchema(classes=eye_color_value_map.keys(), **default_schema_args),\n",
    "    \"apparent_right_eye_color\": tlc.CategoricalLabelListSchema(classes=eye_color_value_map.keys(), **default_schema_args),\n",
    "    \"facial_marks\": tlc.CategoricalLabelListSchema(classes=facial_marks_value_map.keys(), **default_schema_args),\n",
    "    \"action_body_pose\": tlc.CategoricalLabelSchema(classes=action_body_pose_value_map.keys(), **default_schema_args),\n",
    "    \"action_subject_object_interaction\": tlc.CategoricalLabelListSchema(classes=action_subject_object_interaction_value_map.keys(), **default_schema_args),\n",
    "    \"head_pose\": tlc.CategoricalLabelSchema(classes=head_pose_value_map.keys(), **default_schema_args),\n",
    "}\n",
    "# ruff: noqa: enable\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Write the Table\n",
    "\n",
    "Finally, we create a `TableWriter`, and add our rows to the Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_writer = tlc.TableWriter(\n",
    "    table_name=TABLE_NAME,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    project_name=PROJECT_NAME,\n",
    "    column_schemas={\n",
    "        \"image\": tlc.ImageUrlSchema(),\n",
    "        \"keypoints\": tlc.Keypoints2DSchema(\n",
    "            classes=[\"person\"],\n",
    "            num_keypoints=NUM_KEYPOINTS,\n",
    "            lines=SKELETON,\n",
    "            point_attributes=KEYPOINTS,\n",
    "            include_per_point_visibility=True,\n",
    "        ),\n",
    "        \"face_bbox\": tlc.BoundingBoxListSchema(\n",
    "            label_value_map={0: tlc.MapElement(\"face\")},\n",
    "            include_segmentation=False,\n",
    "            x1_number_role=tlc.NUMBER_ROLE_BB_SIZE_X,\n",
    "            y1_number_role=tlc.NUMBER_ROLE_BB_SIZE_Y,\n",
    "        ),\n",
    "        \"segments\": tlc.SegmentationSchema(\n",
    "            label_value_map={v: tlc.MapElement(k) for k, v in segments_value_map.items()},\n",
    "            sample_type=tlc.InstanceSegmentationMasks.sample_type,\n",
    "        ),\n",
    "        **metadata_schemas,\n",
    "    },\n",
    ")\n",
    "\n",
    "for row in tqdm(rows, total=len(rows), desc=\"Writing rows\"):\n",
    "    table_writer.add_row(row)\n",
    "\n",
    "table = table_writer.finalize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
