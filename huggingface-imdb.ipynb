{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤— and 3LC example on the IMDb dataset\n",
    "\n",
    "This notebook demonstrates fine-tuning a pretrained DistilBERT model from `transformers` on the `IMDb` dataset, using the 3LC integrations with `Trainer` and `datasets` from Hugging Face. 3LC metrics are collected before and after one epoch of training.\n",
    "\n",
    "The notebook covers:\n",
    "\n",
    "- Getting a `TLCDataset` from a `datasets` dataset, highlighting key differences between `TLCDataset` and `datasets.Dataset`.\n",
    "- Fine-tuning a pretrained `transformers` model on the IMDb dataset with `TLCTrainer`.\n",
    "- Using a custom function for metrics collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 256\n",
    "TRAIN_DATASET_NAME = \"hf-imdb-train\"\n",
    "EVAL_DATASET_NAME = \"hf-imdb-test\"\n",
    "TRANSIENT_DATA_PATH = \"./transient_data\"\n",
    "DEVICE = \"cuda:0\"\n",
    "PROJECT_NAME = \"hf-imdb\"\n",
    "INSTALL_DEPENDENCIES = False\n",
    "TLC_PUBLIC_EXAMPLES_DEVELOPER_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INSTALL_DEPENDENCIES:\n",
    "    %pip --quiet install ipykernel ipywidgets\n",
    "    %pip --quiet install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "    %pip --quiet install torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "    %pip --quiet install datasets transformers\n",
    "    %pip --quiet install accelerate\n",
    "    %pip --quiet install tlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import tlc\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "### HIDDEN CELL ###\n",
    "\n",
    "## Data & Alias management\n",
    "# See comments in ../mnist.ipynb for details on data and alias management.\n",
    "\n",
    "# Set this variable to True if you just want to run this notebook for local testing purposes\n",
    "if not TLC_PUBLIC_EXAMPLES_DEVELOPER_MODE:\n",
    "    from tlc.client.utils import (\n",
    "        TLC_PUBLIC_EXAMPLES_RUN_ROOT,\n",
    "        TLC_PUBLIC_EXAMPLES_TABLE_ROOT,\n",
    "    )\n",
    "    from tlc.core.objects.mutable_objects import Configuration\n",
    "\n",
    "    print(f\"Runs and Tables will be written to remote location: '{TLC_PUBLIC_EXAMPLES_RUN_ROOT}' and '{TLC_PUBLIC_EXAMPLES_TABLE_ROOT}'\")\n",
    "    Configuration.instance().run_root_url = TLC_PUBLIC_EXAMPLES_RUN_ROOT\n",
    "    Configuration.instance().table_root_url = TLC_PUBLIC_EXAMPLES_TABLE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the 3LC integration, you can use `load_dataset` as a drop-in replacement to create a `TLCDataset`. Notice `.latest()`, which gets the latest version of the 3LC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlc.integration.huggingface import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"imdb\", split=\"train\", project_name=PROJECT_NAME, dataset_name=TRAIN_DATASET_NAME, write_row_cache=True)\n",
    "eval_dataset = load_dataset(\"imdb\", split=\"test\", project_name=PROJECT_NAME, dataset_name=TRAIN_DATASET_NAME, write_row_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's compare the first samples of the training splits in the 3LC integration and `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "train_dataset_hf = datasets.load_dataset(\"imdb\", split=\"train\")\n",
    "train_dataset_hf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out they are different, which is probably quite surprising! The reason for this is that `TLCDataset` randomly samples examples by default, based on the editable column `Sampling Weight` in the 3LC Dashboard. In order to get the expected examples, you can either use `.get_sample_at_index()` or use the `sequential` context manager. The latter is used internally in 3LC when collecting metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TLCDataset` provides a method `map` to apply both preprocessing and on-the-fly transforms to your data before it is used sent to the model. It takes a sample and returns the transformed example. Here `cache=True` is used to persist the result of tokenization for each sample, such that it is only done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenize = lambda sample: {**sample, **tokenizer(sample[\"text\"], truncation=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = train_dataset.map(tokenize)\n",
    "eval_tokenized = eval_dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"neg\", 1: \"pos\"}\n",
    "label2id = {\"neg\": 0, \"pos\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing metrics is done by implementing a function which returns all the per-sample metrics you would like to see in the 3LC Dashboard. We keep the metrics function in Hugging Face to see the intermediate aggregate metrics.\n",
    "\n",
    "For special metrics such as the predicted category we specify that we would like this to be shown as a `CategoricalLabel`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def compute_tlc_metrics(logits, labels):\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, labels, reduction=\"none\")\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    confidence = probabilities.gather(dim=-1, index=predictions.unsqueeze(-1)).squeeze()\n",
    "\n",
    "    return {\n",
    "        \"predicted\": predictions,\n",
    "        \"loss\": loss,\n",
    "        \"confidence\": confidence,\n",
    "    }\n",
    "\n",
    "compute_tlc_metrics.column_schemas = {\n",
    "    \"predicted\": tlc.CategoricalLabelSchema(display_name=\"Predicted Label\", class_names=id2label.values(), display_importance=4005),\n",
    "    \"loss\": tlc.Schema(display_name=\"Loss\", writable=False, value=tlc.Float32Value()),\n",
    "    \"confidence\": tlc.Schema(display_name=\"Confidence\", writable=False, value=tlc.Float32Value()),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with TLCTrainer\n",
    "\n",
    "To perform model training, we replace the usual `Trainer` with `TLCTrainer` and provide the per-sample metrics collection function. We also specify that we would like to collect metrics prior to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlc.init(project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlc.integration.huggingface import TLCTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=TRANSIENT_DATA_PATH,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    use_cpu=DEVICE == \"cpu\",\n",
    ")\n",
    "\n",
    "trainer = TLCTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    compute_tlc_metrics=compute_tlc_metrics,\n",
    "    collect_tlc_metrics_before_training=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
