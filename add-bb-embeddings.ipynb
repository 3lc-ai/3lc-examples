{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per Bounding Box Embeddings Example\n",
    "\n",
    "This notebook demonstrates how to extract embeddings for bounding boxes in `tlc` Tables using a pretrained EfficientNet model. The generated embeddings are then reduced in dimensionality and stored as metrics in a run.\n",
    "\n",
    "Since the example uses a classification model, we can also extract class probabilities for each bounding box. The predicted labels are also stored as metrics in the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"Bounding Box Embeddings\"\n",
    "DATASET_NAME = \"Balloons\"\n",
    "INSTALL_DEPENDENCIES = False\n",
    "TRANSIENT_DATA_PATH = \"./transient_data\"\n",
    "TEST_DATA_PATH = \"../tests/test_data/data\"\n",
    "TLC_PUBLIC_EXAMPLES_DEVELOPER_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if INSTALL_DEPENDENCIES:\n",
    "    %pip --quiet install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "    %pip --quiet install torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "    %pip --quiet install timm\n",
    "    %pip --quiet install tlc[umap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import tlc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Input Table\n",
    "\n",
    "We will use a `TableFromCoco` to load the \"Balloons\" dataset from a annotations file and a folder of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_url = tlc.Table.default_write_location() / f\"{PROJECT_NAME}/table_from_coco.json\"\n",
    "\n",
    "annotations_file = tlc.Url(TEST_DATA_PATH + \"/balloons/train/train-annotations.json\").to_absolute()\n",
    "images_dir = tlc.Url(TEST_DATA_PATH + \"/balloons/train\").to_absolute()\n",
    "\n",
    "input_table = tlc.TableFromCoco(\n",
    "    url=table_url,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    project_name=PROJECT_NAME,\n",
    "    input_url=annotations_file.to_relative(),\n",
    "    image_folder_url=images_dir.to_relative(),\n",
    "    row_cache_url=\"../table_from_coco.parquet\",\n",
    ")\n",
    "\n",
    "print(input_table.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = tlc.init(project_name=PROJECT_NAME)\n",
    "run.add_input_table(input_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the schema of the bounding box column of the input table\n",
    "import json\n",
    "bb_schema = input_table.schema.values[\"rows\"].values[\"bbs\"].values[\"bb_list\"]\n",
    "label_map = input_table.get_value_map_for_column(\"bbs\")\n",
    "print(f\"Input table uses {len(label_map)} unique label(s): {json.dumps(label_map, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model\n",
    "\n",
    "Now we load the EfficientNet model. If a pretrained model is available locally, it will be loaded. Otherwise, we'll download a pretrained version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize a pretrained classifier model\n",
    "try:\n",
    "    model = timm.create_model(\"efficientnet_b0\", num_classes=2, checkpoint_path=TRANSIENT_DATA_PATH + \"/bb_classifier.pth\").to(device)\n",
    "    print(\"Loaded pretrained model\")\n",
    "except:\n",
    "    print(\"Downloading pretrained model\")\n",
    "    model = timm.create_model(\"efficientnet_b0\", num_classes=len(label_map), pretrained=True).to(device)\n",
    "\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hidden layer we will use as embeddings\n",
    "hidden_layer = model.global_pool.flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Bounding Box Embeddings\n",
    "\n",
    "In this section, we'll walk through the process of extracting embeddings for each bounding box present in our input images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize empty lists to store all embeddings and predicted labels\n",
    "all_embeddings: list[np.ndarray] = []\n",
    "all_labels: list[int] = []\n",
    "all_hidden_outputs: list[np.ndarray] = []\n",
    "\n",
    "# Register a hook to pick up the hidden layer output\n",
    "output_list: list[torch.Tensor] = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"Store the output of the hooked layer.\"\"\"\n",
    "    output_list.append(output)\n",
    "\n",
    "hook_handle = hidden_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# Batched inference setup\n",
    "batch_size = 4\n",
    "mini_batch: list[torch.Tensor] = []\n",
    "batch_to_image_map: list[int] = []\n",
    "\n",
    "def run_inference_on_batch(mini_batch: list[torch.Tensor]) -> None:\n",
    "    mini_batch_tensor = torch.stack(mini_batch).to(device)\n",
    "    with torch.no_grad():\n",
    "        mini_batch_embeddings = model(mini_batch_tensor)\n",
    "\n",
    "    # Collect and clear the hook outputs\n",
    "    mini_batch_hidden = output_list.pop().cpu().numpy()\n",
    "    all_hidden_outputs.extend(mini_batch_hidden)\n",
    "    \n",
    "    all_embeddings.extend(mini_batch_embeddings.cpu().numpy())\n",
    "    mini_batch_labels = torch.argmax(mini_batch_embeddings, dim=1)\n",
    "    all_labels.extend(mini_batch_labels.cpu().numpy())\n",
    "\n",
    "for row_idx, row in tqdm.tqdm(enumerate(input_table.table_rows), total=len(input_table), desc=\"Running inference on table\"):\n",
    "    image_bbs = row[\"bbs\"][\"bb_list\"]\n",
    "    if len(image_bbs) == 0:\n",
    "        continue\n",
    "    image_filename = row[\"image\"]\n",
    "    image_bytes = tlc.Url(image_filename).read()\n",
    "    image = Image.open(BytesIO(image_bytes))\n",
    "    w, h = image.size\n",
    "\n",
    "    for bb in image_bbs:\n",
    "        bb_crop = tlc.BBCropInterface.crop(image, bb, bb_schema, h, w)\n",
    "        bb_crop_tensor = preprocess(bb_crop)\n",
    "        \n",
    "        # Check if adding this bb_crop_tensor will overfill the mini_batch\n",
    "        if len(mini_batch) >= batch_size:\n",
    "            run_inference_on_batch(mini_batch)\n",
    "            mini_batch.clear()\n",
    "        \n",
    "        mini_batch.append(bb_crop_tensor)\n",
    "        batch_to_image_map.append(row_idx)\n",
    "\n",
    "# Run inference on remaining items in mini_batch if it's not empty\n",
    "if len(mini_batch) > 0:\n",
    "    run_inference_on_batch(mini_batch)\n",
    "\n",
    "# Remove the hook\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Once the embeddings are collected, the next step is to reduce their dimensionality for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "all_embeddings_np = np.vstack(all_hidden_outputs)\n",
    "print(f\"UMAP input shape: {all_embeddings_np.shape}\")\n",
    "\n",
    "# Fit UMAP\n",
    "reducer = umap.UMAP(n_components=3)\n",
    "embedding_3d = reducer.fit_transform(all_embeddings_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Metrics in a Run\n",
    "\n",
    "Finally, we add these reduced embeddings and predicted labels to our current run for further analysis in the Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repack embeddings and labels into groups per image\n",
    "grouped_embeddings: list[list[np.ndarray]] = [[] for _ in range(len(input_table))]\n",
    "grouped_labels: list[list[int]] = [[] for _ in range(len(input_table))]\n",
    "\n",
    "for img_idx, embed, label in zip(batch_to_image_map, embedding_3d, all_labels):\n",
    "    grouped_labels[img_idx].append(label)\n",
    "    grouped_embeddings[img_idx].append(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a schema for the embeddings\n",
    "float_list_list_schema = tlc.Schema(\n",
    "    value=tlc.Float32Value(),\n",
    "    size0=tlc.DimensionNumericValue(value_min=3, value_max=3),     # 3D embedding\n",
    "    size1=tlc.DimensionNumericValue(value_min=0, value_max=1000),  # Max 1000 bbs per image\n",
    ")\n",
    "\n",
    "# Create a schema with a label map for the labels\n",
    "label_value_map = {\n",
    "    **{float(k): tlc.MapElement(v) for k, v in label_map.items()},\n",
    "    **{len(label_map): tlc.MapElement(\"background\")},\n",
    "}\n",
    "\n",
    "label_schema = tlc.Schema(\n",
    "    value=tlc.Int32Value(value_map=label_value_map),\n",
    "    size0 = tlc.DimensionNumericValue(value_min=0, value_max=1000),\n",
    ")\n",
    "\n",
    "data = {\n",
    "    \"per_bb_embeddings\": grouped_embeddings,\n",
    "    \"per_bb_labels\": grouped_labels,\n",
    "}\n",
    "\n",
    "schemas = {\n",
    "    \"per_bb_embeddings\": float_list_list_schema,\n",
    "    \"per_bb_labels\": label_schema,\n",
    "}\n",
    "\n",
    "# Add the computed per-bb embeddings and labels as metrics to the run\n",
    "run.add_metrics_data(\n",
    "    data,\n",
    "    override_column_schemas=schemas,\n",
    "    input_table_url=input_table.url,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
