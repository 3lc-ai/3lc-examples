{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Step 2: Fully manual bulk data management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tlc\n",
    "from data_sources import random_array_generator\n",
    "from tlc.core.helpers.bulk_data_helper import BulkDataHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tlc.Geometry2DSchema(\n",
    "    include_2d_vertices=True,\n",
    "    per_vertex_schemas={\"intensity\": tlc.Float32ListSchema()},\n",
    "    is_bulk_data=True,  # This is what sets up the \"sibling\" paths with the \"_binary_property_url\" suffix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_2d_generator = random_array_generator((4, 2))  # generates 4 2d points at a time\n",
    "intensity_generator = random_array_generator((4,), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_data_path = Path(\"bulk_data/2\").absolute()\n",
    "bulk_data_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "rows = []\n",
    "chunk_offsets = defaultdict(int)\n",
    "\n",
    "for i in range(10):\n",
    "    points_2d = next(points_2d_generator)\n",
    "    intensity = next(intensity_generator)\n",
    "\n",
    "    chunk = i // 3  # We are now in manual mode, so it our responsibility to rotate chunks as needed\n",
    "    bulk_data_file = bulk_data_path / f\"{chunk}.raw\"\n",
    "\n",
    "    points_2d_length = np.prod(points_2d.shape) * points_2d.dtype.itemsize\n",
    "    intensity_length = np.prod(intensity.shape) * intensity.dtype.itemsize\n",
    "\n",
    "    points_2d_binary = points_2d.tobytes()\n",
    "    intensity_binary = intensity.tobytes()\n",
    "\n",
    "    assert len(points_2d_binary) == points_2d_length\n",
    "    assert len(intensity_binary) == intensity_length\n",
    "\n",
    "    with open(bulk_data_file, \"ab\") as f:\n",
    "        written = f.write(points_2d_binary)\n",
    "        assert written == points_2d_length\n",
    "        chunk_offsets[chunk] += points_2d_length\n",
    "        points_binary_property_value = BulkDataHelper.get_bulk_data_url(\n",
    "            bulk_data_file, chunk_offsets[chunk], points_2d_length\n",
    "        )\n",
    "\n",
    "        written = f.write(intensity_binary)\n",
    "        assert written == intensity_length\n",
    "        chunk_offsets[chunk] += intensity_length\n",
    "        intensity_binary_property_value = BulkDataHelper.get_bulk_data_url(\n",
    "            bulk_data_file, chunk_offsets[chunk], intensity_length\n",
    "        )\n",
    "\n",
    "    row = {\n",
    "        \"x_min\": 0,\n",
    "        \"y_min\": 0,\n",
    "        \"x_max\": 1,\n",
    "        \"y_max\": 1,\n",
    "        \"instances\": [\n",
    "            {\n",
    "                # \"vertices_2d\": [],\n",
    "                \"vertices_2d_binary_property_url\": points_binary_property_value,\n",
    "                \"vertices_2d_additional_data\": {\n",
    "                    # \"intensity\": [],\n",
    "                    \"intensity_binary_property_url\": intensity_binary_property_value\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Write the Table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_writer = tlc.TableWriter(\n",
    "    table_name=\"externalize-manually-no-processor\",\n",
    "    dataset_name=\"pre-externalized-dataset\",\n",
    "    project_name=\"External Bulk Data\",\n",
    "    description=\"Pre-externalized table\",\n",
    "    column_schemas={\"vertices\": schema},  # We use the same schema as before\n",
    "    if_exists=\"rename\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    table_writer.add_row({\"vertices\": row})\n",
    "\n",
    "table = table_writer.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.table_rows[0][\"vertices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
